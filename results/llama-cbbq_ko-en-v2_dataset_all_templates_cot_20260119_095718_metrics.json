{
  "model": "llama-cbbq",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-19T09:57:18.709841",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.44638949671772427,
      "bias_accuracy": 0.014947089947089947,
      "bias_rate": 0.004232804232804233,
      "bias_score": 0.029069767441860465,
      "bias_score_details": {
        "n_biased": 32,
        "n_counter_biased": 27,
        "n_unknown": 113,
        "n_valid": 172
      },
      "culture_accuracy": 0.3192982456140351,
      "culture_total": 7560,
      "culture_valid": 285,
      "culture_correct": 91,
      "culture_incorrect": 101,
      "culture_unknown": 93,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 1.0,
          "total": 2088,
          "valid": 31,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 31
        },
        "gender": {
          "accuracy": 0.7857142857142857,
          "total": 1584,
          "valid": 42,
          "bias_score": 0.21428571428571427,
          "n_biased": 9,
          "n_counter_biased": 0,
          "n_unknown": 33
        },
        "age": {
          "accuracy": 0.9387755102040817,
          "total": 2016,
          "valid": 49,
          "bias_score": 0.061224489795918366,
          "n_biased": 3,
          "n_counter_biased": 0,
          "n_unknown": 46
        },
        "title": {
          "accuracy": 0.06,
          "total": 1008,
          "valid": 50,
          "bias_score": -0.14,
          "n_biased": 20,
          "n_counter_biased": 27,
          "n_unknown": 3
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.02825670498084291,
          "total": 2088,
          "correct": 59
        },
        "gender": {
          "accuracy": 0.0,
          "total": 1584,
          "correct": 0
        },
        "age": {
          "accuracy": 0.001488095238095238,
          "total": 2016,
          "correct": 3
        },
        "title": {
          "accuracy": 0.028769841269841268,
          "total": 1008,
          "correct": 29
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "correct": 0
        }
      },
      "invalid_predictions": 14663,
      "invalid_rate": 0.9697751322751322
    },
    "template_2": {
      "overall_accuracy": 0.32761904761904764,
      "bias_accuracy": 0.0050264550264550265,
      "bias_rate": 0.007142857142857143,
      "bias_score": -0.1657142857142857,
      "bias_score_details": {
        "n_biased": 54,
        "n_counter_biased": 83,
        "n_unknown": 38,
        "n_valid": 175
      },
      "culture_accuracy": 0.38285714285714284,
      "culture_total": 7560,
      "culture_valid": 350,
      "culture_correct": 134,
      "culture_incorrect": 170,
      "culture_unknown": 46,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6363636363636364,
          "total": 2088,
          "valid": 11,
          "bias_score": 0.18181818181818182,
          "n_biased": 3,
          "n_counter_biased": 1,
          "n_unknown": 7
        },
        "gender": {
          "accuracy": 0.5333333333333333,
          "total": 1584,
          "valid": 15,
          "bias_score": 0.4666666666666667,
          "n_biased": 7,
          "n_counter_biased": 0,
          "n_unknown": 8
        },
        "age": {
          "accuracy": 1.0,
          "total": 2016,
          "valid": 23,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 23
        },
        "title": {
          "accuracy": 0.0,
          "total": 1008,
          "valid": 126,
          "bias_score": -0.30158730158730157,
          "n_biased": 44,
          "n_counter_biased": 82,
          "n_unknown": 0
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.04741379310344827,
          "total": 2088,
          "correct": 99
        },
        "gender": {
          "accuracy": 0.0,
          "total": 1584,
          "correct": 0
        },
        "age": {
          "accuracy": 0.0,
          "total": 2016,
          "correct": 0
        },
        "title": {
          "accuracy": 0.034722222222222224,
          "total": 1008,
          "correct": 35
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "correct": 0
        }
      },
      "invalid_predictions": 14595,
      "invalid_rate": 0.9652777777777778
    },
    "template_3": {
      "overall_accuracy": 0.20161943319838058,
      "bias_accuracy": 0.0030423280423280425,
      "bias_rate": 0.0458994708994709,
      "bias_score": 0.060650887573964495,
      "bias_score_details": {
        "n_biased": 347,
        "n_counter_biased": 306,
        "n_unknown": 23,
        "n_valid": 676
      },
      "culture_accuracy": 0.40429338103756707,
      "culture_total": 7560,
      "culture_valid": 559,
      "culture_correct": 226,
      "culture_incorrect": 300,
      "culture_unknown": 33,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.0031746031746031746,
          "total": 2088,
          "valid": 315,
          "bias_score": 0.10158730158730159,
          "n_biased": 173,
          "n_counter_biased": 141,
          "n_unknown": 1
        },
        "gender": {
          "accuracy": 0.2375,
          "total": 1584,
          "valid": 80,
          "bias_score": 0.2125,
          "n_biased": 39,
          "n_counter_biased": 22,
          "n_unknown": 19
        },
        "age": {
          "accuracy": 0.3,
          "total": 2016,
          "valid": 10,
          "bias_score": 0.1,
          "n_biased": 4,
          "n_counter_biased": 3,
          "n_unknown": 3
        },
        "title": {
          "accuracy": 0.0,
          "total": 1008,
          "valid": 267,
          "bias_score": -0.033707865168539325,
          "n_biased": 129,
          "n_counter_biased": 138,
          "n_unknown": 0
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "valid": 4,
          "bias_score": 0.0,
          "n_biased": 2,
          "n_counter_biased": 2,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.07567049808429119,
          "total": 2088,
          "correct": 158
        },
        "gender": {
          "accuracy": 0.01893939393939394,
          "total": 1584,
          "correct": 30
        },
        "age": {
          "accuracy": 0.00248015873015873,
          "total": 2016,
          "correct": 5
        },
        "title": {
          "accuracy": 0.03273809523809524,
          "total": 1008,
          "correct": 33
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "correct": 0
        }
      },
      "invalid_predictions": 13885,
      "invalid_rate": 0.9183201058201058
    },
    "averaged": {
      "overall_accuracy": 0.32520932584505086,
      "bias_accuracy": 0.007671957671957673,
      "bias_rate": 0.019091710758377425,
      "bias_score": -0.025331210232820246,
      "bias_score_details": {
        "n_biased": 144.33333333333334,
        "n_counter_biased": 138.66666666666666,
        "n_unknown": 58.0,
        "n_valid": 341.0
      },
      "culture_accuracy": 0.368816256502915,
      "culture_total": 7560.0,
      "culture_valid": 398.0,
      "culture_correct": 150.33333333333334,
      "culture_incorrect": 190.33333333333334,
      "culture_unknown": 57.333333333333336,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5465127465127465,
          "total": 2088.0,
          "valid": 119.0,
          "bias_score": 0.09446849446849448,
          "n_biased": 58.666666666666664,
          "n_counter_biased": 47.333333333333336,
          "n_unknown": 13.0
        },
        "gender": {
          "accuracy": 0.5188492063492064,
          "total": 1584.0,
          "valid": 45.666666666666664,
          "bias_score": 0.2978174603174603,
          "n_biased": 18.333333333333332,
          "n_counter_biased": 7.333333333333333,
          "n_unknown": 20.0
        },
        "age": {
          "accuracy": 0.7462585034013606,
          "total": 2016.0,
          "valid": 27.333333333333332,
          "bias_score": 0.05374149659863945,
          "n_biased": 2.3333333333333335,
          "n_counter_biased": 1.0,
          "n_unknown": 24.0
        },
        "title": {
          "accuracy": 0.02,
          "total": 1008.0,
          "valid": 147.66666666666666,
          "bias_score": -0.158431722251947,
          "n_biased": 64.33333333333333,
          "n_counter_biased": 82.33333333333333,
          "n_unknown": 1.0
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864.0,
          "valid": 1.3333333333333333,
          "bias_score": 0.0,
          "n_biased": 0.6666666666666666,
          "n_counter_biased": 0.6666666666666666,
          "n_unknown": 0.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.05044699872286079,
          "total": 2088.0,
          "correct": 105.33333333333333
        },
        "gender": {
          "accuracy": 0.006313131313131313,
          "total": 1584.0,
          "correct": 10.0
        },
        "age": {
          "accuracy": 0.0013227513227513227,
          "total": 2016.0,
          "correct": 2.6666666666666665
        },
        "title": {
          "accuracy": 0.03207671957671957,
          "total": 1008.0,
          "correct": 32.333333333333336
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864.0,
          "correct": 0.0
        }
      },
      "invalid_predictions": 14381.0,
      "invalid_rate": 0.9511243386243388
    }
  }
}