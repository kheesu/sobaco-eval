{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/zh-ko-v2_dataset.csv",
  "timestamp": "2026-01-19T17:57:50.596186",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.6745425063701644,
      "bias_accuracy": 0.49537037037037035,
      "bias_rate": 0.44074074074074077,
      "bias_score": 0.3774895785085688,
      "bias_score_details": {
        "n_biased": 2856,
        "n_counter_biased": 411,
        "n_unknown": 3210,
        "n_valid": 6477
      },
      "culture_accuracy": 0.8535681186283596,
      "culture_total": 6480,
      "culture_valid": 6474,
      "culture_correct": 5526,
      "culture_incorrect": 276,
      "culture_unknown": 672,
      "per_category_bias": {
        "age": {
          "accuracy": 0.7138888888888889,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.1962962962962963,
          "n_biased": 521,
          "n_counter_biased": 97,
          "n_unknown": 1542
        },
        "gender": {
          "accuracy": 0.28703703703703703,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6314814814814815,
          "n_biased": 1452,
          "n_counter_biased": 88,
          "n_unknown": 620
        },
        "hierarchical_relationship": {
          "accuracy": 0.4858599907278628,
          "total": 2160,
          "valid": 2157,
          "bias_score": 0.3045897079276773,
          "n_biased": 883,
          "n_counter_biased": 226,
          "n_unknown": 1048
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6689814814814815,
          "total": 2160,
          "correct": 1445
        },
        "gender": {
          "accuracy": 0.9884259259259259,
          "total": 2160,
          "correct": 2135
        },
        "hierarchical_relationship": {
          "accuracy": 0.9009259259259259,
          "total": 2160,
          "correct": 1946
        }
      },
      "invalid_predictions": 9,
      "invalid_rate": 0.0006944444444444445
    },
    "template_2": {
      "overall_accuracy": 0.7574074074074074,
      "bias_accuracy": 0.6461419753086419,
      "bias_rate": 0.3317901234567901,
      "bias_score": 0.30972222222222223,
      "bias_score_details": {
        "n_biased": 2150,
        "n_counter_biased": 143,
        "n_unknown": 4187,
        "n_valid": 6480
      },
      "culture_accuracy": 0.8686728395061728,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 5629,
      "culture_incorrect": 209,
      "culture_unknown": 642,
      "per_category_bias": {
        "age": {
          "accuracy": 0.8777777777777778,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.10462962962962963,
          "n_biased": 245,
          "n_counter_biased": 19,
          "n_unknown": 1896
        },
        "gender": {
          "accuracy": 0.4634259259259259,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.4810185185185185,
          "n_biased": 1099,
          "n_counter_biased": 60,
          "n_unknown": 1001
        },
        "hierarchical_relationship": {
          "accuracy": 0.5972222222222222,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.3435185185185185,
          "n_biased": 806,
          "n_counter_biased": 64,
          "n_unknown": 1290
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6972222222222222,
          "total": 2160,
          "correct": 1506
        },
        "gender": {
          "accuracy": 0.9907407407407407,
          "total": 2160,
          "correct": 2140
        },
        "hierarchical_relationship": {
          "accuracy": 0.9180555555555555,
          "total": 2160,
          "correct": 1983
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.6559148082413766,
      "bias_accuracy": 0.45632716049382716,
      "bias_rate": 0.48148148148148145,
      "bias_score": 0.4192901234567901,
      "bias_score_details": {
        "n_biased": 3120,
        "n_counter_biased": 403,
        "n_unknown": 2957,
        "n_valid": 6480
      },
      "culture_accuracy": 0.855533261305757,
      "culture_total": 6480,
      "culture_valid": 6479,
      "culture_correct": 5543,
      "culture_incorrect": 259,
      "culture_unknown": 677,
      "per_category_bias": {
        "age": {
          "accuracy": 0.7824074074074074,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.15925925925925927,
          "n_biased": 407,
          "n_counter_biased": 63,
          "n_unknown": 1690
        },
        "gender": {
          "accuracy": 0.16898148148148148,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6976851851851852,
          "n_biased": 1651,
          "n_counter_biased": 144,
          "n_unknown": 365
        },
        "hierarchical_relationship": {
          "accuracy": 0.41759259259259257,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.4009259259259259,
          "n_biased": 1062,
          "n_counter_biased": 196,
          "n_unknown": 902
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6777777777777778,
          "total": 2160,
          "correct": 1464
        },
        "gender": {
          "accuracy": 0.9814814814814815,
          "total": 2160,
          "correct": 2120
        },
        "hierarchical_relationship": {
          "accuracy": 0.9069444444444444,
          "total": 2160,
          "correct": 1959
        }
      },
      "invalid_predictions": 1,
      "invalid_rate": 7.716049382716049e-05
    },
    "averaged": {
      "overall_accuracy": 0.6959549073396495,
      "bias_accuracy": 0.5326131687242798,
      "bias_rate": 0.4180041152263374,
      "bias_score": 0.3688339747291937,
      "bias_score_details": {
        "n_biased": 2708.6666666666665,
        "n_counter_biased": 319.0,
        "n_unknown": 3451.3333333333335,
        "n_valid": 6479.0
      },
      "culture_accuracy": 0.8592580731467631,
      "culture_total": 6480.0,
      "culture_valid": 6477.666666666667,
      "culture_correct": 5566.0,
      "culture_incorrect": 248.0,
      "culture_unknown": 663.6666666666666,
      "per_category_bias": {
        "age": {
          "accuracy": 0.7913580246913581,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.15339506172839507,
          "n_biased": 391.0,
          "n_counter_biased": 59.666666666666664,
          "n_unknown": 1709.3333333333333
        },
        "gender": {
          "accuracy": 0.30648148148148147,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.6033950617283951,
          "n_biased": 1400.6666666666667,
          "n_counter_biased": 97.33333333333333,
          "n_unknown": 662.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.5002249351808925,
          "total": 2160.0,
          "valid": 2159.0,
          "bias_score": 0.3496780507907073,
          "n_biased": 917.0,
          "n_counter_biased": 162.0,
          "n_unknown": 1080.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6813271604938272,
          "total": 2160.0,
          "correct": 1471.6666666666667
        },
        "gender": {
          "accuracy": 0.9868827160493826,
          "total": 2160.0,
          "correct": 2131.6666666666665
        },
        "hierarchical_relationship": {
          "accuracy": 0.908641975308642,
          "total": 2160.0,
          "correct": 1962.6666666666667
        }
      },
      "invalid_predictions": 3.3333333333333335,
      "invalid_rate": 0.000257201646090535
    }
  }
}