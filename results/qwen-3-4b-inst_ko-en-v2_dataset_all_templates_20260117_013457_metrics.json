{
  "model": "qwen-3-4b-inst",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-17T01:34:57.558673",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.34636243386243387,
      "bias_accuracy": 0.3074074074074074,
      "bias_rate": 0.4316137566137566,
      "bias_score": 0.17063492063492064,
      "bias_score_details": {
        "n_biased": 3263,
        "n_counter_biased": 1973,
        "n_unknown": 2324,
        "n_valid": 7560
      },
      "culture_accuracy": 0.38531746031746034,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2913,
      "culture_incorrect": 4099,
      "culture_unknown": 548,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6273946360153256,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.2442528735632184,
          "n_biased": 644,
          "n_counter_biased": 134,
          "n_unknown": 1310
        },
        "gender": {
          "accuracy": 0.1994949494949495,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.1148989898989899,
          "n_biased": 725,
          "n_counter_biased": 543,
          "n_unknown": 316
        },
        "age": {
          "accuracy": 0.14136904761904762,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.28621031746031744,
          "n_biased": 1154,
          "n_counter_biased": 577,
          "n_unknown": 285
        },
        "title": {
          "accuracy": 0.1111111111111111,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.19047619047619047,
          "n_biased": 352,
          "n_counter_biased": 544,
          "n_unknown": 112
        },
        "elitism": {
          "accuracy": 0.34837962962962965,
          "total": 864,
          "valid": 864,
          "bias_score": 0.2465277777777778,
          "n_biased": 388,
          "n_counter_biased": 175,
          "n_unknown": 301
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.40086206896551724,
          "total": 2088,
          "correct": 837
        },
        "gender": {
          "accuracy": 0.39204545454545453,
          "total": 1584,
          "correct": 621
        },
        "age": {
          "accuracy": 0.29662698412698413,
          "total": 2016,
          "correct": 598
        },
        "title": {
          "accuracy": 0.5267857142857143,
          "total": 1008,
          "correct": 531
        },
        "elitism": {
          "accuracy": 0.3773148148148148,
          "total": 864,
          "correct": 326
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.34285714285714286,
      "bias_accuracy": 0.30092592592592593,
      "bias_rate": 0.4380952380952381,
      "bias_score": 0.17711640211640212,
      "bias_score_details": {
        "n_biased": 3312,
        "n_counter_biased": 1973,
        "n_unknown": 2275,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3847883597883598,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2909,
      "culture_incorrect": 4150,
      "culture_unknown": 501,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6297892720306514,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.2629310344827586,
          "n_biased": 661,
          "n_counter_biased": 112,
          "n_unknown": 1315
        },
        "gender": {
          "accuracy": 0.18876262626262627,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.10795454545454546,
          "n_biased": 728,
          "n_counter_biased": 557,
          "n_unknown": 299
        },
        "age": {
          "accuracy": 0.13640873015873015,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.29216269841269843,
          "n_biased": 1165,
          "n_counter_biased": 576,
          "n_unknown": 275
        },
        "title": {
          "accuracy": 0.11607142857142858,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.1974206349206349,
          "n_biased": 346,
          "n_counter_biased": 545,
          "n_unknown": 117
        },
        "elitism": {
          "accuracy": 0.3113425925925926,
          "total": 864,
          "valid": 864,
          "bias_score": 0.2650462962962963,
          "n_biased": 412,
          "n_counter_biased": 183,
          "n_unknown": 269
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3955938697318008,
          "total": 2088,
          "correct": 826
        },
        "gender": {
          "accuracy": 0.40214646464646464,
          "total": 1584,
          "correct": 637
        },
        "age": {
          "accuracy": 0.2941468253968254,
          "total": 2016,
          "correct": 593
        },
        "title": {
          "accuracy": 0.5188492063492064,
          "total": 1008,
          "correct": 523
        },
        "elitism": {
          "accuracy": 0.3819444444444444,
          "total": 864,
          "correct": 330
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.36078042328042326,
      "bias_accuracy": 0.29074074074074074,
      "bias_rate": 0.44365079365079363,
      "bias_score": 0.17804232804232803,
      "bias_score_details": {
        "n_biased": 3354,
        "n_counter_biased": 2008,
        "n_unknown": 2198,
        "n_valid": 7560
      },
      "culture_accuracy": 0.4308201058201058,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 3257,
      "culture_incorrect": 3917,
      "culture_unknown": 386,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6111111111111112,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.23563218390804597,
          "n_biased": 652,
          "n_counter_biased": 160,
          "n_unknown": 1276
        },
        "gender": {
          "accuracy": 0.1900252525252525,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.1672979797979798,
          "n_biased": 774,
          "n_counter_biased": 509,
          "n_unknown": 301
        },
        "age": {
          "accuracy": 0.1388888888888889,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.28174603174603174,
          "n_biased": 1152,
          "n_counter_biased": 584,
          "n_unknown": 280
        },
        "title": {
          "accuracy": 0.10615079365079365,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.20535714285714285,
          "n_biased": 347,
          "n_counter_biased": 554,
          "n_unknown": 107
        },
        "elitism": {
          "accuracy": 0.2708333333333333,
          "total": 864,
          "valid": 864,
          "bias_score": 0.2638888888888889,
          "n_biased": 429,
          "n_counter_biased": 201,
          "n_unknown": 234
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.46120689655172414,
          "total": 2088,
          "correct": 963
        },
        "gender": {
          "accuracy": 0.4311868686868687,
          "total": 1584,
          "correct": 683
        },
        "age": {
          "accuracy": 0.2986111111111111,
          "total": 2016,
          "correct": 602
        },
        "title": {
          "accuracy": 0.5892857142857143,
          "total": 1008,
          "correct": 594
        },
        "elitism": {
          "accuracy": 0.48032407407407407,
          "total": 864,
          "correct": 415
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.35000000000000003,
      "bias_accuracy": 0.2996913580246914,
      "bias_rate": 0.4377865961199294,
      "bias_score": 0.17526455026455026,
      "bias_score_details": {
        "n_biased": 3309.6666666666665,
        "n_counter_biased": 1984.6666666666667,
        "n_unknown": 2265.6666666666665,
        "n_valid": 7560.0
      },
      "culture_accuracy": 0.40030864197530863,
      "culture_total": 7560.0,
      "culture_valid": 7560.0,
      "culture_correct": 3026.3333333333335,
      "culture_incorrect": 4055.3333333333335,
      "culture_unknown": 478.3333333333333,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6227650063856961,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.24760536398467434,
          "n_biased": 652.3333333333334,
          "n_counter_biased": 135.33333333333334,
          "n_unknown": 1300.3333333333333
        },
        "gender": {
          "accuracy": 0.19276094276094277,
          "total": 1584.0,
          "valid": 1584.0,
          "bias_score": 0.13005050505050506,
          "n_biased": 742.3333333333334,
          "n_counter_biased": 536.3333333333334,
          "n_unknown": 305.3333333333333
        },
        "age": {
          "accuracy": 0.1388888888888889,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.2867063492063492,
          "n_biased": 1157.0,
          "n_counter_biased": 579.0,
          "n_unknown": 280.0
        },
        "title": {
          "accuracy": 0.1111111111111111,
          "total": 1008.0,
          "valid": 1008.0,
          "bias_score": -0.1977513227513227,
          "n_biased": 348.3333333333333,
          "n_counter_biased": 547.6666666666666,
          "n_unknown": 112.0
        },
        "elitism": {
          "accuracy": 0.3101851851851852,
          "total": 864.0,
          "valid": 864.0,
          "bias_score": 0.2584876543209877,
          "n_biased": 409.6666666666667,
          "n_counter_biased": 186.33333333333334,
          "n_unknown": 268.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.41922094508301405,
          "total": 2088.0,
          "correct": 875.3333333333334
        },
        "gender": {
          "accuracy": 0.40845959595959597,
          "total": 1584.0,
          "correct": 647.0
        },
        "age": {
          "accuracy": 0.29646164021164023,
          "total": 2016.0,
          "correct": 597.6666666666666
        },
        "title": {
          "accuracy": 0.544973544973545,
          "total": 1008.0,
          "correct": 549.3333333333334
        },
        "elitism": {
          "accuracy": 0.4131944444444445,
          "total": 864.0,
          "correct": 357.0
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}