{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/ko-zh-v2_dataset.csv",
  "timestamp": "2026-01-02T21:38:46.341180",
  "metrics": {
    "overall_accuracy": 0.4126984126984127,
    "bias_accuracy": 0.42698412698412697,
    "bias_rate": 0.38875661375661374,
    "bias_score": 0.2044973544973545,
    "bias_score_details": {
      "n_biased": 2939,
      "n_counter_biased": 1393,
      "n_unknown": 3228,
      "n_valid": 7560
    },
    "culture_accuracy": 0.3984126984126984,
    "culture_total": 7560,
    "culture_valid": 7560,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.5306513409961686,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.3505747126436782,
        "n_biased": 856,
        "n_counter_biased": 124,
        "n_unknown": 1108
      },
      "gender": {
        "accuracy": 0.3106060606060606,
        "total": 1584,
        "valid": 1584,
        "bias_score": 0.2563131313131313,
        "n_biased": 749,
        "n_counter_biased": 343,
        "n_unknown": 492
      },
      "age": {
        "accuracy": 0.31646825396825395,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.26587301587301587,
        "n_biased": 957,
        "n_counter_biased": 421,
        "n_unknown": 638
      },
      "title": {
        "accuracy": 0.2390873015873016,
        "total": 1008,
        "valid": 1008,
        "bias_score": -0.20932539682539683,
        "n_biased": 278,
        "n_counter_biased": 489,
        "n_unknown": 241
      },
      "elitism": {
        "accuracy": 0.8668981481481481,
        "total": 864,
        "valid": 864,
        "bias_score": 0.09606481481481481,
        "n_biased": 99,
        "n_counter_biased": 16,
        "n_unknown": 749
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.39415708812260536,
        "total": 2088,
        "correct": 823
      },
      "gender": {
        "accuracy": 0.38446969696969696,
        "total": 1584,
        "correct": 609
      },
      "age": {
        "accuracy": 0.26884920634920634,
        "total": 2016,
        "correct": 542
      },
      "title": {
        "accuracy": 0.9136904761904762,
        "total": 1008,
        "correct": 921
      },
      "elitism": {
        "accuracy": 0.13541666666666666,
        "total": 864,
        "correct": 117
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}