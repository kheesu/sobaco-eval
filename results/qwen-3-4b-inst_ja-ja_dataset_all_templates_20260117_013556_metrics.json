{
  "model": "qwen-3-4b-inst",
  "dataset": "csv/ja-ja_dataset.csv",
  "timestamp": "2026-01-17T01:35:56.596467",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.42511713520749667,
      "bias_accuracy": 0.4581659973226238,
      "bias_rate": 0.38972556894243643,
      "bias_score": 0.23761713520749667,
      "bias_score_details": {
        "n_biased": 2329,
        "n_counter_biased": 909,
        "n_unknown": 2738,
        "n_valid": 5976
      },
      "culture_accuracy": 0.39206827309236947,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2343,
      "culture_incorrect": 2416,
      "culture_unknown": 1217,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5871647509578544,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.25191570881226055,
          "n_biased": 694,
          "n_counter_biased": 168,
          "n_unknown": 1226
        },
        "gender": {
          "accuracy": 0.32264957264957267,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.20512820512820512,
          "n_biased": 826,
          "n_counter_biased": 442,
          "n_unknown": 604
        },
        "age": {
          "accuracy": 0.4503968253968254,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.25297619047619047,
          "n_biased": 809,
          "n_counter_biased": 299,
          "n_unknown": 908
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.38409961685823757,
          "total": 2088,
          "correct": 802
        },
        "gender": {
          "accuracy": 0.42895299145299143,
          "total": 1872,
          "correct": 803
        },
        "age": {
          "accuracy": 0.36607142857142855,
          "total": 2016,
          "correct": 738
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.43540829986613117,
      "bias_accuracy": 0.4864457831325301,
      "bias_rate": 0.37232262382864795,
      "bias_score": 0.23109103078982596,
      "bias_score_details": {
        "n_biased": 2225,
        "n_counter_biased": 844,
        "n_unknown": 2907,
        "n_valid": 5976
      },
      "culture_accuracy": 0.38437081659973227,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2297,
      "culture_incorrect": 2458,
      "culture_unknown": 1221,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6398467432950191,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.25,
          "n_biased": 637,
          "n_counter_biased": 115,
          "n_unknown": 1336
        },
        "gender": {
          "accuracy": 0.3327991452991453,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.2045940170940171,
          "n_biased": 816,
          "n_counter_biased": 433,
          "n_unknown": 623
        },
        "age": {
          "accuracy": 0.47023809523809523,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.2361111111111111,
          "n_biased": 772,
          "n_counter_biased": 296,
          "n_unknown": 948
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3831417624521073,
          "total": 2088,
          "correct": 800
        },
        "gender": {
          "accuracy": 0.4166666666666667,
          "total": 1872,
          "correct": 780
        },
        "age": {
          "accuracy": 0.3556547619047619,
          "total": 2016,
          "correct": 717
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.36579651941097724,
      "bias_accuracy": 0.31977911646586343,
      "bias_rate": 0.46318607764390896,
      "bias_score": 0.2461512717536814,
      "bias_score_details": {
        "n_biased": 2768,
        "n_counter_biased": 1297,
        "n_unknown": 1911,
        "n_valid": 5976
      },
      "culture_accuracy": 0.41181392235609104,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2461,
      "culture_incorrect": 2903,
      "culture_unknown": 612,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.4818007662835249,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.23659003831417624,
          "n_biased": 788,
          "n_counter_biased": 294,
          "n_unknown": 1006
        },
        "gender": {
          "accuracy": 0.17467948717948717,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.2676282051282051,
          "n_biased": 1023,
          "n_counter_biased": 522,
          "n_unknown": 327
        },
        "age": {
          "accuracy": 0.2867063492063492,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.2361111111111111,
          "n_biased": 957,
          "n_counter_biased": 481,
          "n_unknown": 578
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.38936781609195403,
          "total": 2088,
          "correct": 813
        },
        "gender": {
          "accuracy": 0.47702991452991456,
          "total": 1872,
          "correct": 893
        },
        "age": {
          "accuracy": 0.37450396825396826,
          "total": 2016,
          "correct": 755
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.4087739848282017,
      "bias_accuracy": 0.4214636323070058,
      "bias_rate": 0.4084114234716645,
      "bias_score": 0.23828647925033467,
      "bias_score_details": {
        "n_biased": 2440.6666666666665,
        "n_counter_biased": 1016.6666666666666,
        "n_unknown": 2518.6666666666665,
        "n_valid": 5976.0
      },
      "culture_accuracy": 0.3960843373493976,
      "culture_total": 5976.0,
      "culture_valid": 5976.0,
      "culture_correct": 2367.0,
      "culture_incorrect": 2592.3333333333335,
      "culture_unknown": 1016.6666666666666,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.569604086845466,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.24616858237547892,
          "n_biased": 706.3333333333334,
          "n_counter_biased": 192.33333333333334,
          "n_unknown": 1189.3333333333333
        },
        "gender": {
          "accuracy": 0.27670940170940167,
          "total": 1872.0,
          "valid": 1872.0,
          "bias_score": 0.2257834757834758,
          "n_biased": 888.3333333333334,
          "n_counter_biased": 465.6666666666667,
          "n_unknown": 518.0
        },
        "age": {
          "accuracy": 0.40244708994708994,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.24173280423280422,
          "n_biased": 846.0,
          "n_counter_biased": 358.6666666666667,
          "n_unknown": 811.3333333333334
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.38553639846743293,
          "total": 2088.0,
          "correct": 805.0
        },
        "gender": {
          "accuracy": 0.4408831908831909,
          "total": 1872.0,
          "correct": 825.3333333333334
        },
        "age": {
          "accuracy": 0.36541005291005285,
          "total": 2016.0,
          "correct": 736.6666666666666
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}