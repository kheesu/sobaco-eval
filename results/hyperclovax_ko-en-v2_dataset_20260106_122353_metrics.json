{
  "model": "hyperclovax",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-06T12:23:53.869659",
  "metrics": {
    "overall_accuracy": 0.4064153439153439,
    "bias_accuracy": 0.5477513227513228,
    "bias_rate": 0.2361111111111111,
    "bias_score": 0.019973544973544973,
    "bias_score_details": {
      "n_biased": 1785,
      "n_counter_biased": 1634,
      "n_unknown": 4141,
      "n_valid": 7560
    },
    "culture_accuracy": 0.2650793650793651,
    "culture_total": 7560,
    "culture_valid": 7560,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.46695402298850575,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.020593869731800767,
        "n_biased": 578,
        "n_counter_biased": 535,
        "n_unknown": 975
      },
      "gender": {
        "accuracy": 0.7234848484848485,
        "total": 1584,
        "valid": 1584,
        "bias_score": -0.008838383838383838,
        "n_biased": 212,
        "n_counter_biased": 226,
        "n_unknown": 1146
      },
      "age": {
        "accuracy": 0.5138888888888888,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.0882936507936508,
        "n_biased": 579,
        "n_counter_biased": 401,
        "n_unknown": 1036
      },
      "title": {
        "accuracy": 0.4930555555555556,
        "total": 1008,
        "valid": 1008,
        "bias_score": -0.05853174603174603,
        "n_biased": 226,
        "n_counter_biased": 285,
        "n_unknown": 497
      },
      "elitism": {
        "accuracy": 0.5636574074074074,
        "total": 864,
        "valid": 864,
        "bias_score": 0.003472222222222222,
        "n_biased": 190,
        "n_counter_biased": 187,
        "n_unknown": 487
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.34913793103448276,
        "total": 2088,
        "correct": 729
      },
      "gender": {
        "accuracy": 0.1553030303030303,
        "total": 1584,
        "correct": 246
      },
      "age": {
        "accuracy": 0.24454365079365079,
        "total": 2016,
        "correct": 493
      },
      "title": {
        "accuracy": 0.2222222222222222,
        "total": 1008,
        "correct": 224
      },
      "elitism": {
        "accuracy": 0.3611111111111111,
        "total": 864,
        "correct": 312
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}