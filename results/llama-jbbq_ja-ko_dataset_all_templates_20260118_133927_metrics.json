{
  "model": "llama-jbbq",
  "dataset": "csv/ja-ko_dataset.csv",
  "timestamp": "2026-01-18T13:39:27.419227",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.4970716198125837,
      "bias_accuracy": 0.7909973226238286,
      "bias_rate": 0.09354082998661312,
      "bias_score": -0.021921017402945112,
      "bias_score_details": {
        "n_biased": 559,
        "n_counter_biased": 690,
        "n_unknown": 4727,
        "n_valid": 5976
      },
      "culture_accuracy": 0.2031459170013387,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 1214,
      "culture_incorrect": 1019,
      "culture_unknown": 3743,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.8084291187739464,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.0603448275862069,
          "n_biased": 263,
          "n_counter_biased": 137,
          "n_unknown": 1688
        },
        "gender": {
          "accuracy": 0.6324786324786325,
          "total": 1872,
          "valid": 1872,
          "bias_score": -0.11752136752136752,
          "n_biased": 234,
          "n_counter_biased": 454,
          "n_unknown": 1184
        },
        "age": {
          "accuracy": 0.9201388888888888,
          "total": 2016,
          "valid": 2016,
          "bias_score": -0.018353174603174604,
          "n_biased": 62,
          "n_counter_biased": 99,
          "n_unknown": 1855
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2164750957854406,
          "total": 2088,
          "correct": 452
        },
        "gender": {
          "accuracy": 0.07264957264957266,
          "total": 1872,
          "correct": 136
        },
        "age": {
          "accuracy": 0.310515873015873,
          "total": 2016,
          "correct": 626
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.4795013386880857,
      "bias_accuracy": 0.6910977242302544,
      "bias_rate": 0.14508032128514056,
      "bias_score": -0.018741633199464525,
      "bias_score_details": {
        "n_biased": 867,
        "n_counter_biased": 979,
        "n_unknown": 4130,
        "n_valid": 5976
      },
      "culture_accuracy": 0.267904953145917,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 1601,
      "culture_incorrect": 1379,
      "culture_unknown": 2996,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6819923371647509,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.09482758620689655,
          "n_biased": 431,
          "n_counter_biased": 233,
          "n_unknown": 1424
        },
        "gender": {
          "accuracy": 0.5272435897435898,
          "total": 1872,
          "valid": 1872,
          "bias_score": -0.1362179487179487,
          "n_biased": 315,
          "n_counter_biased": 570,
          "n_unknown": 987
        },
        "age": {
          "accuracy": 0.8526785714285714,
          "total": 2016,
          "valid": 2016,
          "bias_score": -0.027281746031746032,
          "n_biased": 121,
          "n_counter_biased": 176,
          "n_unknown": 1719
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2959770114942529,
          "total": 2088,
          "correct": 618
        },
        "gender": {
          "accuracy": 0.1362179487179487,
          "total": 1872,
          "correct": 255
        },
        "age": {
          "accuracy": 0.3611111111111111,
          "total": 2016,
          "correct": 728
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.5102911646586346,
      "bias_accuracy": 0.8887215528781793,
      "bias_rate": 0.06157965194109773,
      "bias_score": 0.011880856760374833,
      "bias_score_details": {
        "n_biased": 368,
        "n_counter_biased": 297,
        "n_unknown": 5311,
        "n_valid": 5976
      },
      "culture_accuracy": 0.1318607764390897,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 788,
      "culture_incorrect": 451,
      "culture_unknown": 4737,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.8764367816091954,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.10919540229885058,
          "n_biased": 243,
          "n_counter_biased": 15,
          "n_unknown": 1830
        },
        "gender": {
          "accuracy": 0.7863247863247863,
          "total": 1872,
          "valid": 1872,
          "bias_score": -0.08547008547008547,
          "n_biased": 120,
          "n_counter_biased": 280,
          "n_unknown": 1472
        },
        "age": {
          "accuracy": 0.9965277777777778,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.001488095238095238,
          "n_biased": 5,
          "n_counter_biased": 2,
          "n_unknown": 2009
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.1925287356321839,
          "total": 2088,
          "correct": 402
        },
        "gender": {
          "accuracy": 0.0,
          "total": 1872,
          "correct": 0
        },
        "age": {
          "accuracy": 0.19146825396825398,
          "total": 2016,
          "correct": 386
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.4956213743864346,
      "bias_accuracy": 0.7902721999107541,
      "bias_rate": 0.10006693440428381,
      "bias_score": -0.009593931280678267,
      "bias_score_details": {
        "n_biased": 598.0,
        "n_counter_biased": 655.3333333333334,
        "n_unknown": 4722.666666666667,
        "n_valid": 5976.0
      },
      "culture_accuracy": 0.2009705488621151,
      "culture_total": 5976.0,
      "culture_valid": 5976.0,
      "culture_correct": 1201.0,
      "culture_incorrect": 949.6666666666666,
      "culture_unknown": 3825.3333333333335,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7889527458492976,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.08812260536398468,
          "n_biased": 312.3333333333333,
          "n_counter_biased": 128.33333333333334,
          "n_unknown": 1647.3333333333333
        },
        "gender": {
          "accuracy": 0.6486823361823362,
          "total": 1872.0,
          "valid": 1872.0,
          "bias_score": -0.11306980056980058,
          "n_biased": 223.0,
          "n_counter_biased": 434.6666666666667,
          "n_unknown": 1214.3333333333333
        },
        "age": {
          "accuracy": 0.9231150793650794,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": -0.014715608465608467,
          "n_biased": 62.666666666666664,
          "n_counter_biased": 92.33333333333333,
          "n_unknown": 1861.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.23499361430395913,
          "total": 2088.0,
          "correct": 490.6666666666667
        },
        "gender": {
          "accuracy": 0.06962250712250712,
          "total": 1872.0,
          "correct": 130.33333333333334
        },
        "age": {
          "accuracy": 0.28769841269841273,
          "total": 2016.0,
          "correct": 580.0
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}