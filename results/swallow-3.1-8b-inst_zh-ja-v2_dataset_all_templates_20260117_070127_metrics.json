{
  "model": "swallow-3.1-8b-inst",
  "dataset": "csv/zh-ja-v2_dataset.csv",
  "timestamp": "2026-01-17T07:01:27.324868",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.5123456790123457,
      "bias_accuracy": 0.358641975308642,
      "bias_rate": 0.4015432098765432,
      "bias_score": 0.1617283950617284,
      "bias_score_details": {
        "n_biased": 2602,
        "n_counter_biased": 1554,
        "n_unknown": 2324,
        "n_valid": 6480
      },
      "culture_accuracy": 0.6660493827160494,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4316,
      "culture_incorrect": 1325,
      "culture_unknown": 839,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9712962962962963,
          "total": 2160,
          "valid": 2160,
          "bias_score": -0.019444444444444445,
          "n_biased": 10,
          "n_counter_biased": 52,
          "n_unknown": 2098
        },
        "gender": {
          "accuracy": 0.10324074074074074,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.24027777777777778,
          "n_biased": 1228,
          "n_counter_biased": 709,
          "n_unknown": 223
        },
        "hierarchical_relationship": {
          "accuracy": 0.001388888888888889,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.26435185185185184,
          "n_biased": 1364,
          "n_counter_biased": 793,
          "n_unknown": 3
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.562037037037037,
          "total": 2160,
          "correct": 1214
        },
        "gender": {
          "accuracy": 0.9726851851851852,
          "total": 2160,
          "correct": 2101
        },
        "hierarchical_relationship": {
          "accuracy": 0.4634259259259259,
          "total": 2160,
          "correct": 1001
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.523070987654321,
      "bias_accuracy": 0.37469135802469133,
      "bias_rate": 0.3998456790123457,
      "bias_score": 0.1743827160493827,
      "bias_score_details": {
        "n_biased": 2591,
        "n_counter_biased": 1461,
        "n_unknown": 2428,
        "n_valid": 6480
      },
      "culture_accuracy": 0.6714506172839506,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4351,
      "culture_incorrect": 1311,
      "culture_unknown": 818,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9847222222222223,
          "total": 2160,
          "valid": 2160,
          "bias_score": -0.009722222222222222,
          "n_biased": 6,
          "n_counter_biased": 27,
          "n_unknown": 2127
        },
        "gender": {
          "accuracy": 0.13657407407407407,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.2569444444444444,
          "n_biased": 1210,
          "n_counter_biased": 655,
          "n_unknown": 295
        },
        "hierarchical_relationship": {
          "accuracy": 0.002777777777777778,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.2759259259259259,
          "n_biased": 1375,
          "n_counter_biased": 779,
          "n_unknown": 6
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5689814814814815,
          "total": 2160,
          "correct": 1229
        },
        "gender": {
          "accuracy": 0.9736111111111111,
          "total": 2160,
          "correct": 2103
        },
        "hierarchical_relationship": {
          "accuracy": 0.47175925925925927,
          "total": 2160,
          "correct": 1019
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.5232827970297029,
      "bias_accuracy": 0.33132716049382716,
      "bias_rate": 0.43256172839506174,
      "bias_score": 0.20238833746898263,
      "bias_score_details": {
        "n_biased": 2803,
        "n_counter_biased": 1498,
        "n_unknown": 2147,
        "n_valid": 6448
      },
      "culture_accuracy": 0.7126543209876544,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4618,
      "culture_incorrect": 1345,
      "culture_unknown": 517,
      "per_category_bias": {
        "age": {
          "accuracy": 0.918233082706767,
          "total": 2160,
          "valid": 2128,
          "bias_score": 0.005639097744360902,
          "n_biased": 93,
          "n_counter_biased": 81,
          "n_unknown": 1954
        },
        "gender": {
          "accuracy": 0.08935185185185185,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.19398148148148148,
          "n_biased": 1193,
          "n_counter_biased": 774,
          "n_unknown": 193
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.4046296296296296,
          "n_biased": 1517,
          "n_counter_biased": 643,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6972222222222222,
          "total": 2160,
          "correct": 1506
        },
        "gender": {
          "accuracy": 0.9587962962962963,
          "total": 2160,
          "correct": 2071
        },
        "hierarchical_relationship": {
          "accuracy": 0.48194444444444445,
          "total": 2160,
          "correct": 1041
        }
      },
      "invalid_predictions": 32,
      "invalid_rate": 0.0024691358024691358
    },
    "averaged": {
      "overall_accuracy": 0.5195664878987899,
      "bias_accuracy": 0.3548868312757201,
      "bias_rate": 0.4113168724279836,
      "bias_score": 0.1794998161933646,
      "bias_score_details": {
        "n_biased": 2665.3333333333335,
        "n_counter_biased": 1504.3333333333333,
        "n_unknown": 2299.6666666666665,
        "n_valid": 6469.333333333333
      },
      "culture_accuracy": 0.6833847736625515,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 4428.333333333333,
      "culture_incorrect": 1327.0,
      "culture_unknown": 724.6666666666666,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9580838670750952,
          "total": 2160.0,
          "valid": 2149.3333333333335,
          "bias_score": -0.007842522974101922,
          "n_biased": 36.333333333333336,
          "n_counter_biased": 53.333333333333336,
          "n_unknown": 2059.6666666666665
        },
        "gender": {
          "accuracy": 0.10972222222222222,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.23040123456790126,
          "n_biased": 1210.3333333333333,
          "n_counter_biased": 712.6666666666666,
          "n_unknown": 237.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.001388888888888889,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.3149691358024691,
          "n_biased": 1418.6666666666667,
          "n_counter_biased": 738.3333333333334,
          "n_unknown": 3.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6094135802469135,
          "total": 2160.0,
          "correct": 1316.3333333333333
        },
        "gender": {
          "accuracy": 0.9683641975308642,
          "total": 2160.0,
          "correct": 2091.6666666666665
        },
        "hierarchical_relationship": {
          "accuracy": 0.47237654320987654,
          "total": 2160.0,
          "correct": 1020.3333333333334
        }
      },
      "invalid_predictions": 10.666666666666666,
      "invalid_rate": 0.0008230452674897119
    }
  }
}