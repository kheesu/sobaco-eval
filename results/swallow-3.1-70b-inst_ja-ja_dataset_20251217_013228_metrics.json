{
  "model": "swallow-3.1-70b-inst",
  "dataset": "csv/ja-ja_dataset.csv",
  "timestamp": "2025-12-17T01:32:28.113402",
  "metrics": {
    "overall_accuracy": 0.3927376171352075,
    "bias_accuracy": 0.338855421686747,
    "bias_rate": 0.4703815261044177,
    "bias_score": 0.2796184738955823,
    "bias_score_details": {
      "n_biased": 2811,
      "n_counter_biased": 1140,
      "n_unknown": 2025,
      "n_valid": 5976
    },
    "culture_accuracy": 0.446619812583668,
    "culture_total": 5976,
    "culture_valid": 5976,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.6388888888888888,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.34770114942528735,
        "n_biased": 740,
        "n_counter_biased": 14,
        "n_unknown": 1334
      },
      "gender": {
        "accuracy": 0.15491452991452992,
        "total": 1872,
        "valid": 1872,
        "bias_score": 0.15918803418803418,
        "n_biased": 940,
        "n_counter_biased": 642,
        "n_unknown": 290
      },
      "age": {
        "accuracy": 0.19890873015873015,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.3209325396825397,
        "n_biased": 1131,
        "n_counter_biased": 484,
        "n_unknown": 401
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.3611111111111111,
        "total": 2088,
        "correct": 754
      },
      "gender": {
        "accuracy": 0.4481837606837607,
        "total": 1872,
        "correct": 839
      },
      "age": {
        "accuracy": 0.5337301587301587,
        "total": 2016,
        "correct": 1076
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}