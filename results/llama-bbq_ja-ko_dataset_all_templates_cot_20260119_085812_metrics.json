{
  "model": "llama-bbq",
  "dataset": "csv/ja-ko_dataset.csv",
  "timestamp": "2026-01-19T08:58:12.055507",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.33229259589652094,
      "bias_accuracy": 0.13671352074966533,
      "bias_rate": 0.4882864792503347,
      "bias_score": 0.20678396517322692,
      "bias_score_details": {
        "n_biased": 2918,
        "n_counter_biased": 1778,
        "n_unknown": 817,
        "n_valid": 5513
      },
      "culture_accuracy": 0.510444093382482,
      "culture_total": 5976,
      "culture_valid": 5697,
      "culture_correct": 2908,
      "culture_incorrect": 2042,
      "culture_unknown": 747,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.14816810344827586,
          "total": 2088,
          "valid": 1856,
          "bias_score": 0.3432112068965517,
          "n_biased": 1109,
          "n_counter_biased": 472,
          "n_unknown": 275
        },
        "gender": {
          "accuracy": 0.09628378378378379,
          "total": 1872,
          "valid": 1776,
          "bias_score": 0.13795045045045046,
          "n_biased": 925,
          "n_counter_biased": 680,
          "n_unknown": 171
        },
        "age": {
          "accuracy": 0.1972355130249867,
          "total": 2016,
          "valid": 1881,
          "bias_score": 0.1371610845295056,
          "n_biased": 884,
          "n_counter_biased": 626,
          "n_unknown": 371
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4425287356321839,
          "total": 2088,
          "correct": 924
        },
        "gender": {
          "accuracy": 0.5192307692307693,
          "total": 1872,
          "correct": 972
        },
        "age": {
          "accuracy": 0.501984126984127,
          "total": 2016,
          "correct": 1012
        }
      },
      "invalid_predictions": 742,
      "invalid_rate": 0.06208165997322624
    },
    "template_2": {
      "overall_accuracy": 0.33494633116311806,
      "bias_accuracy": 0.19009370816599733,
      "bias_rate": 0.4380856760374833,
      "bias_score": 0.13199502575945993,
      "bias_score_details": {
        "n_biased": 2618,
        "n_counter_biased": 1875,
        "n_unknown": 1136,
        "n_valid": 5629
      },
      "culture_accuracy": 0.4655743419905874,
      "culture_total": 5976,
      "culture_valid": 5737,
      "culture_correct": 2671,
      "culture_incorrect": 1951,
      "culture_unknown": 1115,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.20103359173126614,
          "total": 2088,
          "valid": 1935,
          "bias_score": 0.19224806201550387,
          "n_biased": 959,
          "n_counter_biased": 587,
          "n_unknown": 389
        },
        "gender": {
          "accuracy": 0.15950920245398773,
          "total": 1872,
          "valid": 1793,
          "bias_score": 0.13106525376464026,
          "n_biased": 871,
          "n_counter_biased": 636,
          "n_unknown": 286
        },
        "age": {
          "accuracy": 0.2425039452919516,
          "total": 2016,
          "valid": 1901,
          "bias_score": 0.07154129405576012,
          "n_biased": 788,
          "n_counter_biased": 652,
          "n_unknown": 461
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.40900383141762453,
          "total": 2088,
          "correct": 854
        },
        "gender": {
          "accuracy": 0.4855769230769231,
          "total": 1872,
          "correct": 909
        },
        "age": {
          "accuracy": 0.4503968253968254,
          "total": 2016,
          "correct": 908
        }
      },
      "invalid_predictions": 586,
      "invalid_rate": 0.04902945113788487
    },
    "template_3": {
      "overall_accuracy": 0.3294952100221076,
      "bias_accuracy": 0.09889558232931726,
      "bias_rate": 0.49062918340026773,
      "bias_score": 0.2090279078479116,
      "bias_score_details": {
        "n_biased": 2932,
        "n_counter_biased": 1816,
        "n_unknown": 591,
        "n_valid": 5339
      },
      "culture_accuracy": 0.5412361790828348,
      "culture_total": 5976,
      "culture_valid": 5517,
      "culture_correct": 2986,
      "culture_incorrect": 2032,
      "culture_unknown": 499,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.14789915966386555,
          "total": 2088,
          "valid": 1785,
          "bias_score": 0.36022408963585434,
          "n_biased": 1082,
          "n_counter_biased": 439,
          "n_unknown": 264
        },
        "gender": {
          "accuracy": 0.07053469852104664,
          "total": 1872,
          "valid": 1758,
          "bias_score": 0.2036405005688282,
          "n_biased": 996,
          "n_counter_biased": 638,
          "n_unknown": 124
        },
        "age": {
          "accuracy": 0.11302895322939867,
          "total": 2016,
          "valid": 1796,
          "bias_score": 0.06403118040089087,
          "n_biased": 854,
          "n_counter_biased": 739,
          "n_unknown": 203
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.46599616858237547,
          "total": 2088,
          "correct": 973
        },
        "gender": {
          "accuracy": 0.5368589743589743,
          "total": 1872,
          "correct": 1005
        },
        "age": {
          "accuracy": 0.5,
          "total": 2016,
          "correct": 1008
        }
      },
      "invalid_predictions": 1096,
      "invalid_rate": 0.09170013386880857
    },
    "averaged": {
      "overall_accuracy": 0.33224471236058223,
      "bias_accuracy": 0.14190093708166,
      "bias_rate": 0.4723337795626952,
      "bias_score": 0.1826022995935328,
      "bias_score_details": {
        "n_biased": 2822.6666666666665,
        "n_counter_biased": 1823.0,
        "n_unknown": 848.0,
        "n_valid": 5493.666666666667
      },
      "culture_accuracy": 0.505751538151968,
      "culture_total": 5976.0,
      "culture_valid": 5650.333333333333,
      "culture_correct": 2855.0,
      "culture_incorrect": 2008.3333333333333,
      "culture_unknown": 787.0,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.16570028494780253,
          "total": 2088.0,
          "valid": 1858.6666666666667,
          "bias_score": 0.29856111951597003,
          "n_biased": 1050.0,
          "n_counter_biased": 499.3333333333333,
          "n_unknown": 309.3333333333333
        },
        "gender": {
          "accuracy": 0.10877589491960606,
          "total": 1872.0,
          "valid": 1775.6666666666667,
          "bias_score": 0.1575520682613063,
          "n_biased": 930.6666666666666,
          "n_counter_biased": 651.3333333333334,
          "n_unknown": 193.66666666666666
        },
        "age": {
          "accuracy": 0.1842561371821123,
          "total": 2016.0,
          "valid": 1859.3333333333333,
          "bias_score": 0.09091118632871886,
          "n_biased": 842.0,
          "n_counter_biased": 672.3333333333334,
          "n_unknown": 345.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.43917624521072796,
          "total": 2088.0,
          "correct": 917.0
        },
        "gender": {
          "accuracy": 0.5138888888888888,
          "total": 1872.0,
          "correct": 962.0
        },
        "age": {
          "accuracy": 0.48412698412698413,
          "total": 2016.0,
          "correct": 976.0
        }
      },
      "invalid_predictions": 808.0,
      "invalid_rate": 0.06760374832663989
    }
  }
}