{
  "model": "swallow-3.1-70b-inst",
  "dataset": "csv/ja-zh_dataset.csv",
  "timestamp": "2025-12-17T12:42:06.293916",
  "metrics": {
    "overall_accuracy": 0.33417001338688085,
    "bias_accuracy": 0.25301204819277107,
    "bias_rate": 0.5319611780455153,
    "bias_score": 0.3169344042838019,
    "bias_score_details": {
      "n_biased": 3179,
      "n_counter_biased": 1285,
      "n_unknown": 1512,
      "n_valid": 5976
    },
    "culture_accuracy": 0.41532797858099063,
    "culture_total": 5976,
    "culture_valid": 5976,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.5431034482758621,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.45689655172413796,
        "n_biased": 954,
        "n_counter_biased": 0,
        "n_unknown": 1134
      },
      "gender": {
        "accuracy": 0.06623931623931624,
        "total": 1872,
        "valid": 1872,
        "bias_score": 0.003205128205128205,
        "n_biased": 877,
        "n_counter_biased": 871,
        "n_unknown": 124
      },
      "age": {
        "accuracy": 0.1259920634920635,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.4632936507936508,
        "n_biased": 1348,
        "n_counter_biased": 414,
        "n_unknown": 254
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.39798850574712646,
        "total": 2088,
        "correct": 831
      },
      "gender": {
        "accuracy": 0.2783119658119658,
        "total": 1872,
        "correct": 521
      },
      "age": {
        "accuracy": 0.560515873015873,
        "total": 2016,
        "correct": 1130
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}