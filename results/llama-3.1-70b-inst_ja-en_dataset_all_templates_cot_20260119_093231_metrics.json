{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/ja-en_dataset.csv",
  "timestamp": "2026-01-19T09:32:31.973537",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.5819795679115726,
      "bias_accuracy": 0.6005689424364123,
      "bias_rate": 0.3070615796519411,
      "bias_score": 0.21469210174029452,
      "bias_score_details": {
        "n_biased": 1835,
        "n_counter_biased": 552,
        "n_unknown": 3589,
        "n_valid": 5976
      },
      "culture_accuracy": 0.5633590345289976,
      "culture_total": 5976,
      "culture_valid": 5966,
      "culture_correct": 3361,
      "culture_incorrect": 1132,
      "culture_unknown": 1473,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7346743295019157,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.1743295019157088,
          "n_biased": 459,
          "n_counter_biased": 95,
          "n_unknown": 1534
        },
        "gender": {
          "accuracy": 0.5064102564102564,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.15918803418803418,
          "n_biased": 611,
          "n_counter_biased": 313,
          "n_unknown": 948
        },
        "age": {
          "accuracy": 0.5491071428571429,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3080357142857143,
          "n_biased": 765,
          "n_counter_biased": 144,
          "n_unknown": 1107
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4664750957854406,
          "total": 2088,
          "correct": 974
        },
        "gender": {
          "accuracy": 0.5881410256410257,
          "total": 1872,
          "correct": 1101
        },
        "age": {
          "accuracy": 0.6378968253968254,
          "total": 2016,
          "correct": 1286
        }
      },
      "invalid_predictions": 10,
      "invalid_rate": 0.0008366800535475234
    },
    "template_2": {
      "overall_accuracy": 0.5741670852168089,
      "bias_accuracy": 0.572623828647925,
      "bias_rate": 0.3286479250334672,
      "bias_score": 0.22991967871485944,
      "bias_score_details": {
        "n_biased": 1964,
        "n_counter_biased": 590,
        "n_unknown": 3422,
        "n_valid": 5976
      },
      "culture_accuracy": 0.5757118927973199,
      "culture_total": 5976,
      "culture_valid": 5970,
      "culture_correct": 3437,
      "culture_incorrect": 1146,
      "culture_unknown": 1387,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7083333333333334,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.2092911877394636,
          "n_biased": 523,
          "n_counter_biased": 86,
          "n_unknown": 1479
        },
        "gender": {
          "accuracy": 0.5256410256410257,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.15064102564102563,
          "n_biased": 585,
          "n_counter_biased": 303,
          "n_unknown": 984
        },
        "age": {
          "accuracy": 0.4756944444444444,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.32490079365079366,
          "n_biased": 856,
          "n_counter_biased": 201,
          "n_unknown": 959
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.5095785440613027,
          "total": 2088,
          "correct": 1064
        },
        "gender": {
          "accuracy": 0.5817307692307693,
          "total": 1872,
          "correct": 1089
        },
        "age": {
          "accuracy": 0.6369047619047619,
          "total": 2016,
          "correct": 1284
        }
      },
      "invalid_predictions": 6,
      "invalid_rate": 0.000502008032128514
    },
    "template_3": {
      "overall_accuracy": 0.6483093404753933,
      "bias_accuracy": 0.6690093708165997,
      "bias_rate": 0.28480589022757696,
      "bias_score": 0.23862115127175368,
      "bias_score_details": {
        "n_biased": 1702,
        "n_counter_biased": 276,
        "n_unknown": 3998,
        "n_valid": 5976
      },
      "culture_accuracy": 0.6275954454119224,
      "culture_total": 5976,
      "culture_valid": 5972,
      "culture_correct": 3748,
      "culture_incorrect": 1080,
      "culture_unknown": 1144,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7868773946360154,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.19013409961685823,
          "n_biased": 421,
          "n_counter_biased": 24,
          "n_unknown": 1643
        },
        "gender": {
          "accuracy": 0.6020299145299145,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.23130341880341881,
          "n_biased": 589,
          "n_counter_biased": 156,
          "n_unknown": 1127
        },
        "age": {
          "accuracy": 0.6091269841269841,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.29563492063492064,
          "n_biased": 692,
          "n_counter_biased": 96,
          "n_unknown": 1228
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.5871647509578544,
          "total": 2088,
          "correct": 1226
        },
        "gender": {
          "accuracy": 0.6239316239316239,
          "total": 1872,
          "correct": 1168
        },
        "age": {
          "accuracy": 0.6716269841269841,
          "total": 2016,
          "correct": 1354
        }
      },
      "invalid_predictions": 4,
      "invalid_rate": 0.00033467202141900936
    },
    "averaged": {
      "overall_accuracy": 0.6014853312012582,
      "bias_accuracy": 0.6140673806336457,
      "bias_rate": 0.30683846497099504,
      "bias_score": 0.22774431057563585,
      "bias_score_details": {
        "n_biased": 1833.6666666666667,
        "n_counter_biased": 472.6666666666667,
        "n_unknown": 3669.6666666666665,
        "n_valid": 5976.0
      },
      "culture_accuracy": 0.5888887909127466,
      "culture_total": 5976.0,
      "culture_valid": 5969.333333333333,
      "culture_correct": 3515.3333333333335,
      "culture_incorrect": 1119.3333333333333,
      "culture_unknown": 1334.6666666666667,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7432950191570882,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.1912515964240102,
          "n_biased": 467.6666666666667,
          "n_counter_biased": 68.33333333333333,
          "n_unknown": 1552.0
        },
        "gender": {
          "accuracy": 0.5446937321937321,
          "total": 1872.0,
          "valid": 1872.0,
          "bias_score": 0.1803774928774929,
          "n_biased": 595.0,
          "n_counter_biased": 257.3333333333333,
          "n_unknown": 1019.6666666666666
        },
        "age": {
          "accuracy": 0.5446428571428572,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.30952380952380953,
          "n_biased": 771.0,
          "n_counter_biased": 147.0,
          "n_unknown": 1098.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.5210727969348659,
          "total": 2088.0,
          "correct": 1088.0
        },
        "gender": {
          "accuracy": 0.597934472934473,
          "total": 1872.0,
          "correct": 1119.3333333333333
        },
        "age": {
          "accuracy": 0.6488095238095237,
          "total": 2016.0,
          "correct": 1308.0
        }
      },
      "invalid_predictions": 6.666666666666667,
      "invalid_rate": 0.0005577867023650157
    }
  }
}