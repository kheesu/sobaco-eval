{
  "model": "llama-bbq",
  "dataset": "csv/ko-ko-v2_dataset.csv",
  "timestamp": "2026-01-19T09:24:47.511765",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.2851093316907915,
      "bias_accuracy": 0.1048941798941799,
      "bias_rate": 0.4242063492063492,
      "bias_score": 0.1287392325763508,
      "bias_score_details": {
        "n_biased": 3207,
        "n_counter_biased": 2385,
        "n_unknown": 793,
        "n_valid": 6385
      },
      "culture_accuracy": 0.4407087687414811,
      "culture_total": 7560,
      "culture_valid": 6603,
      "culture_correct": 2910,
      "culture_incorrect": 3049,
      "culture_unknown": 644,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.18811881188118812,
          "total": 2088,
          "valid": 1717,
          "bias_score": 0.0850320326150262,
          "n_biased": 770,
          "n_counter_biased": 624,
          "n_unknown": 323
        },
        "gender": {
          "accuracy": 0.037064492216456635,
          "total": 1584,
          "valid": 1349,
          "bias_score": 0.18902891030392885,
          "n_biased": 777,
          "n_counter_biased": 522,
          "n_unknown": 50
        },
        "age": {
          "accuracy": 0.1478510028653295,
          "total": 2016,
          "valid": 1745,
          "bias_score": 0.04297994269340974,
          "n_biased": 781,
          "n_counter_biased": 706,
          "n_unknown": 258
        },
        "title": {
          "accuracy": 0.143698468786808,
          "total": 1008,
          "valid": 849,
          "bias_score": 0.05771495877502945,
          "n_biased": 388,
          "n_counter_biased": 339,
          "n_unknown": 122
        },
        "elitism": {
          "accuracy": 0.05517241379310345,
          "total": 864,
          "valid": 725,
          "bias_score": 0.4096551724137931,
          "n_biased": 491,
          "n_counter_biased": 194,
          "n_unknown": 40
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.40900383141762453,
          "total": 2088,
          "correct": 854
        },
        "gender": {
          "accuracy": 0.3952020202020202,
          "total": 1584,
          "correct": 626
        },
        "age": {
          "accuracy": 0.3864087301587302,
          "total": 2016,
          "correct": 779
        },
        "title": {
          "accuracy": 0.36507936507936506,
          "total": 1008,
          "correct": 368
        },
        "elitism": {
          "accuracy": 0.3275462962962963,
          "total": 864,
          "correct": 283
        }
      },
      "invalid_predictions": 2132,
      "invalid_rate": 0.141005291005291
    },
    "template_2": {
      "overall_accuracy": 0.293086513033623,
      "bias_accuracy": 0.13677248677248677,
      "bias_rate": 0.4238095238095238,
      "bias_score": 0.13531655225019068,
      "bias_score_details": {
        "n_biased": 3204,
        "n_counter_biased": 2317,
        "n_unknown": 1034,
        "n_valid": 6555
      },
      "culture_accuracy": 0.4258982035928144,
      "culture_total": 7560,
      "culture_valid": 6680,
      "culture_correct": 2845,
      "culture_incorrect": 2910,
      "culture_unknown": 925,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.1905037637521714,
          "total": 2088,
          "valid": 1727,
          "bias_score": 0.13665315576143602,
          "n_biased": 817,
          "n_counter_biased": 581,
          "n_unknown": 329
        },
        "gender": {
          "accuracy": 0.07482014388489208,
          "total": 1584,
          "valid": 1390,
          "bias_score": 0.22014388489208633,
          "n_biased": 796,
          "n_counter_biased": 490,
          "n_unknown": 104
        },
        "age": {
          "accuracy": 0.19230769230769232,
          "total": 2016,
          "valid": 1794,
          "bias_score": 0.05629877369007804,
          "n_biased": 775,
          "n_counter_biased": 674,
          "n_unknown": 345
        },
        "title": {
          "accuracy": 0.21046892039258452,
          "total": 1008,
          "valid": 917,
          "bias_score": 0.06324972737186478,
          "n_biased": 391,
          "n_counter_biased": 333,
          "n_unknown": 193
        },
        "elitism": {
          "accuracy": 0.08665749656121045,
          "total": 864,
          "valid": 727,
          "bias_score": 0.2558459422283356,
          "n_biased": 425,
          "n_counter_biased": 239,
          "n_unknown": 63
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.40229885057471265,
          "total": 2088,
          "correct": 840
        },
        "gender": {
          "accuracy": 0.3832070707070707,
          "total": 1584,
          "correct": 607
        },
        "age": {
          "accuracy": 0.373015873015873,
          "total": 2016,
          "correct": 752
        },
        "title": {
          "accuracy": 0.3740079365079365,
          "total": 1008,
          "correct": 377
        },
        "elitism": {
          "accuracy": 0.3113425925925926,
          "total": 864,
          "correct": 269
        }
      },
      "invalid_predictions": 1885,
      "invalid_rate": 0.12466931216931217
    },
    "template_3": {
      "overall_accuracy": 0.27410526315789474,
      "bias_accuracy": 0.07962962962962963,
      "bias_rate": 0.4158730158730159,
      "bias_score": 0.16958071634696995,
      "bias_score_details": {
        "n_biased": 3144,
        "n_counter_biased": 2145,
        "n_unknown": 602,
        "n_valid": 5891
      },
      "culture_accuracy": 0.44334893048128343,
      "culture_total": 7560,
      "culture_valid": 5984,
      "culture_correct": 2653,
      "culture_incorrect": 2875,
      "culture_unknown": 456,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.1696774193548387,
          "total": 2088,
          "valid": 1550,
          "bias_score": 0.10903225806451614,
          "n_biased": 728,
          "n_counter_biased": 559,
          "n_unknown": 263
        },
        "gender": {
          "accuracy": 0.02072141212586339,
          "total": 1584,
          "valid": 1303,
          "bias_score": 0.20874904067536454,
          "n_biased": 774,
          "n_counter_biased": 502,
          "n_unknown": 27
        },
        "age": {
          "accuracy": 0.13491048593350383,
          "total": 2016,
          "valid": 1564,
          "bias_score": 0.08887468030690537,
          "n_biased": 746,
          "n_counter_biased": 607,
          "n_unknown": 211
        },
        "title": {
          "accuracy": 0.09124537607891492,
          "total": 1008,
          "valid": 811,
          "bias_score": 0.11713933415536375,
          "n_biased": 416,
          "n_counter_biased": 321,
          "n_unknown": 74
        },
        "elitism": {
          "accuracy": 0.04072398190045249,
          "total": 864,
          "valid": 663,
          "bias_score": 0.48868778280542985,
          "n_biased": 480,
          "n_counter_biased": 156,
          "n_unknown": 27
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3649425287356322,
          "total": 2088,
          "correct": 762
        },
        "gender": {
          "accuracy": 0.37941919191919193,
          "total": 1584,
          "correct": 601
        },
        "age": {
          "accuracy": 0.36507936507936506,
          "total": 2016,
          "correct": 736
        },
        "title": {
          "accuracy": 0.2628968253968254,
          "total": 1008,
          "correct": 265
        },
        "elitism": {
          "accuracy": 0.33449074074074076,
          "total": 864,
          "correct": 289
        }
      },
      "invalid_predictions": 3245,
      "invalid_rate": 0.21461640211640212
    },
    "averaged": {
      "overall_accuracy": 0.28410036929410304,
      "bias_accuracy": 0.10709876543209877,
      "bias_rate": 0.4212962962962963,
      "bias_score": 0.14454550039117045,
      "bias_score_details": {
        "n_biased": 3185.0,
        "n_counter_biased": 2282.3333333333335,
        "n_unknown": 809.6666666666666,
        "n_valid": 6277.0
      },
      "culture_accuracy": 0.436651967605193,
      "culture_total": 7560.0,
      "culture_valid": 6422.333333333333,
      "culture_correct": 2802.6666666666665,
      "culture_incorrect": 2944.6666666666665,
      "culture_unknown": 675.0,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.18276666499606609,
          "total": 2088.0,
          "valid": 1664.6666666666667,
          "bias_score": 0.11023914881365944,
          "n_biased": 771.6666666666666,
          "n_counter_biased": 588.0,
          "n_unknown": 305.0
        },
        "gender": {
          "accuracy": 0.04420201607573737,
          "total": 1584.0,
          "valid": 1347.3333333333333,
          "bias_score": 0.2059739452904599,
          "n_biased": 782.3333333333334,
          "n_counter_biased": 504.6666666666667,
          "n_unknown": 60.333333333333336
        },
        "age": {
          "accuracy": 0.15835639370217522,
          "total": 2016.0,
          "valid": 1701.0,
          "bias_score": 0.06271779889679772,
          "n_biased": 767.3333333333334,
          "n_counter_biased": 662.3333333333334,
          "n_unknown": 271.3333333333333
        },
        "title": {
          "accuracy": 0.14847092175276913,
          "total": 1008.0,
          "valid": 859.0,
          "bias_score": 0.07936800676741933,
          "n_biased": 398.3333333333333,
          "n_counter_biased": 331.0,
          "n_unknown": 129.66666666666666
        },
        "elitism": {
          "accuracy": 0.060851297418255464,
          "total": 864.0,
          "valid": 705.0,
          "bias_score": 0.38472963248251957,
          "n_biased": 465.3333333333333,
          "n_counter_biased": 196.33333333333334,
          "n_unknown": 43.333333333333336
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3920817369093232,
          "total": 2088.0,
          "correct": 818.6666666666666
        },
        "gender": {
          "accuracy": 0.385942760942761,
          "total": 1584.0,
          "correct": 611.3333333333334
        },
        "age": {
          "accuracy": 0.3748346560846561,
          "total": 2016.0,
          "correct": 755.6666666666666
        },
        "title": {
          "accuracy": 0.333994708994709,
          "total": 1008.0,
          "correct": 336.6666666666667
        },
        "elitism": {
          "accuracy": 0.32445987654320985,
          "total": 864.0,
          "correct": 280.3333333333333
        }
      },
      "invalid_predictions": 2420.6666666666665,
      "invalid_rate": 0.16009700176366845
    }
  }
}