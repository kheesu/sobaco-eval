{
  "model": "swallow-3.1-70b-inst",
  "dataset": "csv/ko-ja-v2_dataset.csv",
  "timestamp": "2025-12-17T05:42:54.394642",
  "metrics": {
    "overall_accuracy": 0.38816137566137565,
    "bias_accuracy": 0.44246031746031744,
    "bias_rate": 0.38822751322751325,
    "bias_score": 0.21891534391534392,
    "bias_score_details": {
      "n_biased": 2935,
      "n_counter_biased": 1280,
      "n_unknown": 3345,
      "n_valid": 7560
    },
    "culture_accuracy": 0.33386243386243386,
    "culture_total": 7560,
    "culture_valid": 7560,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.7140804597701149,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.2725095785440613,
        "n_biased": 583,
        "n_counter_biased": 14,
        "n_unknown": 1491
      },
      "gender": {
        "accuracy": 0.19823232323232323,
        "total": 1584,
        "valid": 1584,
        "bias_score": 0.25757575757575757,
        "n_biased": 839,
        "n_counter_biased": 431,
        "n_unknown": 314
      },
      "age": {
        "accuracy": 0.18948412698412698,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.29365079365079366,
        "n_biased": 1113,
        "n_counter_biased": 521,
        "n_unknown": 382
      },
      "title": {
        "accuracy": 0.37896825396825395,
        "total": 1008,
        "valid": 1008,
        "bias_score": -0.001984126984126984,
        "n_biased": 312,
        "n_counter_biased": 314,
        "n_unknown": 382
      },
      "elitism": {
        "accuracy": 0.8981481481481481,
        "total": 864,
        "valid": 864,
        "bias_score": 0.10185185185185185,
        "n_biased": 88,
        "n_counter_biased": 0,
        "n_unknown": 776
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.20881226053639848,
        "total": 2088,
        "correct": 436
      },
      "gender": {
        "accuracy": 0.2916666666666667,
        "total": 1584,
        "correct": 462
      },
      "age": {
        "accuracy": 0.3229166666666667,
        "total": 2016,
        "correct": 651
      },
      "title": {
        "accuracy": 0.7390873015873016,
        "total": 1008,
        "correct": 745
      },
      "elitism": {
        "accuracy": 0.2662037037037037,
        "total": 864,
        "correct": 230
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}