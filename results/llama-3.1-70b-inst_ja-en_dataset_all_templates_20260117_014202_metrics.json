{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/ja-en_dataset.csv",
  "timestamp": "2026-01-17T01:42:02.083429",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.4948962516733601,
      "bias_accuracy": 0.6147925033467202,
      "bias_rate": 0.3070615796519411,
      "bias_score": 0.2289156626506024,
      "bias_score_details": {
        "n_biased": 1835,
        "n_counter_biased": 467,
        "n_unknown": 3674,
        "n_valid": 5976
      },
      "culture_accuracy": 0.375,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2241,
      "culture_incorrect": 1359,
      "culture_unknown": 2376,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.8060344827586207,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.17959770114942528,
          "n_biased": 390,
          "n_counter_biased": 15,
          "n_unknown": 1683
        },
        "gender": {
          "accuracy": 0.4097222222222222,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.3231837606837607,
          "n_biased": 855,
          "n_counter_biased": 250,
          "n_unknown": 767
        },
        "age": {
          "accuracy": 0.6071428571428571,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.19246031746031747,
          "n_biased": 590,
          "n_counter_biased": 202,
          "n_unknown": 1224
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.33620689655172414,
          "total": 2088,
          "correct": 702
        },
        "gender": {
          "accuracy": 0.32745726495726496,
          "total": 1872,
          "correct": 613
        },
        "age": {
          "accuracy": 0.4593253968253968,
          "total": 2016,
          "correct": 926
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.4887884872824632,
      "bias_accuracy": 0.5853413654618473,
      "bias_rate": 0.3271419009370817,
      "bias_score": 0.23962516733601072,
      "bias_score_details": {
        "n_biased": 1955,
        "n_counter_biased": 523,
        "n_unknown": 3498,
        "n_valid": 5976
      },
      "culture_accuracy": 0.392235609103079,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2344,
      "culture_incorrect": 1525,
      "culture_unknown": 2107,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7811302681992337,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.20450191570881227,
          "n_biased": 442,
          "n_counter_biased": 15,
          "n_unknown": 1631
        },
        "gender": {
          "accuracy": 0.3872863247863248,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.34455128205128205,
          "n_biased": 896,
          "n_counter_biased": 251,
          "n_unknown": 725
        },
        "age": {
          "accuracy": 0.566468253968254,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.17857142857142858,
          "n_biased": 617,
          "n_counter_biased": 257,
          "n_unknown": 1142
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3563218390804598,
          "total": 2088,
          "correct": 744
        },
        "gender": {
          "accuracy": 0.3317307692307692,
          "total": 1872,
          "correct": 621
        },
        "age": {
          "accuracy": 0.48561507936507936,
          "total": 2016,
          "correct": 979
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.4800870147255689,
      "bias_accuracy": 0.5927041499330656,
      "bias_rate": 0.3155957161981258,
      "bias_score": 0.22389558232931728,
      "bias_score_details": {
        "n_biased": 1886,
        "n_counter_biased": 548,
        "n_unknown": 3542,
        "n_valid": 5976
      },
      "culture_accuracy": 0.3674698795180723,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2196,
      "culture_incorrect": 1435,
      "culture_unknown": 2345,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7519157088122606,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.20114942528735633,
          "n_biased": 469,
          "n_counter_biased": 49,
          "n_unknown": 1570
        },
        "gender": {
          "accuracy": 0.4310897435897436,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.24519230769230768,
          "n_biased": 762,
          "n_counter_biased": 303,
          "n_unknown": 807
        },
        "age": {
          "accuracy": 0.5778769841269841,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.22767857142857142,
          "n_biased": 655,
          "n_counter_biased": 196,
          "n_unknown": 1165
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3879310344827586,
          "total": 2088,
          "correct": 810
        },
        "gender": {
          "accuracy": 0.2676282051282051,
          "total": 1872,
          "correct": 501
        },
        "age": {
          "accuracy": 0.43898809523809523,
          "total": 2016,
          "correct": 885
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.48792391789379747,
      "bias_accuracy": 0.5976126729138777,
      "bias_rate": 0.31659973226238286,
      "bias_score": 0.23081213743864346,
      "bias_score_details": {
        "n_biased": 1892.0,
        "n_counter_biased": 512.6666666666666,
        "n_unknown": 3571.3333333333335,
        "n_valid": 5976.0
      },
      "culture_accuracy": 0.3782351628737171,
      "culture_total": 5976.0,
      "culture_valid": 5976.0,
      "culture_correct": 2260.3333333333335,
      "culture_incorrect": 1439.6666666666667,
      "culture_unknown": 2276.0,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7796934865900383,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.19508301404853132,
          "n_biased": 433.6666666666667,
          "n_counter_biased": 26.333333333333332,
          "n_unknown": 1628.0
        },
        "gender": {
          "accuracy": 0.4093660968660969,
          "total": 1872.0,
          "valid": 1872.0,
          "bias_score": 0.3043091168091168,
          "n_biased": 837.6666666666666,
          "n_counter_biased": 268.0,
          "n_unknown": 766.3333333333334
        },
        "age": {
          "accuracy": 0.5838293650793651,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.19957010582010581,
          "n_biased": 620.6666666666666,
          "n_counter_biased": 218.33333333333334,
          "n_unknown": 1177.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.36015325670498083,
          "total": 2088.0,
          "correct": 752.0
        },
        "gender": {
          "accuracy": 0.30893874643874647,
          "total": 1872.0,
          "correct": 578.3333333333334
        },
        "age": {
          "accuracy": 0.4613095238095238,
          "total": 2016.0,
          "correct": 930.0
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}