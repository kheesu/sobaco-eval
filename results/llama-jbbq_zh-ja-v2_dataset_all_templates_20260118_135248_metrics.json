{
  "model": "llama-jbbq",
  "dataset": "csv/zh-ja-v2_dataset.csv",
  "timestamp": "2026-01-18T13:52:48.188795",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.5708220764183712,
      "bias_accuracy": 0.9959876543209877,
      "bias_rate": 0.0030864197530864196,
      "bias_score": 0.0021604938271604936,
      "bias_score_details": {
        "n_biased": 20,
        "n_counter_biased": 6,
        "n_unknown": 6454,
        "n_valid": 6480
      },
      "culture_accuracy": 0.14532818532818534,
      "culture_total": 6480,
      "culture_valid": 6475,
      "culture_correct": 941,
      "culture_incorrect": 463,
      "culture_unknown": 5071,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 2160
        },
        "gender": {
          "accuracy": 0.999537037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.000462962962962963,
          "n_biased": 1,
          "n_counter_biased": 0,
          "n_unknown": 2159
        },
        "hierarchical_relationship": {
          "accuracy": 0.9884259259259259,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0060185185185185185,
          "n_biased": 19,
          "n_counter_biased": 6,
          "n_unknown": 2135
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.07731481481481481,
          "total": 2160,
          "correct": 167
        },
        "gender": {
          "accuracy": 0.1675925925925926,
          "total": 2160,
          "correct": 362
        },
        "hierarchical_relationship": {
          "accuracy": 0.19074074074074074,
          "total": 2160,
          "correct": 412
        }
      },
      "invalid_predictions": 5,
      "invalid_rate": 0.00038580246913580245
    },
    "template_2": {
      "overall_accuracy": 0.6231190678293078,
      "bias_accuracy": 0.9850308641975308,
      "bias_rate": 0.011882716049382716,
      "bias_score": 0.00895199876524155,
      "bias_score_details": {
        "n_biased": 77,
        "n_counter_biased": 19,
        "n_unknown": 6383,
        "n_valid": 6479
      },
      "culture_accuracy": 0.2611111111111111,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 1692,
      "culture_incorrect": 986,
      "culture_unknown": 3802,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 2160
        },
        "gender": {
          "accuracy": 0.9990740740740741,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.000925925925925926,
          "n_biased": 2,
          "n_counter_biased": 0,
          "n_unknown": 2158
        },
        "hierarchical_relationship": {
          "accuracy": 0.9564613246873552,
          "total": 2160,
          "valid": 2159,
          "bias_score": 0.025937934228809634,
          "n_biased": 75,
          "n_counter_biased": 19,
          "n_unknown": 2065
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.20833333333333334,
          "total": 2160,
          "correct": 450
        },
        "gender": {
          "accuracy": 0.2337962962962963,
          "total": 2160,
          "correct": 505
        },
        "hierarchical_relationship": {
          "accuracy": 0.34120370370370373,
          "total": 2160,
          "correct": 737
        }
      },
      "invalid_predictions": 1,
      "invalid_rate": 7.716049382716049e-05
    },
    "template_3": {
      "overall_accuracy": 0.4791876131107983,
      "bias_accuracy": 0.7233024691358024,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 4687,
        "n_valid": 4687
      },
      "culture_accuracy": 0.015021867275147366,
      "culture_total": 6480,
      "culture_valid": 5259,
      "culture_correct": 79,
      "culture_incorrect": 1,
      "culture_unknown": 5179,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1466,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1466
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1579,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1579
        },
        "hierarchical_relationship": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1642,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1642
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "gender": {
          "accuracy": 0.03657407407407407,
          "total": 2160,
          "correct": 79
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        }
      },
      "invalid_predictions": 3014,
      "invalid_rate": 0.23256172839506173
    },
    "averaged": {
      "overall_accuracy": 0.5577095857861591,
      "bias_accuracy": 0.901440329218107,
      "bias_rate": 0.004989711934156379,
      "bias_score": 0.003704164197467348,
      "bias_score_details": {
        "n_biased": 32.333333333333336,
        "n_counter_biased": 8.333333333333334,
        "n_unknown": 5841.333333333333,
        "n_valid": 5882.0
      },
      "culture_accuracy": 0.14048705457148128,
      "culture_total": 6480.0,
      "culture_valid": 6071.333333333333,
      "culture_correct": 904.0,
      "culture_incorrect": 483.3333333333333,
      "culture_unknown": 4684.0,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160.0,
          "valid": 1928.6666666666667,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 1928.6666666666667
        },
        "gender": {
          "accuracy": 0.999537037037037,
          "total": 2160.0,
          "valid": 1966.3333333333333,
          "bias_score": 0.000462962962962963,
          "n_biased": 1.0,
          "n_counter_biased": 0.0,
          "n_unknown": 1965.3333333333333
        },
        "hierarchical_relationship": {
          "accuracy": 0.9816290835377605,
          "total": 2160.0,
          "valid": 1987.0,
          "bias_score": 0.010652150915776051,
          "n_biased": 31.333333333333332,
          "n_counter_biased": 8.333333333333334,
          "n_unknown": 1947.3333333333333
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.09521604938271605,
          "total": 2160.0,
          "correct": 205.66666666666666
        },
        "gender": {
          "accuracy": 0.14598765432098768,
          "total": 2160.0,
          "correct": 315.3333333333333
        },
        "hierarchical_relationship": {
          "accuracy": 0.17731481481481481,
          "total": 2160.0,
          "correct": 383.0
        }
      },
      "invalid_predictions": 1006.6666666666666,
      "invalid_rate": 0.07767489711934157
    }
  }
}