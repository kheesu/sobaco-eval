{
  "model": "qwen-3-30b-inst",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-19T04:31:58.800874",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.4869047619047619,
      "bias_accuracy": 0.5764550264550264,
      "bias_rate": 0.28994708994708995,
      "bias_score": 0.15634920634920635,
      "bias_score_details": {
        "n_biased": 2192,
        "n_counter_biased": 1010,
        "n_unknown": 4358,
        "n_valid": 7560
      },
      "culture_accuracy": 0.39735449735449735,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 3004,
      "culture_incorrect": 2198,
      "culture_unknown": 2358,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.8113026819923371,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.18007662835249041,
          "n_biased": 385,
          "n_counter_biased": 9,
          "n_unknown": 1694
        },
        "gender": {
          "accuracy": 0.44823232323232326,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.16792929292929293,
          "n_biased": 570,
          "n_counter_biased": 304,
          "n_unknown": 710
        },
        "age": {
          "accuracy": 0.3586309523809524,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.2544642857142857,
          "n_biased": 903,
          "n_counter_biased": 390,
          "n_unknown": 723
        },
        "title": {
          "accuracy": 0.5436507936507936,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.057539682539682536,
          "n_biased": 201,
          "n_counter_biased": 259,
          "n_unknown": 548
        },
        "elitism": {
          "accuracy": 0.7905092592592593,
          "total": 864,
          "valid": 864,
          "bias_score": 0.09837962962962964,
          "n_biased": 133,
          "n_counter_biased": 48,
          "n_unknown": 683
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.35727969348659006,
          "total": 2088,
          "correct": 746
        },
        "gender": {
          "accuracy": 0.36742424242424243,
          "total": 1584,
          "correct": 582
        },
        "age": {
          "accuracy": 0.2857142857142857,
          "total": 2016,
          "correct": 576
        },
        "title": {
          "accuracy": 0.7251984126984127,
          "total": 1008,
          "correct": 731
        },
        "elitism": {
          "accuracy": 0.4270833333333333,
          "total": 864,
          "correct": 369
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.47473544973544973,
      "bias_accuracy": 0.5658730158730159,
      "bias_rate": 0.29642857142857143,
      "bias_score": 0.15873015873015872,
      "bias_score_details": {
        "n_biased": 2241,
        "n_counter_biased": 1041,
        "n_unknown": 4278,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3835978835978836,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2900,
      "culture_incorrect": 2224,
      "culture_unknown": 2436,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.789272030651341,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.19731800766283525,
          "n_biased": 426,
          "n_counter_biased": 14,
          "n_unknown": 1648
        },
        "gender": {
          "accuracy": 0.4292929292929293,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.1691919191919192,
          "n_biased": 586,
          "n_counter_biased": 318,
          "n_unknown": 680
        },
        "age": {
          "accuracy": 0.38591269841269843,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.23809523809523808,
          "n_biased": 859,
          "n_counter_biased": 379,
          "n_unknown": 778
        },
        "title": {
          "accuracy": 0.49107142857142855,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.03273809523809524,
          "n_biased": 240,
          "n_counter_biased": 273,
          "n_unknown": 495
        },
        "elitism": {
          "accuracy": 0.7835648148148148,
          "total": 864,
          "valid": 864,
          "bias_score": 0.08449074074074074,
          "n_biased": 130,
          "n_counter_biased": 57,
          "n_unknown": 677
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3381226053639847,
          "total": 2088,
          "correct": 706
        },
        "gender": {
          "accuracy": 0.3573232323232323,
          "total": 1584,
          "correct": 566
        },
        "age": {
          "accuracy": 0.2683531746031746,
          "total": 2016,
          "correct": 541
        },
        "title": {
          "accuracy": 0.7113095238095238,
          "total": 1008,
          "correct": 717
        },
        "elitism": {
          "accuracy": 0.42824074074074076,
          "total": 864,
          "correct": 370
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.4937169312169312,
      "bias_accuracy": 0.5376984126984127,
      "bias_rate": 0.3194444444444444,
      "bias_score": 0.1765873015873016,
      "bias_score_details": {
        "n_biased": 2415,
        "n_counter_biased": 1080,
        "n_unknown": 4065,
        "n_valid": 7560
      },
      "culture_accuracy": 0.4497354497354497,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 3400,
      "culture_incorrect": 2302,
      "culture_unknown": 1858,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7691570881226054,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.20306513409961685,
          "n_biased": 453,
          "n_counter_biased": 29,
          "n_unknown": 1606
        },
        "gender": {
          "accuracy": 0.39330808080808083,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.26073232323232326,
          "n_biased": 687,
          "n_counter_biased": 274,
          "n_unknown": 623
        },
        "age": {
          "accuracy": 0.3521825396825397,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.25793650793650796,
          "n_biased": 913,
          "n_counter_biased": 393,
          "n_unknown": 710
        },
        "title": {
          "accuracy": 0.38293650793650796,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.05357142857142857,
          "n_biased": 284,
          "n_counter_biased": 338,
          "n_unknown": 386
        },
        "elitism": {
          "accuracy": 0.8564814814814815,
          "total": 864,
          "valid": 864,
          "bias_score": 0.037037037037037035,
          "n_biased": 78,
          "n_counter_biased": 46,
          "n_unknown": 740
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.5047892720306514,
          "total": 2088,
          "correct": 1054
        },
        "gender": {
          "accuracy": 0.4046717171717172,
          "total": 1584,
          "correct": 641
        },
        "age": {
          "accuracy": 0.31200396825396826,
          "total": 2016,
          "correct": 629
        },
        "title": {
          "accuracy": 0.7718253968253969,
          "total": 1008,
          "correct": 778
        },
        "elitism": {
          "accuracy": 0.3449074074074074,
          "total": 864,
          "correct": 298
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.4851190476190476,
      "bias_accuracy": 0.5600088183421517,
      "bias_rate": 0.3019400352733686,
      "bias_score": 0.1638888888888889,
      "bias_score_details": {
        "n_biased": 2282.6666666666665,
        "n_counter_biased": 1043.6666666666667,
        "n_unknown": 4233.666666666667,
        "n_valid": 7560.0
      },
      "culture_accuracy": 0.4102292768959435,
      "culture_total": 7560.0,
      "culture_valid": 7560.0,
      "culture_correct": 3101.3333333333335,
      "culture_incorrect": 2241.3333333333335,
      "culture_unknown": 2217.3333333333335,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7899106002554279,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.19348659003831417,
          "n_biased": 421.3333333333333,
          "n_counter_biased": 17.333333333333332,
          "n_unknown": 1649.3333333333333
        },
        "gender": {
          "accuracy": 0.42361111111111116,
          "total": 1584.0,
          "valid": 1584.0,
          "bias_score": 0.19928451178451181,
          "n_biased": 614.3333333333334,
          "n_counter_biased": 298.6666666666667,
          "n_unknown": 671.0
        },
        "age": {
          "accuracy": 0.3655753968253968,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.2501653439153439,
          "n_biased": 891.6666666666666,
          "n_counter_biased": 387.3333333333333,
          "n_unknown": 737.0
        },
        "title": {
          "accuracy": 0.47255291005291,
          "total": 1008.0,
          "valid": 1008.0,
          "bias_score": -0.047949735449735444,
          "n_biased": 241.66666666666666,
          "n_counter_biased": 290.0,
          "n_unknown": 476.3333333333333
        },
        "elitism": {
          "accuracy": 0.8101851851851851,
          "total": 864.0,
          "valid": 864.0,
          "bias_score": 0.07330246913580248,
          "n_biased": 113.66666666666667,
          "n_counter_biased": 50.333333333333336,
          "n_unknown": 700.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4000638569604087,
          "total": 2088.0,
          "correct": 835.3333333333334
        },
        "gender": {
          "accuracy": 0.37647306397306396,
          "total": 1584.0,
          "correct": 596.3333333333334
        },
        "age": {
          "accuracy": 0.28869047619047616,
          "total": 2016.0,
          "correct": 582.0
        },
        "title": {
          "accuracy": 0.7361111111111112,
          "total": 1008.0,
          "correct": 742.0
        },
        "elitism": {
          "accuracy": 0.40007716049382713,
          "total": 864.0,
          "correct": 345.6666666666667
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}