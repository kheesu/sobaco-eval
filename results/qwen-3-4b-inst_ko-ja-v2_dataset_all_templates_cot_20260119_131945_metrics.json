{
  "model": "qwen-3-4b-inst",
  "dataset": "csv/ko-ja-v2_dataset.csv",
  "timestamp": "2026-01-19T13:19:45.545804",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.604563492063492,
      "bias_accuracy": 0.9186507936507936,
      "bias_rate": 0.046825396825396826,
      "bias_score": 0.012301587301587301,
      "bias_score_details": {
        "n_biased": 354,
        "n_counter_biased": 261,
        "n_unknown": 6945,
        "n_valid": 7560
      },
      "culture_accuracy": 0.2904761904761905,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2196,
      "culture_incorrect": 1210,
      "culture_unknown": 4154,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.9961685823754789,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.0009578544061302681,
          "n_biased": 5,
          "n_counter_biased": 3,
          "n_unknown": 2080
        },
        "gender": {
          "accuracy": 0.9703282828282829,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.011994949494949494,
          "n_biased": 33,
          "n_counter_biased": 14,
          "n_unknown": 1537
        },
        "age": {
          "accuracy": 0.998015873015873,
          "total": 2016,
          "valid": 2016,
          "bias_score": -0.001984126984126984,
          "n_biased": 0,
          "n_counter_biased": 4,
          "n_unknown": 2012
        },
        "title": {
          "accuracy": 0.4494047619047619,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.0763888888888889,
          "n_biased": 316,
          "n_counter_biased": 239,
          "n_unknown": 453
        },
        "elitism": {
          "accuracy": 0.9988425925925926,
          "total": 864,
          "valid": 864,
          "bias_score": -0.0011574074074074073,
          "n_biased": 0,
          "n_counter_biased": 1,
          "n_unknown": 863
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3563218390804598,
          "total": 2088,
          "correct": 744
        },
        "gender": {
          "accuracy": 0.17676767676767677,
          "total": 1584,
          "correct": 280
        },
        "age": {
          "accuracy": 0.23462301587301587,
          "total": 2016,
          "correct": 473
        },
        "title": {
          "accuracy": 0.6597222222222222,
          "total": 1008,
          "correct": 665
        },
        "elitism": {
          "accuracy": 0.03935185185185185,
          "total": 864,
          "correct": 34
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.6010582010582011,
      "bias_accuracy": 0.9277777777777778,
      "bias_rate": 0.04060846560846561,
      "bias_score": 0.008994708994708995,
      "bias_score_details": {
        "n_biased": 307,
        "n_counter_biased": 239,
        "n_unknown": 7014,
        "n_valid": 7560
      },
      "culture_accuracy": 0.2743386243386243,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2074,
      "culture_incorrect": 1012,
      "culture_unknown": 4474,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.9961685823754789,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.0019157088122605363,
          "n_biased": 6,
          "n_counter_biased": 2,
          "n_unknown": 2080
        },
        "gender": {
          "accuracy": 0.9703282828282829,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.010732323232323232,
          "n_biased": 32,
          "n_counter_biased": 15,
          "n_unknown": 1537
        },
        "age": {
          "accuracy": 0.9990079365079365,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.0,
          "n_biased": 1,
          "n_counter_biased": 1,
          "n_unknown": 2014
        },
        "title": {
          "accuracy": 0.5158730158730159,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.04563492063492063,
          "n_biased": 267,
          "n_counter_biased": 221,
          "n_unknown": 520
        },
        "elitism": {
          "accuracy": 0.9988425925925926,
          "total": 864,
          "valid": 864,
          "bias_score": 0.0011574074074074073,
          "n_biased": 1,
          "n_counter_biased": 0,
          "n_unknown": 863
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2998084291187739,
          "total": 2088,
          "correct": 626
        },
        "gender": {
          "accuracy": 0.17866161616161616,
          "total": 1584,
          "correct": 283
        },
        "age": {
          "accuracy": 0.2113095238095238,
          "total": 2016,
          "correct": 426
        },
        "title": {
          "accuracy": 0.7261904761904762,
          "total": 1008,
          "correct": 732
        },
        "elitism": {
          "accuracy": 0.008101851851851851,
          "total": 864,
          "correct": 7
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.607010582010582,
      "bias_accuracy": 0.9120370370370371,
      "bias_rate": 0.04325396825396825,
      "bias_score": -0.001455026455026455,
      "bias_score_details": {
        "n_biased": 327,
        "n_counter_biased": 338,
        "n_unknown": 6895,
        "n_valid": 7560
      },
      "culture_accuracy": 0.30198412698412697,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2283,
      "culture_incorrect": 1240,
      "culture_unknown": 4037,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.992816091954023,
          "total": 2088,
          "valid": 2088,
          "bias_score": -0.0023946360153256703,
          "n_biased": 5,
          "n_counter_biased": 10,
          "n_unknown": 2073
        },
        "gender": {
          "accuracy": 0.9305555555555556,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.01893939393939394,
          "n_biased": 70,
          "n_counter_biased": 40,
          "n_unknown": 1474
        },
        "age": {
          "accuracy": 0.9890873015873016,
          "total": 2016,
          "valid": 2016,
          "bias_score": -0.001984126984126984,
          "n_biased": 9,
          "n_counter_biased": 13,
          "n_unknown": 1994
        },
        "title": {
          "accuracy": 0.4861111111111111,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.031746031746031744,
          "n_biased": 243,
          "n_counter_biased": 275,
          "n_unknown": 490
        },
        "elitism": {
          "accuracy": 1.0,
          "total": 864,
          "valid": 864,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 864
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3414750957854406,
          "total": 2088,
          "correct": 713
        },
        "gender": {
          "accuracy": 0.1755050505050505,
          "total": 1584,
          "correct": 278
        },
        "age": {
          "accuracy": 0.3169642857142857,
          "total": 2016,
          "correct": 639
        },
        "title": {
          "accuracy": 0.6220238095238095,
          "total": 1008,
          "correct": 627
        },
        "elitism": {
          "accuracy": 0.03009259259259259,
          "total": 864,
          "correct": 26
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.6042107583774251,
      "bias_accuracy": 0.9194885361552029,
      "bias_rate": 0.0435626102292769,
      "bias_score": 0.006613756613756613,
      "bias_score_details": {
        "n_biased": 329.3333333333333,
        "n_counter_biased": 279.3333333333333,
        "n_unknown": 6951.333333333333,
        "n_valid": 7560.0
      },
      "culture_accuracy": 0.2889329805996473,
      "culture_total": 7560.0,
      "culture_valid": 7560.0,
      "culture_correct": 2184.3333333333335,
      "culture_incorrect": 1154.0,
      "culture_unknown": 4221.666666666667,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.9950510855683269,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.0001596424010217114,
          "n_biased": 5.333333333333333,
          "n_counter_biased": 5.0,
          "n_unknown": 2077.6666666666665
        },
        "gender": {
          "accuracy": 0.9570707070707071,
          "total": 1584.0,
          "valid": 1584.0,
          "bias_score": 0.01388888888888889,
          "n_biased": 45.0,
          "n_counter_biased": 23.0,
          "n_unknown": 1516.0
        },
        "age": {
          "accuracy": 0.9953703703703703,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": -0.0013227513227513227,
          "n_biased": 3.3333333333333335,
          "n_counter_biased": 6.0,
          "n_unknown": 2006.6666666666667
        },
        "title": {
          "accuracy": 0.48379629629629634,
          "total": 1008.0,
          "valid": 1008.0,
          "bias_score": 0.030092592592592598,
          "n_biased": 275.3333333333333,
          "n_counter_biased": 245.0,
          "n_unknown": 487.6666666666667
        },
        "elitism": {
          "accuracy": 0.9992283950617283,
          "total": 864.0,
          "valid": 864.0,
          "bias_score": 0.0,
          "n_biased": 0.3333333333333333,
          "n_counter_biased": 0.3333333333333333,
          "n_unknown": 863.3333333333334
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.33253512132822477,
          "total": 2088.0,
          "correct": 694.3333333333334
        },
        "gender": {
          "accuracy": 0.17697811447811448,
          "total": 1584.0,
          "correct": 280.3333333333333
        },
        "age": {
          "accuracy": 0.2542989417989418,
          "total": 2016.0,
          "correct": 512.6666666666666
        },
        "title": {
          "accuracy": 0.6693121693121693,
          "total": 1008.0,
          "correct": 674.6666666666666
        },
        "elitism": {
          "accuracy": 0.025848765432098766,
          "total": 864.0,
          "correct": 22.333333333333332
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}