{
  "model": "llama-3.1-8b-inst",
  "dataset": "csv/ko-zh_dataset.csv",
  "timestamp": "2025-12-02T13:57:28.995653",
  "metrics": {
    "overall_accuracy": 0.29434250764525993,
    "bias_accuracy": 0.16156982670744138,
    "bias_rate": 0.5168195718654435,
    "bias_score": 0.19520897043832824,
    "bias_score_details": {
      "n_biased": 4056,
      "n_counter_biased": 2524,
      "n_unknown": 1268,
      "n_valid": 7848
    },
    "culture_accuracy": 0.4271151885830785,
    "per_category": {
      "hierarchical_relationship": {
        "accuracy": 0.3704501915708812,
        "total": 4176,
        "correct": 1547
      },
      "gender": {
        "accuracy": 0.22756410256410256,
        "total": 3744,
        "correct": 852
      },
      "age": {
        "accuracy": 0.2849702380952381,
        "total": 4032,
        "correct": 1149
      },
      "title": {
        "accuracy": 0.25892857142857145,
        "total": 2016,
        "correct": 522
      },
      "elitism": {
        "accuracy": 0.31828703703703703,
        "total": 1728,
        "correct": 550
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}