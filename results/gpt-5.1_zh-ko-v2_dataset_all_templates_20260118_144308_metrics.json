{
  "model": "gpt-5.1",
  "dataset": "csv/zh-ko-v2_dataset.csv",
  "timestamp": "2026-01-18T14:43:08.508275",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 1.0,
      "bias_accuracy": 0.00030864197530864197,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 2,
        "n_valid": 2
      },
      "culture_accuracy": 1.0,
      "culture_total": 6480,
      "culture_valid": 8,
      "culture_correct": 8,
      "culture_incorrect": 0,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.000925925925925926,
          "total": 2160,
          "correct": 2
        },
        "gender": {
          "accuracy": 0.000925925925925926,
          "total": 2160,
          "correct": 2
        },
        "hierarchical_relationship": {
          "accuracy": 0.001851851851851852,
          "total": 2160,
          "correct": 4
        }
      },
      "invalid_predictions": 12950,
      "invalid_rate": 0.9992283950617284
    },
    "template_2": {
      "overall_accuracy": 0.8,
      "bias_accuracy": 0.0007716049382716049,
      "bias_rate": 0.00030864197530864197,
      "bias_score": 0.2857142857142857,
      "bias_score_details": {
        "n_biased": 2,
        "n_counter_biased": 0,
        "n_unknown": 5,
        "n_valid": 7
      },
      "culture_accuracy": 1.0,
      "culture_total": 6480,
      "culture_valid": 3,
      "culture_correct": 3,
      "culture_incorrect": 0,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1
        },
        "hierarchical_relationship": {
          "accuracy": 0.6666666666666666,
          "total": 2160,
          "valid": 6,
          "bias_score": 0.3333333333333333,
          "n_biased": 2,
          "n_counter_biased": 0,
          "n_unknown": 4
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "gender": {
          "accuracy": 0.001388888888888889,
          "total": 2160,
          "correct": 3
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        }
      },
      "invalid_predictions": 12950,
      "invalid_rate": 0.9992283950617284
    },
    "template_3": {
      "overall_accuracy": 0.7,
      "bias_accuracy": 0.0007716049382716049,
      "bias_rate": 0.000462962962962963,
      "bias_score": 0.375,
      "bias_score_details": {
        "n_biased": 3,
        "n_counter_biased": 0,
        "n_unknown": 5,
        "n_valid": 8
      },
      "culture_accuracy": 1.0,
      "culture_total": 6480,
      "culture_valid": 2,
      "culture_correct": 2,
      "culture_incorrect": 0,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 2,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 2
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1
        },
        "hierarchical_relationship": {
          "accuracy": 0.4,
          "total": 2160,
          "valid": 5,
          "bias_score": 0.6,
          "n_biased": 3,
          "n_counter_biased": 0,
          "n_unknown": 2
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "gender": {
          "accuracy": 0.000462962962962963,
          "total": 2160,
          "correct": 1
        },
        "hierarchical_relationship": {
          "accuracy": 0.000462962962962963,
          "total": 2160,
          "correct": 1
        }
      },
      "invalid_predictions": 12950,
      "invalid_rate": 0.9992283950617284
    },
    "averaged": {
      "overall_accuracy": 0.8333333333333334,
      "bias_accuracy": 0.0006172839506172839,
      "bias_rate": 0.00025720164609053495,
      "bias_score": 0.22023809523809523,
      "bias_score_details": {
        "n_biased": 1.6666666666666667,
        "n_counter_biased": 0.0,
        "n_unknown": 4.0,
        "n_valid": 5.666666666666667
      },
      "culture_accuracy": 1.0,
      "culture_total": 6480.0,
      "culture_valid": 4.333333333333333,
      "culture_correct": 4.333333333333333,
      "culture_incorrect": 0.0,
      "culture_unknown": 0.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.6666666666666666,
          "total": 2160.0,
          "valid": 1.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 1.0
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160.0,
          "valid": 1.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 1.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.35555555555555557,
          "total": 2160.0,
          "valid": 3.6666666666666665,
          "bias_score": 0.3111111111111111,
          "n_biased": 1.6666666666666667,
          "n_counter_biased": 0.0,
          "n_unknown": 2.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.00030864197530864197,
          "total": 2160.0,
          "correct": 0.6666666666666666
        },
        "gender": {
          "accuracy": 0.0009259259259259259,
          "total": 2160.0,
          "correct": 2.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0007716049382716049,
          "total": 2160.0,
          "correct": 1.6666666666666667
        }
      },
      "invalid_predictions": 12950.0,
      "invalid_rate": 0.9992283950617283
    }
  }
}