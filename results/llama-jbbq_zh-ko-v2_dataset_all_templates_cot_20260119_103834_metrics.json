{
  "model": "llama-jbbq",
  "dataset": "csv/zh-ko-v2_dataset.csv",
  "timestamp": "2026-01-19T10:38:34.946833",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.0,
      "bias_accuracy": 0.0,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 0,
        "n_valid": 0
      },
      "culture_accuracy": 0.0,
      "culture_total": 6480,
      "culture_valid": 0,
      "culture_correct": 0,
      "culture_incorrect": 0,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        }
      },
      "invalid_predictions": 12960,
      "invalid_rate": 1.0
    },
    "template_2": {
      "overall_accuracy": 0.0,
      "bias_accuracy": 0.0,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 0,
        "n_valid": 0
      },
      "culture_accuracy": 0.0,
      "culture_total": 6480,
      "culture_valid": 0,
      "culture_correct": 0,
      "culture_incorrect": 0,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        }
      },
      "invalid_predictions": 12960,
      "invalid_rate": 1.0
    },
    "template_3": {
      "overall_accuracy": 0.7710843373493976,
      "bias_accuracy": 0.009876543209876543,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 64,
        "n_valid": 64
      },
      "culture_accuracy": 0.0,
      "culture_total": 6480,
      "culture_valid": 19,
      "culture_correct": 0,
      "culture_incorrect": 0,
      "culture_unknown": 19,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 30,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 30
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 24,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 24
        },
        "hierarchical_relationship": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 10,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 10
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        }
      },
      "invalid_predictions": 12877,
      "invalid_rate": 0.9935956790123457
    },
    "averaged": {
      "overall_accuracy": 0.25702811244979923,
      "bias_accuracy": 0.0032921810699588477,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0.0,
        "n_counter_biased": 0.0,
        "n_unknown": 21.333333333333332,
        "n_valid": 21.333333333333332
      },
      "culture_accuracy": 0.0,
      "culture_total": 6480.0,
      "culture_valid": 6.333333333333333,
      "culture_correct": 0.0,
      "culture_incorrect": 0.0,
      "culture_unknown": 6.333333333333333,
      "per_category_bias": {
        "age": {
          "accuracy": 0.3333333333333333,
          "total": 2160.0,
          "valid": 10.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 10.0
        },
        "gender": {
          "accuracy": 0.3333333333333333,
          "total": 2160.0,
          "valid": 8.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 8.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.3333333333333333,
          "total": 2160.0,
          "valid": 3.3333333333333335,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 3.3333333333333335
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0,
          "total": 2160.0,
          "correct": 0.0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160.0,
          "correct": 0.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160.0,
          "correct": 0.0
        }
      },
      "invalid_predictions": 12932.333333333334,
      "invalid_rate": 0.9978652263374487
    }
  }
}