{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/zh-zh-v2_dataset.csv",
  "timestamp": "2026-01-19T16:25:37.742672",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.7296296296296296,
      "bias_accuracy": 0.475,
      "bias_rate": 0.5027777777777778,
      "bias_score": 0.48055555555555557,
      "bias_score_details": {
        "n_biased": 3258,
        "n_counter_biased": 144,
        "n_unknown": 3078,
        "n_valid": 6480
      },
      "culture_accuracy": 0.9842592592592593,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 6378,
      "culture_incorrect": 8,
      "culture_unknown": 94,
      "per_category_bias": {
        "age": {
          "accuracy": 0.7953703703703704,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.14351851851851852,
          "n_biased": 376,
          "n_counter_biased": 66,
          "n_unknown": 1718
        },
        "gender": {
          "accuracy": 0.20046296296296295,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.7486111111111111,
          "n_biased": 1672,
          "n_counter_biased": 55,
          "n_unknown": 433
        },
        "hierarchical_relationship": {
          "accuracy": 0.42916666666666664,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.549537037037037,
          "n_biased": 1210,
          "n_counter_biased": 23,
          "n_unknown": 927
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.9541666666666667,
          "total": 2160,
          "correct": 2061
        },
        "gender": {
          "accuracy": 0.9986111111111111,
          "total": 2160,
          "correct": 2157
        },
        "hierarchical_relationship": {
          "accuracy": 1.0,
          "total": 2160,
          "correct": 2160
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.7050925925925926,
      "bias_accuracy": 0.4287037037037037,
      "bias_rate": 0.5299382716049382,
      "bias_score": 0.48858024691358026,
      "bias_score_details": {
        "n_biased": 3434,
        "n_counter_biased": 268,
        "n_unknown": 2778,
        "n_valid": 6480
      },
      "culture_accuracy": 0.9814814814814815,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 6360,
      "culture_incorrect": 1,
      "culture_unknown": 119,
      "per_category_bias": {
        "age": {
          "accuracy": 0.7537037037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.1814814814814815,
          "n_biased": 462,
          "n_counter_biased": 70,
          "n_unknown": 1628
        },
        "gender": {
          "accuracy": 0.1925925925925926,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.7675925925925926,
          "n_biased": 1701,
          "n_counter_biased": 43,
          "n_unknown": 416
        },
        "hierarchical_relationship": {
          "accuracy": 0.3398148148148148,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.5166666666666667,
          "n_biased": 1271,
          "n_counter_biased": 155,
          "n_unknown": 734
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.9444444444444444,
          "total": 2160,
          "correct": 2040
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "correct": 2160
        },
        "hierarchical_relationship": {
          "accuracy": 1.0,
          "total": 2160,
          "correct": 2160
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.7317901234567902,
      "bias_accuracy": 0.4817901234567901,
      "bias_rate": 0.49969135802469133,
      "bias_score": 0.48117283950617284,
      "bias_score_details": {
        "n_biased": 3238,
        "n_counter_biased": 120,
        "n_unknown": 3122,
        "n_valid": 6480
      },
      "culture_accuracy": 0.9817901234567902,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 6362,
      "culture_incorrect": 2,
      "culture_unknown": 116,
      "per_category_bias": {
        "age": {
          "accuracy": 0.8078703703703703,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.15416666666666667,
          "n_biased": 374,
          "n_counter_biased": 41,
          "n_unknown": 1745
        },
        "gender": {
          "accuracy": 0.2518518518518518,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6888888888888889,
          "n_biased": 1552,
          "n_counter_biased": 64,
          "n_unknown": 544
        },
        "hierarchical_relationship": {
          "accuracy": 0.38564814814814813,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.600462962962963,
          "n_biased": 1312,
          "n_counter_biased": 15,
          "n_unknown": 833
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.9458333333333333,
          "total": 2160,
          "correct": 2043
        },
        "gender": {
          "accuracy": 0.999537037037037,
          "total": 2160,
          "correct": 2159
        },
        "hierarchical_relationship": {
          "accuracy": 1.0,
          "total": 2160,
          "correct": 2160
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.7221707818930042,
      "bias_accuracy": 0.4618312757201646,
      "bias_rate": 0.5108024691358024,
      "bias_score": 0.48343621399176956,
      "bias_score_details": {
        "n_biased": 3310.0,
        "n_counter_biased": 177.33333333333334,
        "n_unknown": 2992.6666666666665,
        "n_valid": 6480.0
      },
      "culture_accuracy": 0.9825102880658436,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 6366.666666666667,
      "culture_incorrect": 3.6666666666666665,
      "culture_unknown": 109.66666666666667,
      "per_category_bias": {
        "age": {
          "accuracy": 0.7856481481481481,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.15972222222222224,
          "n_biased": 404.0,
          "n_counter_biased": 59.0,
          "n_unknown": 1697.0
        },
        "gender": {
          "accuracy": 0.21496913580246912,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.7350308641975308,
          "n_biased": 1641.6666666666667,
          "n_counter_biased": 54.0,
          "n_unknown": 464.3333333333333
        },
        "hierarchical_relationship": {
          "accuracy": 0.3848765432098766,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.5555555555555556,
          "n_biased": 1264.3333333333333,
          "n_counter_biased": 64.33333333333333,
          "n_unknown": 831.3333333333334
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.9481481481481482,
          "total": 2160.0,
          "correct": 2048.0
        },
        "gender": {
          "accuracy": 0.9993827160493827,
          "total": 2160.0,
          "correct": 2158.6666666666665
        },
        "hierarchical_relationship": {
          "accuracy": 1.0,
          "total": 2160.0,
          "correct": 2160.0
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}