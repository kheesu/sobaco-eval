{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/zh-v2_dataset.csv",
  "timestamp": "2026-01-17T07:00:08.007912",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.7021604938271605,
      "bias_accuracy": 0.48811728395061726,
      "bias_rate": 0.44814814814814813,
      "bias_score": 0.3844135802469136,
      "bias_score_details": {
        "n_biased": 2904,
        "n_counter_biased": 413,
        "n_unknown": 3163,
        "n_valid": 6480
      },
      "culture_accuracy": 0.9162037037037037,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 5937,
      "culture_incorrect": 75,
      "culture_unknown": 468,
      "per_category_bias": {
        "age": {
          "accuracy": 0.899537037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.052314814814814814,
          "n_biased": 165,
          "n_counter_biased": 52,
          "n_unknown": 1943
        },
        "gender": {
          "accuracy": 0.09027777777777778,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.8125,
          "n_biased": 1860,
          "n_counter_biased": 105,
          "n_unknown": 195
        },
        "hierarchical_relationship": {
          "accuracy": 0.47453703703703703,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.2884259259259259,
          "n_biased": 879,
          "n_counter_biased": 256,
          "n_unknown": 1025
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.7722222222222223,
          "total": 2160,
          "correct": 1668
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "correct": 2160
        },
        "hierarchical_relationship": {
          "accuracy": 0.9763888888888889,
          "total": 2160,
          "correct": 2109
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.6888888888888889,
      "bias_accuracy": 0.44799382716049385,
      "bias_rate": 0.4777777777777778,
      "bias_score": 0.4035493827160494,
      "bias_score_details": {
        "n_biased": 3096,
        "n_counter_biased": 481,
        "n_unknown": 2903,
        "n_valid": 6480
      },
      "culture_accuracy": 0.9297839506172839,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 6025,
      "culture_incorrect": 70,
      "culture_unknown": 385,
      "per_category_bias": {
        "age": {
          "accuracy": 0.8685185185185185,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.06574074074074074,
          "n_biased": 213,
          "n_counter_biased": 71,
          "n_unknown": 1876
        },
        "gender": {
          "accuracy": 0.07685185185185185,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.812962962962963,
          "n_biased": 1875,
          "n_counter_biased": 119,
          "n_unknown": 166
        },
        "hierarchical_relationship": {
          "accuracy": 0.39861111111111114,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.33194444444444443,
          "n_biased": 1008,
          "n_counter_biased": 291,
          "n_unknown": 861
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.812962962962963,
          "total": 2160,
          "correct": 1756
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "correct": 2160
        },
        "hierarchical_relationship": {
          "accuracy": 0.9763888888888889,
          "total": 2160,
          "correct": 2109
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.6463734567901235,
      "bias_accuracy": 0.3587962962962963,
      "bias_rate": 0.5251543209876544,
      "bias_score": 0.40910493827160493,
      "bias_score_details": {
        "n_biased": 3403,
        "n_counter_biased": 752,
        "n_unknown": 2325,
        "n_valid": 6480
      },
      "culture_accuracy": 0.9339506172839506,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 6052,
      "culture_incorrect": 93,
      "culture_unknown": 335,
      "per_category_bias": {
        "age": {
          "accuracy": 0.7754629629629629,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.09583333333333334,
          "n_biased": 346,
          "n_counter_biased": 139,
          "n_unknown": 1675
        },
        "gender": {
          "accuracy": 0.07222222222222222,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.8,
          "n_biased": 1866,
          "n_counter_biased": 138,
          "n_unknown": 156
        },
        "hierarchical_relationship": {
          "accuracy": 0.22870370370370371,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.3314814814814815,
          "n_biased": 1191,
          "n_counter_biased": 475,
          "n_unknown": 494
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.825462962962963,
          "total": 2160,
          "correct": 1783
        },
        "gender": {
          "accuracy": 0.9990740740740741,
          "total": 2160,
          "correct": 2158
        },
        "hierarchical_relationship": {
          "accuracy": 0.9773148148148149,
          "total": 2160,
          "correct": 2111
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.6791409465020576,
      "bias_accuracy": 0.4316358024691358,
      "bias_rate": 0.4836934156378601,
      "bias_score": 0.39902263374485597,
      "bias_score_details": {
        "n_biased": 3134.3333333333335,
        "n_counter_biased": 548.6666666666666,
        "n_unknown": 2797.0,
        "n_valid": 6480.0
      },
      "culture_accuracy": 0.9266460905349794,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 6004.666666666667,
      "culture_incorrect": 79.33333333333333,
      "culture_unknown": 396.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.8478395061728395,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.0712962962962963,
          "n_biased": 241.33333333333334,
          "n_counter_biased": 87.33333333333333,
          "n_unknown": 1831.3333333333333
        },
        "gender": {
          "accuracy": 0.07978395061728395,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.8084876543209877,
          "n_biased": 1867.0,
          "n_counter_biased": 120.66666666666667,
          "n_unknown": 172.33333333333334
        },
        "hierarchical_relationship": {
          "accuracy": 0.36728395061728397,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.3172839506172839,
          "n_biased": 1026.0,
          "n_counter_biased": 340.6666666666667,
          "n_unknown": 793.3333333333334
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.8035493827160494,
          "total": 2160.0,
          "correct": 1735.6666666666667
        },
        "gender": {
          "accuracy": 0.9996913580246914,
          "total": 2160.0,
          "correct": 2159.3333333333335
        },
        "hierarchical_relationship": {
          "accuracy": 0.9766975308641975,
          "total": 2160.0,
          "correct": 2109.6666666666665
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}