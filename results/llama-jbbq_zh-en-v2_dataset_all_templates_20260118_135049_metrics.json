{
  "model": "llama-jbbq",
  "dataset": "csv/zh-en-v2_dataset.csv",
  "timestamp": "2026-01-18T13:50:49.501553",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.6492283950617284,
      "bias_accuracy": 0.9242283950617284,
      "bias_rate": 0.07469135802469136,
      "bias_score": 0.07361111111111111,
      "bias_score_details": {
        "n_biased": 484,
        "n_counter_biased": 7,
        "n_unknown": 5989,
        "n_valid": 6480
      },
      "culture_accuracy": 0.3742283950617284,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 2425,
      "culture_incorrect": 524,
      "culture_unknown": 3531,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 2160
        },
        "gender": {
          "accuracy": 0.9949074074074075,
          "total": 2160,
          "valid": 2160,
          "bias_score": -0.000462962962962963,
          "n_biased": 5,
          "n_counter_biased": 6,
          "n_unknown": 2149
        },
        "hierarchical_relationship": {
          "accuracy": 0.7777777777777778,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.2212962962962963,
          "n_biased": 479,
          "n_counter_biased": 1,
          "n_unknown": 1680
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.05787037037037037,
          "total": 2160,
          "correct": 125
        },
        "gender": {
          "accuracy": 0.48935185185185187,
          "total": 2160,
          "correct": 1057
        },
        "hierarchical_relationship": {
          "accuracy": 0.575462962962963,
          "total": 2160,
          "correct": 1243
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.6695987654320987,
      "bias_accuracy": 0.8388888888888889,
      "bias_rate": 0.15709876543209877,
      "bias_score": 0.15308641975308643,
      "bias_score_details": {
        "n_biased": 1018,
        "n_counter_biased": 26,
        "n_unknown": 5436,
        "n_valid": 6480
      },
      "culture_accuracy": 0.5003086419753087,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 3242,
      "culture_incorrect": 758,
      "culture_unknown": 2480,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 2160
        },
        "gender": {
          "accuracy": 0.9740740740740741,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.009259259259259259,
          "n_biased": 38,
          "n_counter_biased": 18,
          "n_unknown": 2104
        },
        "hierarchical_relationship": {
          "accuracy": 0.5425925925925926,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.45,
          "n_biased": 980,
          "n_counter_biased": 8,
          "n_unknown": 1172
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.18564814814814815,
          "total": 2160,
          "correct": 401
        },
        "gender": {
          "accuracy": 0.6513888888888889,
          "total": 2160,
          "correct": 1407
        },
        "hierarchical_relationship": {
          "accuracy": 0.6638888888888889,
          "total": 2160,
          "correct": 1434
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.5294784580498866,
      "bias_accuracy": 0.9376543209876543,
      "bias_rate": 0.0012345679012345679,
      "bias_score": 0.0013149243918474688,
      "bias_score_details": {
        "n_biased": 8,
        "n_counter_biased": 0,
        "n_unknown": 6076,
        "n_valid": 6084
      },
      "culture_accuracy": 0.07375478927203065,
      "culture_total": 6480,
      "culture_valid": 6264,
      "culture_correct": 462,
      "culture_incorrect": 22,
      "culture_unknown": 5780,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 2019,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 2019
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 1999,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 1999
        },
        "hierarchical_relationship": {
          "accuracy": 0.9961277831558567,
          "total": 2160,
          "valid": 2066,
          "bias_score": 0.003872216844143272,
          "n_biased": 8,
          "n_counter_biased": 0,
          "n_unknown": 2058
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0023148148148148147,
          "total": 2160,
          "correct": 5
        },
        "gender": {
          "accuracy": 0.04953703703703704,
          "total": 2160,
          "correct": 107
        },
        "hierarchical_relationship": {
          "accuracy": 0.16203703703703703,
          "total": 2160,
          "correct": 350
        }
      },
      "invalid_predictions": 612,
      "invalid_rate": 0.04722222222222222
    },
    "averaged": {
      "overall_accuracy": 0.6161018728479045,
      "bias_accuracy": 0.9002572016460905,
      "bias_rate": 0.07767489711934157,
      "bias_score": 0.076004151752015,
      "bias_score_details": {
        "n_biased": 503.3333333333333,
        "n_counter_biased": 11.0,
        "n_unknown": 5833.666666666667,
        "n_valid": 6348.0
      },
      "culture_accuracy": 0.3160972754363559,
      "culture_total": 6480.0,
      "culture_valid": 6408.0,
      "culture_correct": 2043.0,
      "culture_incorrect": 434.6666666666667,
      "culture_unknown": 3930.3333333333335,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160.0,
          "valid": 2113.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 2113.0
        },
        "gender": {
          "accuracy": 0.9896604938271606,
          "total": 2160.0,
          "valid": 2106.3333333333335,
          "bias_score": 0.0029320987654320985,
          "n_biased": 14.333333333333334,
          "n_counter_biased": 8.0,
          "n_unknown": 2084.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.772166051175409,
          "total": 2160.0,
          "valid": 2128.6666666666665,
          "bias_score": 0.2250561710468132,
          "n_biased": 489.0,
          "n_counter_biased": 3.0,
          "n_unknown": 1636.6666666666667
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.08194444444444444,
          "total": 2160.0,
          "correct": 177.0
        },
        "gender": {
          "accuracy": 0.39675925925925926,
          "total": 2160.0,
          "correct": 857.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.4671296296296296,
          "total": 2160.0,
          "correct": 1009.0
        }
      },
      "invalid_predictions": 204.0,
      "invalid_rate": 0.01574074074074074
    }
  }
}