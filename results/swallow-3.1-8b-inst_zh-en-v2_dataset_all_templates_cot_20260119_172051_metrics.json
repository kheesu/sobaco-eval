{
  "model": "swallow-3.1-8b-inst",
  "dataset": "csv/zh-en-v2_dataset.csv",
  "timestamp": "2026-01-19T17:20:51.652770",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.6666666666666666,
      "bias_accuracy": 0.5925925925925926,
      "bias_rate": 0.34120370370370373,
      "bias_score": 0.275,
      "bias_score_details": {
        "n_biased": 2211,
        "n_counter_biased": 429,
        "n_unknown": 3840,
        "n_valid": 6480
      },
      "culture_accuracy": 0.7407407407407407,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4800,
      "culture_incorrect": 631,
      "culture_unknown": 1049,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9356481481481481,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.034722222222222224,
          "n_biased": 107,
          "n_counter_biased": 32,
          "n_unknown": 2021
        },
        "gender": {
          "accuracy": 0.6370370370370371,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.03425925925925926,
          "n_biased": 429,
          "n_counter_biased": 355,
          "n_unknown": 1376
        },
        "hierarchical_relationship": {
          "accuracy": 0.2050925925925926,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.7560185185185185,
          "n_biased": 1675,
          "n_counter_biased": 42,
          "n_unknown": 443
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5486111111111112,
          "total": 2160,
          "correct": 1185
        },
        "gender": {
          "accuracy": 0.9481481481481482,
          "total": 2160,
          "correct": 2048
        },
        "hierarchical_relationship": {
          "accuracy": 0.725462962962963,
          "total": 2160,
          "correct": 1567
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.6843364197530865,
      "bias_accuracy": 0.6405864197530864,
      "bias_rate": 0.3001543209876543,
      "bias_score": 0.24089506172839506,
      "bias_score_details": {
        "n_biased": 1945,
        "n_counter_biased": 384,
        "n_unknown": 4151,
        "n_valid": 6480
      },
      "culture_accuracy": 0.7280864197530864,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4718,
      "culture_incorrect": 610,
      "culture_unknown": 1152,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9430555555555555,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0412037037037037,
          "n_biased": 106,
          "n_counter_biased": 17,
          "n_unknown": 2037
        },
        "gender": {
          "accuracy": 0.6731481481481482,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.019444444444444445,
          "n_biased": 374,
          "n_counter_biased": 332,
          "n_unknown": 1454
        },
        "hierarchical_relationship": {
          "accuracy": 0.3055555555555556,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6620370370370371,
          "n_biased": 1465,
          "n_counter_biased": 35,
          "n_unknown": 660
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5120370370370371,
          "total": 2160,
          "correct": 1106
        },
        "gender": {
          "accuracy": 0.9435185185185185,
          "total": 2160,
          "correct": 2038
        },
        "hierarchical_relationship": {
          "accuracy": 0.7287037037037037,
          "total": 2160,
          "correct": 1574
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.6767746913580247,
      "bias_accuracy": 0.6,
      "bias_rate": 0.3472222222222222,
      "bias_score": 0.29444444444444445,
      "bias_score_details": {
        "n_biased": 2250,
        "n_counter_biased": 342,
        "n_unknown": 3888,
        "n_valid": 6480
      },
      "culture_accuracy": 0.7535493827160494,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4883,
      "culture_incorrect": 699,
      "culture_unknown": 898,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9222222222222223,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.053703703703703705,
          "n_biased": 142,
          "n_counter_biased": 26,
          "n_unknown": 1992
        },
        "gender": {
          "accuracy": 0.6231481481481481,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.13518518518518519,
          "n_biased": 553,
          "n_counter_biased": 261,
          "n_unknown": 1346
        },
        "hierarchical_relationship": {
          "accuracy": 0.25462962962962965,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6944444444444444,
          "n_biased": 1555,
          "n_counter_biased": 55,
          "n_unknown": 550
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5828703703703704,
          "total": 2160,
          "correct": 1259
        },
        "gender": {
          "accuracy": 0.9555555555555556,
          "total": 2160,
          "correct": 2064
        },
        "hierarchical_relationship": {
          "accuracy": 0.7222222222222222,
          "total": 2160,
          "correct": 1560
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.6759259259259259,
      "bias_accuracy": 0.611059670781893,
      "bias_rate": 0.32952674897119344,
      "bias_score": 0.2701131687242799,
      "bias_score_details": {
        "n_biased": 2135.3333333333335,
        "n_counter_biased": 385.0,
        "n_unknown": 3959.6666666666665,
        "n_valid": 6480.0
      },
      "culture_accuracy": 0.7407921810699589,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 4800.333333333333,
      "culture_incorrect": 646.6666666666666,
      "culture_unknown": 1033.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9336419753086419,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.043209876543209874,
          "n_biased": 118.33333333333333,
          "n_counter_biased": 25.0,
          "n_unknown": 2016.6666666666667
        },
        "gender": {
          "accuracy": 0.6444444444444444,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.06296296296296296,
          "n_biased": 452.0,
          "n_counter_biased": 316.0,
          "n_unknown": 1392.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.2550925925925926,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.7041666666666666,
          "n_biased": 1565.0,
          "n_counter_biased": 44.0,
          "n_unknown": 551.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5478395061728395,
          "total": 2160.0,
          "correct": 1183.3333333333333
        },
        "gender": {
          "accuracy": 0.9490740740740741,
          "total": 2160.0,
          "correct": 2050.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.725462962962963,
          "total": 2160.0,
          "correct": 1567.0
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}