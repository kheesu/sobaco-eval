{
  "model": "llama-jbbq",
  "dataset": "csv/zh-zh-v2_dataset.csv",
  "timestamp": "2026-01-18T14:25:25.729253",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.6137345679012346,
      "bias_accuracy": 0.5722222222222222,
      "bias_rate": 0.35555555555555557,
      "bias_score": 0.2833333333333333,
      "bias_score_details": {
        "n_biased": 2304,
        "n_counter_biased": 468,
        "n_unknown": 3708,
        "n_valid": 6480
      },
      "culture_accuracy": 0.6552469135802469,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4246,
      "culture_incorrect": 997,
      "culture_unknown": 1237,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9972222222222222,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0,
          "n_biased": 3,
          "n_counter_biased": 3,
          "n_unknown": 2154
        },
        "gender": {
          "accuracy": 0.5583333333333333,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.053703703703703705,
          "n_biased": 535,
          "n_counter_biased": 419,
          "n_unknown": 1206
        },
        "hierarchical_relationship": {
          "accuracy": 0.16111111111111112,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.7962962962962963,
          "n_biased": 1766,
          "n_counter_biased": 46,
          "n_unknown": 348
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.30972222222222223,
          "total": 2160,
          "correct": 669
        },
        "gender": {
          "accuracy": 0.9546296296296296,
          "total": 2160,
          "correct": 2062
        },
        "hierarchical_relationship": {
          "accuracy": 0.7013888888888888,
          "total": 2160,
          "correct": 1515
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.6013117283950618,
      "bias_accuracy": 0.5643518518518519,
      "bias_rate": 0.3483024691358025,
      "bias_score": 0.2609567901234568,
      "bias_score_details": {
        "n_biased": 2257,
        "n_counter_biased": 566,
        "n_unknown": 3657,
        "n_valid": 6480
      },
      "culture_accuracy": 0.6382716049382716,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4136,
      "culture_incorrect": 1004,
      "culture_unknown": 1340,
      "per_category_bias": {
        "age": {
          "accuracy": 0.999537037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.000462962962962963,
          "n_biased": 1,
          "n_counter_biased": 0,
          "n_unknown": 2159
        },
        "gender": {
          "accuracy": 0.5217592592592593,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.043055555555555555,
          "n_biased": 563,
          "n_counter_biased": 470,
          "n_unknown": 1127
        },
        "hierarchical_relationship": {
          "accuracy": 0.17175925925925925,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.7393518518518518,
          "n_biased": 1693,
          "n_counter_biased": 96,
          "n_unknown": 371
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.2949074074074074,
          "total": 2160,
          "correct": 637
        },
        "gender": {
          "accuracy": 0.9449074074074074,
          "total": 2160,
          "correct": 2041
        },
        "hierarchical_relationship": {
          "accuracy": 0.675,
          "total": 2160,
          "correct": 1458
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.6616512345679012,
      "bias_accuracy": 0.9410493827160494,
      "bias_rate": 0.03796296296296296,
      "bias_score": 0.016975308641975308,
      "bias_score_details": {
        "n_biased": 246,
        "n_counter_biased": 136,
        "n_unknown": 6098,
        "n_valid": 6480
      },
      "culture_accuracy": 0.3822530864197531,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 2477,
      "culture_incorrect": 734,
      "culture_unknown": 3269,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 2160
        },
        "gender": {
          "accuracy": 0.9861111111111112,
          "total": 2160,
          "valid": 2160,
          "bias_score": -0.000925925925925926,
          "n_biased": 14,
          "n_counter_biased": 16,
          "n_unknown": 2130
        },
        "hierarchical_relationship": {
          "accuracy": 0.837037037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.05185185185185185,
          "n_biased": 232,
          "n_counter_biased": 120,
          "n_unknown": 1808
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.05185185185185185,
          "total": 2160,
          "correct": 112
        },
        "gender": {
          "accuracy": 0.5166666666666667,
          "total": 2160,
          "correct": 1116
        },
        "hierarchical_relationship": {
          "accuracy": 0.5782407407407407,
          "total": 2160,
          "correct": 1249
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.6255658436213992,
      "bias_accuracy": 0.6925411522633745,
      "bias_rate": 0.24727366255144034,
      "bias_score": 0.1870884773662551,
      "bias_score_details": {
        "n_biased": 1602.3333333333333,
        "n_counter_biased": 390.0,
        "n_unknown": 4487.666666666667,
        "n_valid": 6480.0
      },
      "culture_accuracy": 0.5585905349794239,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 3619.6666666666665,
      "culture_incorrect": 911.6666666666666,
      "culture_unknown": 1948.6666666666667,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9989197530864198,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.00015432098765432098,
          "n_biased": 1.3333333333333333,
          "n_counter_biased": 1.0,
          "n_unknown": 2157.6666666666665
        },
        "gender": {
          "accuracy": 0.6887345679012347,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.03194444444444445,
          "n_biased": 370.6666666666667,
          "n_counter_biased": 301.6666666666667,
          "n_unknown": 1487.6666666666667
        },
        "hierarchical_relationship": {
          "accuracy": 0.3899691358024691,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.5291666666666667,
          "n_biased": 1230.3333333333333,
          "n_counter_biased": 87.33333333333333,
          "n_unknown": 842.3333333333334
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.21882716049382714,
          "total": 2160.0,
          "correct": 472.6666666666667
        },
        "gender": {
          "accuracy": 0.8054012345679012,
          "total": 2160.0,
          "correct": 1739.6666666666667
        },
        "hierarchical_relationship": {
          "accuracy": 0.6515432098765431,
          "total": 2160.0,
          "correct": 1407.3333333333333
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}