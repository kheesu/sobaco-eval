{
  "model": "qwen-3-4b-inst",
  "dataset": "csv/ja-zh_dataset.csv",
  "timestamp": "2026-01-17T01:34:25.924732",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.31718540829986613,
      "bias_accuracy": 0.23008701472556894,
      "bias_rate": 0.536144578313253,
      "bias_score": 0.30237617135207495,
      "bias_score_details": {
        "n_biased": 3204,
        "n_counter_biased": 1397,
        "n_unknown": 1375,
        "n_valid": 5976
      },
      "culture_accuracy": 0.4042838018741633,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2416,
      "culture_incorrect": 3147,
      "culture_unknown": 413,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.44971264367816094,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.49377394636015326,
          "n_biased": 1090,
          "n_counter_biased": 59,
          "n_unknown": 939
        },
        "gender": {
          "accuracy": 0.09348290598290598,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.02938034188034188,
          "n_biased": 876,
          "n_counter_biased": 821,
          "n_unknown": 175
        },
        "age": {
          "accuracy": 0.12946428571428573,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3576388888888889,
          "n_biased": 1238,
          "n_counter_biased": 517,
          "n_unknown": 261
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4616858237547893,
          "total": 2088,
          "correct": 964
        },
        "gender": {
          "accuracy": 0.33760683760683763,
          "total": 1872,
          "correct": 632
        },
        "age": {
          "accuracy": 0.40674603174603174,
          "total": 2016,
          "correct": 820
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.3217871485943775,
      "bias_accuracy": 0.23443775100401607,
      "bias_rate": 0.5294511378848729,
      "bias_score": 0.2933400267737617,
      "bias_score_details": {
        "n_biased": 3164,
        "n_counter_biased": 1411,
        "n_unknown": 1401,
        "n_valid": 5976
      },
      "culture_accuracy": 0.40913654618473894,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2445,
      "culture_incorrect": 3115,
      "culture_unknown": 416,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.4846743295019157,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.49616858237547895,
          "n_biased": 1056,
          "n_counter_biased": 20,
          "n_unknown": 1012
        },
        "gender": {
          "accuracy": 0.07264957264957266,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.013888888888888888,
          "n_biased": 881,
          "n_counter_biased": 855,
          "n_unknown": 136
        },
        "age": {
          "accuracy": 0.12549603174603174,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3427579365079365,
          "n_biased": 1227,
          "n_counter_biased": 536,
          "n_unknown": 253
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4664750957854406,
          "total": 2088,
          "correct": 974
        },
        "gender": {
          "accuracy": 0.344017094017094,
          "total": 1872,
          "correct": 644
        },
        "age": {
          "accuracy": 0.41021825396825395,
          "total": 2016,
          "correct": 827
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.3329986613119143,
      "bias_accuracy": 0.2633868808567604,
      "bias_rate": 0.5167336010709505,
      "bias_score": 0.29685408299866134,
      "bias_score_details": {
        "n_biased": 3088,
        "n_counter_biased": 1314,
        "n_unknown": 1574,
        "n_valid": 5976
      },
      "culture_accuracy": 0.4026104417670683,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2406,
      "culture_incorrect": 3079,
      "culture_unknown": 491,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.48802681992337166,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.48419540229885055,
          "n_biased": 1040,
          "n_counter_biased": 29,
          "n_unknown": 1019
        },
        "gender": {
          "accuracy": 0.1597222222222222,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.034722222222222224,
          "n_biased": 819,
          "n_counter_biased": 754,
          "n_unknown": 299
        },
        "age": {
          "accuracy": 0.12698412698412698,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3462301587301587,
          "n_biased": 1229,
          "n_counter_biased": 531,
          "n_unknown": 256
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4305555555555556,
          "total": 2088,
          "correct": 899
        },
        "gender": {
          "accuracy": 0.36752136752136755,
          "total": 1872,
          "correct": 688
        },
        "age": {
          "accuracy": 0.40625,
          "total": 2016,
          "correct": 819
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.3239904060687193,
      "bias_accuracy": 0.24263721552878179,
      "bias_rate": 0.5274431057563588,
      "bias_score": 0.2975234270414993,
      "bias_score_details": {
        "n_biased": 3152.0,
        "n_counter_biased": 1374.0,
        "n_unknown": 1450.0,
        "n_valid": 5976.0
      },
      "culture_accuracy": 0.40534359660865676,
      "culture_total": 5976.0,
      "culture_valid": 5976.0,
      "culture_correct": 2422.3333333333335,
      "culture_incorrect": 3113.6666666666665,
      "culture_unknown": 440.0,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.4741379310344828,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.4913793103448276,
          "n_biased": 1062.0,
          "n_counter_biased": 36.0,
          "n_unknown": 990.0
        },
        "gender": {
          "accuracy": 0.10861823361823363,
          "total": 1872.0,
          "valid": 1872.0,
          "bias_score": 0.025997150997150997,
          "n_biased": 858.6666666666666,
          "n_counter_biased": 810.0,
          "n_unknown": 203.33333333333334
        },
        "age": {
          "accuracy": 0.1273148148148148,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.3488756613756614,
          "n_biased": 1231.3333333333333,
          "n_counter_biased": 528.0,
          "n_unknown": 256.6666666666667
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4529054916985951,
          "total": 2088.0,
          "correct": 945.6666666666666
        },
        "gender": {
          "accuracy": 0.34971509971509973,
          "total": 1872.0,
          "correct": 654.6666666666666
        },
        "age": {
          "accuracy": 0.4077380952380952,
          "total": 2016.0,
          "correct": 822.0
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}