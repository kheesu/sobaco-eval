{
  "model": "qwen-3-30b-inst",
  "dataset": "csv/ko-ja_dataset.csv",
  "timestamp": "2025-12-18T15:02:03.749087",
  "metrics": {
    "overall_accuracy": 0.32849486377847253,
    "bias_accuracy": 0.17558613659531092,
    "bias_rate": 0.18692660550458715,
    "bias_score": -0.018661811561219845,
    "bias_score_details": {
      "n_biased": 1467,
      "n_counter_biased": 1549,
      "n_unknown": 1378,
      "n_valid": 4394
    },
    "culture_accuracy": 0.3428320911880754,
    "culture_total": 7848,
    "culture_valid": 4562,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.36340425531914894,
        "total": 2088,
        "valid": 1175,
        "bias_score": -0.05106382978723404,
        "n_biased": 344,
        "n_counter_biased": 404,
        "n_unknown": 427
      },
      "gender": {
        "accuracy": 0.3103448275862069,
        "total": 1872,
        "valid": 1073,
        "bias_score": -0.05405405405405406,
        "n_biased": 341,
        "n_counter_biased": 399,
        "n_unknown": 333
      },
      "age": {
        "accuracy": 0.218214607754734,
        "total": 2016,
        "valid": 1109,
        "bias_score": -0.017132551848512173,
        "n_biased": 424,
        "n_counter_biased": 443,
        "n_unknown": 242
      },
      "title": {
        "accuracy": 0.41354723707664887,
        "total": 1008,
        "valid": 561,
        "bias_score": 0.026737967914438502,
        "n_biased": 172,
        "n_counter_biased": 157,
        "n_unknown": 232
      },
      "elitism": {
        "accuracy": 0.3025210084033613,
        "total": 864,
        "valid": 476,
        "bias_score": 0.08403361344537816,
        "n_biased": 186,
        "n_counter_biased": 146,
        "n_unknown": 144
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.2092911877394636,
        "total": 2088,
        "correct": 437
      },
      "gender": {
        "accuracy": 0.19444444444444445,
        "total": 1872,
        "correct": 364
      },
      "age": {
        "accuracy": 0.18700396825396826,
        "total": 2016,
        "correct": 377
      },
      "title": {
        "accuracy": 0.2152777777777778,
        "total": 1008,
        "correct": 217
      },
      "elitism": {
        "accuracy": 0.19560185185185186,
        "total": 864,
        "correct": 169
      }
    },
    "invalid_predictions": 6740,
    "invalid_rate": 0.42940876656472987
  }
}