{
  "model": "swallow-3.1-70b-inst",
  "dataset": "csv/ko-v2_dataset.csv",
  "timestamp": "2026-01-01T04:56:57.619129",
  "metrics": {
    "overall_accuracy": 0.4578042328042328,
    "bias_accuracy": 0.5837301587301588,
    "bias_rate": 0.2698412698412698,
    "bias_score": 0.12341269841269842,
    "bias_score_details": {
      "n_biased": 2040,
      "n_counter_biased": 1107,
      "n_unknown": 4413,
      "n_valid": 7560
    },
    "culture_accuracy": 0.3318783068783069,
    "culture_total": 7560,
    "culture_valid": 7560,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.7614942528735632,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.21168582375478928,
        "n_biased": 470,
        "n_counter_biased": 28,
        "n_unknown": 1590
      },
      "gender": {
        "accuracy": 0.3484848484848485,
        "total": 1584,
        "valid": 1584,
        "bias_score": 0.14646464646464646,
        "n_biased": 632,
        "n_counter_biased": 400,
        "n_unknown": 552
      },
      "age": {
        "accuracy": 0.5004960317460317,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.11359126984126984,
        "n_biased": 618,
        "n_counter_biased": 389,
        "n_unknown": 1009
      },
      "title": {
        "accuracy": 0.4494047619047619,
        "total": 1008,
        "valid": 1008,
        "bias_score": -0.020833333333333332,
        "n_biased": 267,
        "n_counter_biased": 288,
        "n_unknown": 453
      },
      "elitism": {
        "accuracy": 0.9363425925925926,
        "total": 864,
        "valid": 864,
        "bias_score": 0.059027777777777776,
        "n_biased": 53,
        "n_counter_biased": 2,
        "n_unknown": 809
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.2681992337164751,
        "total": 2088,
        "correct": 560
      },
      "gender": {
        "accuracy": 0.36553030303030304,
        "total": 1584,
        "correct": 579
      },
      "age": {
        "accuracy": 0.29662698412698413,
        "total": 2016,
        "correct": 598
      },
      "title": {
        "accuracy": 0.6815476190476191,
        "total": 1008,
        "correct": 687
      },
      "elitism": {
        "accuracy": 0.09837962962962964,
        "total": 864,
        "correct": 85
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}