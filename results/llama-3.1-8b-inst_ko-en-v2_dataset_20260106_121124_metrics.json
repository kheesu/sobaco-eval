{
  "model": "llama-3.1-8b-inst",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-06T12:11:25.038451",
  "metrics": {
    "overall_accuracy": 0.2871693121693122,
    "bias_accuracy": 0.18624338624338624,
    "bias_rate": 0.47314814814814815,
    "bias_score": 0.13253968253968254,
    "bias_score_details": {
      "n_biased": 3577,
      "n_counter_biased": 2575,
      "n_unknown": 1408,
      "n_valid": 7560
    },
    "culture_accuracy": 0.3880952380952381,
    "culture_total": 7560,
    "culture_valid": 7560,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.22365900383141762,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.3654214559386973,
        "n_biased": 1192,
        "n_counter_biased": 429,
        "n_unknown": 467
      },
      "gender": {
        "accuracy": 0.14583333333333334,
        "total": 1584,
        "valid": 1584,
        "bias_score": -0.03977272727272727,
        "n_biased": 645,
        "n_counter_biased": 708,
        "n_unknown": 231
      },
      "age": {
        "accuracy": 0.12748015873015872,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.21378968253968253,
        "n_biased": 1095,
        "n_counter_biased": 664,
        "n_unknown": 257
      },
      "title": {
        "accuracy": 0.17559523809523808,
        "total": 1008,
        "valid": 1008,
        "bias_score": -0.17559523809523808,
        "n_biased": 327,
        "n_counter_biased": 504,
        "n_unknown": 177
      },
      "elitism": {
        "accuracy": 0.3194444444444444,
        "total": 864,
        "valid": 864,
        "bias_score": 0.05555555555555555,
        "n_biased": 318,
        "n_counter_biased": 270,
        "n_unknown": 276
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.40229885057471265,
        "total": 2088,
        "correct": 840
      },
      "gender": {
        "accuracy": 0.3440656565656566,
        "total": 1584,
        "correct": 545
      },
      "age": {
        "accuracy": 0.3998015873015873,
        "total": 2016,
        "correct": 806
      },
      "title": {
        "accuracy": 0.43353174603174605,
        "total": 1008,
        "correct": 437
      },
      "elitism": {
        "accuracy": 0.3541666666666667,
        "total": 864,
        "correct": 306
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}