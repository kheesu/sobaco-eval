{
  "model": "llama-3.1-8b-inst",
  "dataset": "csv/ko-ko-v2_dataset.csv",
  "timestamp": "2026-01-17T01:33:06.477252",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.30072751322751323,
      "bias_accuracy": 0.20555555555555555,
      "bias_rate": 0.41904761904761906,
      "bias_score": 0.04365079365079365,
      "bias_score_details": {
        "n_biased": 3168,
        "n_counter_biased": 2838,
        "n_unknown": 1554,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3958994708994709,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2993,
      "culture_incorrect": 3107,
      "culture_unknown": 1460,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.2959770114942529,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.05172413793103448,
          "n_biased": 789,
          "n_counter_biased": 681,
          "n_unknown": 618
        },
        "gender": {
          "accuracy": 0.24494949494949494,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.015151515151515152,
          "n_biased": 610,
          "n_counter_biased": 586,
          "n_unknown": 388
        },
        "age": {
          "accuracy": 0.09771825396825397,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.06597222222222222,
          "n_biased": 976,
          "n_counter_biased": 843,
          "n_unknown": 197
        },
        "title": {
          "accuracy": 0.22023809523809523,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.023809523809523808,
          "n_biased": 405,
          "n_counter_biased": 381,
          "n_unknown": 222
        },
        "elitism": {
          "accuracy": 0.14930555555555555,
          "total": 864,
          "valid": 864,
          "bias_score": 0.047453703703703706,
          "n_biased": 388,
          "n_counter_biased": 347,
          "n_unknown": 129
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4051724137931034,
          "total": 2088,
          "correct": 846
        },
        "gender": {
          "accuracy": 0.3472222222222222,
          "total": 1584,
          "correct": 550
        },
        "age": {
          "accuracy": 0.37896825396825395,
          "total": 2016,
          "correct": 764
        },
        "title": {
          "accuracy": 0.44246031746031744,
          "total": 1008,
          "correct": 446
        },
        "elitism": {
          "accuracy": 0.4479166666666667,
          "total": 864,
          "correct": 387
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.30376984126984125,
      "bias_accuracy": 0.22063492063492063,
      "bias_rate": 0.41494708994708995,
      "bias_score": 0.05052910052910053,
      "bias_score_details": {
        "n_biased": 3137,
        "n_counter_biased": 2755,
        "n_unknown": 1668,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3869047619047619,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2925,
      "culture_incorrect": 3112,
      "culture_unknown": 1523,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.32375478927203066,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.04118773946360153,
          "n_biased": 749,
          "n_counter_biased": 663,
          "n_unknown": 676
        },
        "gender": {
          "accuracy": 0.24684343434343434,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.017045454545454544,
          "n_biased": 610,
          "n_counter_biased": 583,
          "n_unknown": 391
        },
        "age": {
          "accuracy": 0.08779761904761904,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.1195436507936508,
          "n_biased": 1040,
          "n_counter_biased": 799,
          "n_unknown": 177
        },
        "title": {
          "accuracy": 0.2390873015873016,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.000992063492063492,
          "n_biased": 383,
          "n_counter_biased": 384,
          "n_unknown": 241
        },
        "elitism": {
          "accuracy": 0.21180555555555555,
          "total": 864,
          "valid": 864,
          "bias_score": 0.03356481481481482,
          "n_biased": 355,
          "n_counter_biased": 326,
          "n_unknown": 183
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.39176245210727967,
          "total": 2088,
          "correct": 818
        },
        "gender": {
          "accuracy": 0.3327020202020202,
          "total": 1584,
          "correct": 527
        },
        "age": {
          "accuracy": 0.37648809523809523,
          "total": 2016,
          "correct": 759
        },
        "title": {
          "accuracy": 0.4305555555555556,
          "total": 1008,
          "correct": 434
        },
        "elitism": {
          "accuracy": 0.4479166666666667,
          "total": 864,
          "correct": 387
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.3167989417989418,
      "bias_accuracy": 0.2443121693121693,
      "bias_rate": 0.4064814814814815,
      "bias_score": 0.05727513227513228,
      "bias_score_details": {
        "n_biased": 3073,
        "n_counter_biased": 2640,
        "n_unknown": 1847,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3892857142857143,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2943,
      "culture_incorrect": 3059,
      "culture_unknown": 1558,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.2921455938697318,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.08045977011494253,
          "n_biased": 823,
          "n_counter_biased": 655,
          "n_unknown": 610
        },
        "gender": {
          "accuracy": 0.3611111111111111,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.020202020202020204,
          "n_biased": 522,
          "n_counter_biased": 490,
          "n_unknown": 572
        },
        "age": {
          "accuracy": 0.1076388888888889,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.09375,
          "n_biased": 994,
          "n_counter_biased": 805,
          "n_unknown": 217
        },
        "title": {
          "accuracy": 0.25992063492063494,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.00992063492063492,
          "n_biased": 368,
          "n_counter_biased": 378,
          "n_unknown": 262
        },
        "elitism": {
          "accuracy": 0.2152777777777778,
          "total": 864,
          "valid": 864,
          "bias_score": 0.0625,
          "n_biased": 366,
          "n_counter_biased": 312,
          "n_unknown": 186
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4185823754789272,
          "total": 2088,
          "correct": 874
        },
        "gender": {
          "accuracy": 0.2935606060606061,
          "total": 1584,
          "correct": 465
        },
        "age": {
          "accuracy": 0.3705357142857143,
          "total": 2016,
          "correct": 747
        },
        "title": {
          "accuracy": 0.4494047619047619,
          "total": 1008,
          "correct": 453
        },
        "elitism": {
          "accuracy": 0.4675925925925926,
          "total": 864,
          "correct": 404
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.30709876543209874,
      "bias_accuracy": 0.22350088183421515,
      "bias_rate": 0.4134920634920635,
      "bias_score": 0.05048500881834215,
      "bias_score_details": {
        "n_biased": 3126.0,
        "n_counter_biased": 2744.3333333333335,
        "n_unknown": 1689.6666666666667,
        "n_valid": 7560.0
      },
      "culture_accuracy": 0.39069664902998236,
      "culture_total": 7560.0,
      "culture_valid": 7560.0,
      "culture_correct": 2953.6666666666665,
      "culture_incorrect": 3092.6666666666665,
      "culture_unknown": 1513.6666666666667,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.30395913154533843,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.05779054916985952,
          "n_biased": 787.0,
          "n_counter_biased": 666.3333333333334,
          "n_unknown": 634.6666666666666
        },
        "gender": {
          "accuracy": 0.28430134680134683,
          "total": 1584.0,
          "valid": 1584.0,
          "bias_score": 0.017466329966329967,
          "n_biased": 580.6666666666666,
          "n_counter_biased": 553.0,
          "n_unknown": 450.3333333333333
        },
        "age": {
          "accuracy": 0.09771825396825397,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.09308862433862435,
          "n_biased": 1003.3333333333334,
          "n_counter_biased": 815.6666666666666,
          "n_unknown": 197.0
        },
        "title": {
          "accuracy": 0.2397486772486773,
          "total": 1008.0,
          "valid": 1008.0,
          "bias_score": 0.004298941798941799,
          "n_biased": 385.3333333333333,
          "n_counter_biased": 381.0,
          "n_unknown": 241.66666666666666
        },
        "elitism": {
          "accuracy": 0.19212962962962962,
          "total": 864.0,
          "valid": 864.0,
          "bias_score": 0.047839506172839504,
          "n_biased": 369.6666666666667,
          "n_counter_biased": 328.3333333333333,
          "n_unknown": 166.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4051724137931034,
          "total": 2088.0,
          "correct": 846.0
        },
        "gender": {
          "accuracy": 0.3244949494949495,
          "total": 1584.0,
          "correct": 514.0
        },
        "age": {
          "accuracy": 0.37533068783068785,
          "total": 2016.0,
          "correct": 756.6666666666666
        },
        "title": {
          "accuracy": 0.4408068783068783,
          "total": 1008.0,
          "correct": 444.3333333333333
        },
        "elitism": {
          "accuracy": 0.45447530864197533,
          "total": 864.0,
          "correct": 392.6666666666667
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}