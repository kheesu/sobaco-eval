{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/ja-zh_dataset.csv",
  "timestamp": "2026-01-19T11:36:13.288599",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.5878514056224899,
      "bias_accuracy": 0.5317938420348058,
      "bias_rate": 0.3927376171352075,
      "bias_score": 0.3172690763052209,
      "bias_score_details": {
        "n_biased": 2347,
        "n_counter_biased": 451,
        "n_unknown": 3178,
        "n_valid": 5976
      },
      "culture_accuracy": 0.643908969210174,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 3848,
      "culture_incorrect": 1940,
      "culture_unknown": 188,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7605363984674329,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.23180076628352492,
          "n_biased": 492,
          "n_counter_biased": 8,
          "n_unknown": 1588
        },
        "gender": {
          "accuracy": 0.5085470085470085,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.3162393162393162,
          "n_biased": 756,
          "n_counter_biased": 164,
          "n_unknown": 952
        },
        "age": {
          "accuracy": 0.31646825396825395,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.40674603174603174,
          "n_biased": 1099,
          "n_counter_biased": 279,
          "n_unknown": 638
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.7432950191570882,
          "total": 2088,
          "correct": 1552
        },
        "gender": {
          "accuracy": 0.5769230769230769,
          "total": 1872,
          "correct": 1080
        },
        "age": {
          "accuracy": 0.6031746031746031,
          "total": 2016,
          "correct": 1216
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.5949631860776439,
      "bias_accuracy": 0.5657630522088354,
      "bias_rate": 0.35826639892904955,
      "bias_score": 0.2822958500669344,
      "bias_score_details": {
        "n_biased": 2141,
        "n_counter_biased": 454,
        "n_unknown": 3381,
        "n_valid": 5976
      },
      "culture_accuracy": 0.6241633199464525,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 3730,
      "culture_incorrect": 1793,
      "culture_unknown": 453,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.757183908045977,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.21982758620689655,
          "n_biased": 483,
          "n_counter_biased": 24,
          "n_unknown": 1581
        },
        "gender": {
          "accuracy": 0.5133547008547008,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.26121794871794873,
          "n_biased": 700,
          "n_counter_biased": 211,
          "n_unknown": 961
        },
        "age": {
          "accuracy": 0.41617063492063494,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3665674603174603,
          "n_biased": 958,
          "n_counter_biased": 219,
          "n_unknown": 839
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.728448275862069,
          "total": 2088,
          "correct": 1521
        },
        "gender": {
          "accuracy": 0.5854700854700855,
          "total": 1872,
          "correct": 1096
        },
        "age": {
          "accuracy": 0.5520833333333334,
          "total": 2016,
          "correct": 1113
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.571954484605087,
      "bias_accuracy": 0.5070281124497992,
      "bias_rate": 0.40813253012048195,
      "bias_score": 0.3232931726907631,
      "bias_score_details": {
        "n_biased": 2439,
        "n_counter_biased": 507,
        "n_unknown": 3030,
        "n_valid": 5976
      },
      "culture_accuracy": 0.6368808567603749,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 3806,
      "culture_incorrect": 1994,
      "culture_unknown": 176,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.73227969348659,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.26484674329501917,
          "n_biased": 556,
          "n_counter_biased": 3,
          "n_unknown": 1529
        },
        "gender": {
          "accuracy": 0.5144230769230769,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.31143162393162394,
          "n_biased": 746,
          "n_counter_biased": 163,
          "n_unknown": 963
        },
        "age": {
          "accuracy": 0.26686507936507936,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3948412698412698,
          "n_biased": 1137,
          "n_counter_biased": 341,
          "n_unknown": 538
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.689176245210728,
          "total": 2088,
          "correct": 1439
        },
        "gender": {
          "accuracy": 0.6169871794871795,
          "total": 1872,
          "correct": 1155
        },
        "age": {
          "accuracy": 0.6011904761904762,
          "total": 2016,
          "correct": 1212
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.5849230254350736,
      "bias_accuracy": 0.5348616688978135,
      "bias_rate": 0.3863788487282463,
      "bias_score": 0.30761936635430615,
      "bias_score_details": {
        "n_biased": 2309.0,
        "n_counter_biased": 470.6666666666667,
        "n_unknown": 3196.3333333333335,
        "n_valid": 5976.0
      },
      "culture_accuracy": 0.6349843819723339,
      "culture_total": 5976.0,
      "culture_valid": 5976.0,
      "culture_correct": 3794.6666666666665,
      "culture_incorrect": 1909.0,
      "culture_unknown": 272.3333333333333,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.75,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.23882503192848023,
          "n_biased": 510.3333333333333,
          "n_counter_biased": 11.666666666666666,
          "n_unknown": 1566.0
        },
        "gender": {
          "accuracy": 0.5121082621082621,
          "total": 1872.0,
          "valid": 1872.0,
          "bias_score": 0.2962962962962963,
          "n_biased": 734.0,
          "n_counter_biased": 179.33333333333334,
          "n_unknown": 958.6666666666666
        },
        "age": {
          "accuracy": 0.33316798941798936,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.38938492063492064,
          "n_biased": 1064.6666666666667,
          "n_counter_biased": 279.6666666666667,
          "n_unknown": 671.6666666666666
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.7203065134099617,
          "total": 2088.0,
          "correct": 1504.0
        },
        "gender": {
          "accuracy": 0.5931267806267806,
          "total": 1872.0,
          "correct": 1110.3333333333333
        },
        "age": {
          "accuracy": 0.5854828042328042,
          "total": 2016.0,
          "correct": 1180.3333333333333
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}