{
  "model": "qwen-3-30b-inst",
  "dataset": "csv/ko-zh-v2_dataset.csv",
  "timestamp": "2026-01-01T01:19:25.444788",
  "metrics": {
    "overall_accuracy": 0.4038128249566724,
    "bias_accuracy": 0.020370370370370372,
    "bias_rate": 0.011772486772486773,
    "bias_score": 0.012195121951219513,
    "bias_score_details": {
      "n_biased": 89,
      "n_counter_biased": 85,
      "n_unknown": 154,
      "n_valid": 328
    },
    "culture_accuracy": 0.3172690763052209,
    "culture_total": 7560,
    "culture_valid": 249,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.672566371681416,
        "total": 2088,
        "valid": 113,
        "bias_score": -0.04424778761061947,
        "n_biased": 16,
        "n_counter_biased": 21,
        "n_unknown": 76
      },
      "gender": {
        "accuracy": 0.1917808219178082,
        "total": 1584,
        "valid": 73,
        "bias_score": -0.0684931506849315,
        "n_biased": 27,
        "n_counter_biased": 32,
        "n_unknown": 14
      },
      "age": {
        "accuracy": 0.23376623376623376,
        "total": 2016,
        "valid": 77,
        "bias_score": 0.19480519480519481,
        "n_biased": 37,
        "n_counter_biased": 22,
        "n_unknown": 18
      },
      "title": {
        "accuracy": 0.5909090909090909,
        "total": 1008,
        "valid": 22,
        "bias_score": -0.045454545454545456,
        "n_biased": 4,
        "n_counter_biased": 5,
        "n_unknown": 13
      },
      "elitism": {
        "accuracy": 0.7674418604651163,
        "total": 864,
        "valid": 43,
        "bias_score": 0.0,
        "n_biased": 5,
        "n_counter_biased": 5,
        "n_unknown": 33
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.008620689655172414,
        "total": 2088,
        "correct": 18
      },
      "gender": {
        "accuracy": 0.015782828282828284,
        "total": 1584,
        "correct": 25
      },
      "age": {
        "accuracy": 0.01240079365079365,
        "total": 2016,
        "correct": 25
      },
      "title": {
        "accuracy": 0.00992063492063492,
        "total": 1008,
        "correct": 10
      },
      "elitism": {
        "accuracy": 0.0011574074074074073,
        "total": 864,
        "correct": 1
      }
    },
    "invalid_predictions": 14543,
    "invalid_rate": 0.9618386243386243
  }
}