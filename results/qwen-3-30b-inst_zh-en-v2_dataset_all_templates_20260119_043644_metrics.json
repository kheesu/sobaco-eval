{
  "model": "qwen-3-30b-inst",
  "dataset": "csv/zh-en-v2_dataset.csv",
  "timestamp": "2026-01-19T04:36:44.034040",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.6557098765432099,
      "bias_accuracy": 0.5330246913580247,
      "bias_rate": 0.4396604938271605,
      "bias_score": 0.4123456790123457,
      "bias_score_details": {
        "n_biased": 2849,
        "n_counter_biased": 177,
        "n_unknown": 3454,
        "n_valid": 6480
      },
      "culture_accuracy": 0.778395061728395,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 5044,
      "culture_incorrect": 240,
      "culture_unknown": 1196,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9851851851851852,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.014814814814814815,
          "n_biased": 32,
          "n_counter_biased": 0,
          "n_unknown": 2128
        },
        "gender": {
          "accuracy": 0.17962962962962964,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6731481481481482,
          "n_biased": 1613,
          "n_counter_biased": 159,
          "n_unknown": 388
        },
        "hierarchical_relationship": {
          "accuracy": 0.43425925925925923,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.549074074074074,
          "n_biased": 1204,
          "n_counter_biased": 18,
          "n_unknown": 938
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5625,
          "total": 2160,
          "correct": 1215
        },
        "gender": {
          "accuracy": 0.9805555555555555,
          "total": 2160,
          "correct": 2118
        },
        "hierarchical_relationship": {
          "accuracy": 0.7921296296296296,
          "total": 2160,
          "correct": 1711
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.6655864197530864,
      "bias_accuracy": 0.5546296296296296,
      "bias_rate": 0.4141975308641975,
      "bias_score": 0.3830246913580247,
      "bias_score_details": {
        "n_biased": 2684,
        "n_counter_biased": 202,
        "n_unknown": 3594,
        "n_valid": 6480
      },
      "culture_accuracy": 0.7765432098765432,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 5032,
      "culture_incorrect": 300,
      "culture_unknown": 1148,
      "per_category_bias": {
        "age": {
          "accuracy": 0.987037037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.012962962962962963,
          "n_biased": 28,
          "n_counter_biased": 0,
          "n_unknown": 2132
        },
        "gender": {
          "accuracy": 0.21898148148148147,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6162037037037037,
          "n_biased": 1509,
          "n_counter_biased": 178,
          "n_unknown": 473
        },
        "hierarchical_relationship": {
          "accuracy": 0.45787037037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.5199074074074074,
          "n_biased": 1147,
          "n_counter_biased": 24,
          "n_unknown": 989
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.562037037037037,
          "total": 2160,
          "correct": 1214
        },
        "gender": {
          "accuracy": 0.9722222222222222,
          "total": 2160,
          "correct": 2100
        },
        "hierarchical_relationship": {
          "accuracy": 0.7953703703703704,
          "total": 2160,
          "correct": 1718
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.6318672839506173,
      "bias_accuracy": 0.4847222222222222,
      "bias_rate": 0.47160493827160493,
      "bias_score": 0.4279320987654321,
      "bias_score_details": {
        "n_biased": 3056,
        "n_counter_biased": 283,
        "n_unknown": 3141,
        "n_valid": 6480
      },
      "culture_accuracy": 0.7790123456790123,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 5048,
      "culture_incorrect": 353,
      "culture_unknown": 1079,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9861111111111112,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.012037037037037037,
          "n_biased": 28,
          "n_counter_biased": 2,
          "n_unknown": 2130
        },
        "gender": {
          "accuracy": 0.11527777777777778,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6699074074074074,
          "n_biased": 1679,
          "n_counter_biased": 232,
          "n_unknown": 249
        },
        "hierarchical_relationship": {
          "accuracy": 0.3527777777777778,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6018518518518519,
          "n_biased": 1349,
          "n_counter_biased": 49,
          "n_unknown": 762
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5087962962962963,
          "total": 2160,
          "correct": 1099
        },
        "gender": {
          "accuracy": 0.999537037037037,
          "total": 2160,
          "correct": 2159
        },
        "hierarchical_relationship": {
          "accuracy": 0.8287037037037037,
          "total": 2160,
          "correct": 1790
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.6510545267489712,
      "bias_accuracy": 0.5241255144032922,
      "bias_rate": 0.44182098765432104,
      "bias_score": 0.40776748971193416,
      "bias_score_details": {
        "n_biased": 2863.0,
        "n_counter_biased": 220.66666666666666,
        "n_unknown": 3396.3333333333335,
        "n_valid": 6480.0
      },
      "culture_accuracy": 0.7779835390946502,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 5041.333333333333,
      "culture_incorrect": 297.6666666666667,
      "culture_unknown": 1141.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9861111111111112,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.013271604938271606,
          "n_biased": 29.333333333333332,
          "n_counter_biased": 0.6666666666666666,
          "n_unknown": 2130.0
        },
        "gender": {
          "accuracy": 0.1712962962962963,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.6530864197530865,
          "n_biased": 1600.3333333333333,
          "n_counter_biased": 189.66666666666666,
          "n_unknown": 370.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.41496913580246914,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.5569444444444444,
          "n_biased": 1233.3333333333333,
          "n_counter_biased": 30.333333333333332,
          "n_unknown": 896.3333333333334
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5444444444444444,
          "total": 2160.0,
          "correct": 1176.0
        },
        "gender": {
          "accuracy": 0.9841049382716048,
          "total": 2160.0,
          "correct": 2125.6666666666665
        },
        "hierarchical_relationship": {
          "accuracy": 0.8054012345679012,
          "total": 2160.0,
          "correct": 1739.6666666666667
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}