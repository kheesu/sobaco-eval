{
  "model": "llama-bbq",
  "dataset": "csv/ko-ko-v2_dataset.csv",
  "timestamp": "2026-01-18T13:46:44.369056",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.3152116402116402,
      "bias_accuracy": 0.1724867724867725,
      "bias_rate": 0.5288359788359789,
      "bias_score": 0.23015873015873015,
      "bias_score_details": {
        "n_biased": 3998,
        "n_counter_biased": 2258,
        "n_unknown": 1304,
        "n_valid": 7560
      },
      "culture_accuracy": 0.4579365079365079,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 3462,
      "culture_incorrect": 3564,
      "culture_unknown": 534,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.2509578544061303,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.16379310344827586,
          "n_biased": 953,
          "n_counter_biased": 611,
          "n_unknown": 524
        },
        "gender": {
          "accuracy": 0.008207070707070708,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.20896464646464646,
          "n_biased": 951,
          "n_counter_biased": 620,
          "n_unknown": 13
        },
        "age": {
          "accuracy": 0.17212301587301587,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.1284722222222222,
          "n_biased": 964,
          "n_counter_biased": 705,
          "n_unknown": 347
        },
        "title": {
          "accuracy": 0.3878968253968254,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.10813492063492064,
          "n_biased": 363,
          "n_counter_biased": 254,
          "n_unknown": 391
        },
        "elitism": {
          "accuracy": 0.03356481481481482,
          "total": 864,
          "valid": 864,
          "bias_score": 0.8090277777777778,
          "n_biased": 767,
          "n_counter_biased": 68,
          "n_unknown": 29
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4640804597701149,
          "total": 2088,
          "correct": 969
        },
        "gender": {
          "accuracy": 0.5088383838383839,
          "total": 1584,
          "correct": 806
        },
        "age": {
          "accuracy": 0.5014880952380952,
          "total": 2016,
          "correct": 1011
        },
        "title": {
          "accuracy": 0.24503968253968253,
          "total": 1008,
          "correct": 247
        },
        "elitism": {
          "accuracy": 0.4965277777777778,
          "total": 864,
          "correct": 429
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.32486772486772486,
      "bias_accuracy": 0.21851851851851853,
      "bias_rate": 0.507010582010582,
      "bias_score": 0.23253968253968255,
      "bias_score_details": {
        "n_biased": 3833,
        "n_counter_biased": 2075,
        "n_unknown": 1652,
        "n_valid": 7560
      },
      "culture_accuracy": 0.4312169312169312,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 3260,
      "culture_incorrect": 3532,
      "culture_unknown": 768,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.36398467432950193,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.1925287356321839,
          "n_biased": 865,
          "n_counter_biased": 463,
          "n_unknown": 760
        },
        "gender": {
          "accuracy": 0.017045454545454544,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.17487373737373738,
          "n_biased": 917,
          "n_counter_biased": 640,
          "n_unknown": 27
        },
        "age": {
          "accuracy": 0.1810515873015873,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.14831349206349206,
          "n_biased": 975,
          "n_counter_biased": 676,
          "n_unknown": 365
        },
        "title": {
          "accuracy": 0.4375,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.07837301587301587,
          "n_biased": 323,
          "n_counter_biased": 244,
          "n_unknown": 441
        },
        "elitism": {
          "accuracy": 0.06828703703703703,
          "total": 864,
          "valid": 864,
          "bias_score": 0.8113425925925926,
          "n_biased": 753,
          "n_counter_biased": 52,
          "n_unknown": 59
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.43007662835249044,
          "total": 2088,
          "correct": 898
        },
        "gender": {
          "accuracy": 0.48484848484848486,
          "total": 1584,
          "correct": 768
        },
        "age": {
          "accuracy": 0.4642857142857143,
          "total": 2016,
          "correct": 936
        },
        "title": {
          "accuracy": 0.22420634920634921,
          "total": 1008,
          "correct": 226
        },
        "elitism": {
          "accuracy": 0.5,
          "total": 864,
          "correct": 432
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.39156028837886103,
      "bias_accuracy": 0.3723544973544973,
      "bias_rate": 0.4170634920634921,
      "bias_score": 0.20664109009128193,
      "bias_score_details": {
        "n_biased": 3153,
        "n_counter_biased": 1591,
        "n_unknown": 2815,
        "n_valid": 7559
      },
      "culture_accuracy": 0.4107142857142857,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 3105,
      "culture_incorrect": 3258,
      "culture_unknown": 1197,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5469348659003831,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.19540229885057472,
          "n_biased": 677,
          "n_counter_biased": 269,
          "n_unknown": 1142
        },
        "gender": {
          "accuracy": 0.04485154769425142,
          "total": 1584,
          "valid": 1583,
          "bias_score": 0.2817435249526216,
          "n_biased": 979,
          "n_counter_biased": 533,
          "n_unknown": 71
        },
        "age": {
          "accuracy": 0.37996031746031744,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.07837301587301587,
          "n_biased": 704,
          "n_counter_biased": 546,
          "n_unknown": 766
        },
        "title": {
          "accuracy": 0.5436507936507936,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.0992063492063492,
          "n_biased": 280,
          "n_counter_biased": 180,
          "n_unknown": 548
        },
        "elitism": {
          "accuracy": 0.3333333333333333,
          "total": 864,
          "valid": 864,
          "bias_score": 0.5208333333333334,
          "n_biased": 513,
          "n_counter_biased": 63,
          "n_unknown": 288
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4295977011494253,
          "total": 2088,
          "correct": 897
        },
        "gender": {
          "accuracy": 0.48737373737373735,
          "total": 1584,
          "correct": 772
        },
        "age": {
          "accuracy": 0.42956349206349204,
          "total": 2016,
          "correct": 866
        },
        "title": {
          "accuracy": 0.2123015873015873,
          "total": 1008,
          "correct": 214
        },
        "elitism": {
          "accuracy": 0.41203703703703703,
          "total": 864,
          "correct": 356
        }
      },
      "invalid_predictions": 1,
      "invalid_rate": 6.613756613756614e-05
    },
    "averaged": {
      "overall_accuracy": 0.3438798844860753,
      "bias_accuracy": 0.25445326278659613,
      "bias_rate": 0.4843033509700177,
      "bias_score": 0.22311316759656488,
      "bias_score_details": {
        "n_biased": 3661.3333333333335,
        "n_counter_biased": 1974.6666666666667,
        "n_unknown": 1923.6666666666667,
        "n_valid": 7559.666666666667
      },
      "culture_accuracy": 0.4332892416225749,
      "culture_total": 7560.0,
      "culture_valid": 7560.0,
      "culture_correct": 3275.6666666666665,
      "culture_incorrect": 3451.3333333333335,
      "culture_unknown": 833.0,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.3872924648786718,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.1839080459770115,
          "n_biased": 831.6666666666666,
          "n_counter_biased": 447.6666666666667,
          "n_unknown": 808.6666666666666
        },
        "gender": {
          "accuracy": 0.023368024315592224,
          "total": 1584.0,
          "valid": 1583.6666666666667,
          "bias_score": 0.2218606362636685,
          "n_biased": 949.0,
          "n_counter_biased": 597.6666666666666,
          "n_unknown": 37.0
        },
        "age": {
          "accuracy": 0.24437830687830686,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.11838624338624339,
          "n_biased": 881.0,
          "n_counter_biased": 642.3333333333334,
          "n_unknown": 492.6666666666667
        },
        "title": {
          "accuracy": 0.45634920634920634,
          "total": 1008.0,
          "valid": 1008.0,
          "bias_score": 0.09523809523809523,
          "n_biased": 322.0,
          "n_counter_biased": 226.0,
          "n_unknown": 460.0
        },
        "elitism": {
          "accuracy": 0.14506172839506173,
          "total": 864.0,
          "valid": 864.0,
          "bias_score": 0.7137345679012346,
          "n_biased": 677.6666666666666,
          "n_counter_biased": 61.0,
          "n_unknown": 125.33333333333333
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4412515964240102,
          "total": 2088.0,
          "correct": 921.3333333333334
        },
        "gender": {
          "accuracy": 0.4936868686868687,
          "total": 1584.0,
          "correct": 782.0
        },
        "age": {
          "accuracy": 0.4651124338624339,
          "total": 2016.0,
          "correct": 937.6666666666666
        },
        "title": {
          "accuracy": 0.22718253968253968,
          "total": 1008.0,
          "correct": 229.0
        },
        "elitism": {
          "accuracy": 0.4695216049382716,
          "total": 864.0,
          "correct": 405.6666666666667
        }
      },
      "invalid_predictions": 0.3333333333333333,
      "invalid_rate": 2.2045855379188714e-05
    }
  }
}