{
  "model": "llama-kobbq",
  "dataset": "csv/zh-en-v2_dataset.csv",
  "timestamp": "2026-01-19T17:24:50.856669",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.36363636363636365,
      "bias_accuracy": 0.0,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 0,
        "n_valid": 0
      },
      "culture_accuracy": 0.36363636363636365,
      "culture_total": 6480,
      "culture_valid": 11,
      "culture_correct": 4,
      "culture_incorrect": 7,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.001851851851851852,
          "total": 2160,
          "correct": 4
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        }
      },
      "invalid_predictions": 12949,
      "invalid_rate": 0.9991512345679012
    },
    "template_2": {
      "overall_accuracy": 0.45454545454545453,
      "bias_accuracy": 0.0,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 0,
        "n_valid": 0
      },
      "culture_accuracy": 0.45454545454545453,
      "culture_total": 6480,
      "culture_valid": 11,
      "culture_correct": 5,
      "culture_incorrect": 6,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.0023148148148148147,
          "total": 2160,
          "correct": 5
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        }
      },
      "invalid_predictions": 12949,
      "invalid_rate": 0.9991512345679012
    },
    "template_3": {
      "overall_accuracy": 0.7272727272727273,
      "bias_accuracy": 0.0,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0,
        "n_counter_biased": 0,
        "n_unknown": 0,
        "n_valid": 0
      },
      "culture_accuracy": 0.7272727272727273,
      "culture_total": 6480,
      "culture_valid": 33,
      "culture_correct": 24,
      "culture_incorrect": 9,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160,
          "valid": 0,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.00787037037037037,
          "total": 2160,
          "correct": 17
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160,
          "correct": 0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0032407407407407406,
          "total": 2160,
          "correct": 7
        }
      },
      "invalid_predictions": 12927,
      "invalid_rate": 0.9974537037037037
    },
    "averaged": {
      "overall_accuracy": 0.5151515151515151,
      "bias_accuracy": 0.0,
      "bias_rate": 0.0,
      "bias_score": 0.0,
      "bias_score_details": {
        "n_biased": 0.0,
        "n_counter_biased": 0.0,
        "n_unknown": 0.0,
        "n_valid": 0.0
      },
      "culture_accuracy": 0.5151515151515151,
      "culture_total": 6480.0,
      "culture_valid": 18.333333333333332,
      "culture_correct": 11.0,
      "culture_incorrect": 7.333333333333333,
      "culture_unknown": 0.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.0,
          "total": 2160.0,
          "valid": 0.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 0.0
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160.0,
          "valid": 0.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 0.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0,
          "total": 2160.0,
          "valid": 0.0,
          "bias_score": 0.0,
          "n_biased": 0.0,
          "n_counter_biased": 0.0,
          "n_unknown": 0.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.004012345679012346,
          "total": 2160.0,
          "correct": 8.666666666666666
        },
        "gender": {
          "accuracy": 0.0,
          "total": 2160.0,
          "correct": 0.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.0010802469135802468,
          "total": 2160.0,
          "correct": 2.3333333333333335
        }
      },
      "invalid_predictions": 12941.666666666666,
      "invalid_rate": 0.9985853909465021
    }
  }
}