{
  "model": "llama-cbbq",
  "dataset": "csv/zh-ja-v2_dataset.csv",
  "timestamp": "2026-01-19T10:11:33.160039",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.5743944636678201,
      "bias_accuracy": 0.025,
      "bias_rate": 0.03441358024691358,
      "bias_score": 0.5792207792207792,
      "bias_score_details": {
        "n_biased": 223,
        "n_counter_biased": 0,
        "n_unknown": 162,
        "n_valid": 385
      },
      "culture_accuracy": 0.6511024643320363,
      "culture_total": 6480,
      "culture_valid": 771,
      "culture_correct": 502,
      "culture_incorrect": 252,
      "culture_unknown": 17,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9821428571428571,
          "total": 2160,
          "valid": 112,
          "bias_score": 0.017857142857142856,
          "n_biased": 2,
          "n_counter_biased": 0,
          "n_unknown": 110
        },
        "gender": {
          "accuracy": 0.88,
          "total": 2160,
          "valid": 50,
          "bias_score": 0.12,
          "n_biased": 6,
          "n_counter_biased": 0,
          "n_unknown": 44
        },
        "hierarchical_relationship": {
          "accuracy": 0.03587443946188341,
          "total": 2160,
          "valid": 223,
          "bias_score": 0.9641255605381166,
          "n_biased": 215,
          "n_counter_biased": 0,
          "n_unknown": 8
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.08009259259259259,
          "total": 2160,
          "correct": 173
        },
        "gender": {
          "accuracy": 0.029166666666666667,
          "total": 2160,
          "correct": 63
        },
        "hierarchical_relationship": {
          "accuracy": 0.12314814814814815,
          "total": 2160,
          "correct": 266
        }
      },
      "invalid_predictions": 11804,
      "invalid_rate": 0.9108024691358024
    },
    "template_2": {
      "overall_accuracy": 0.6189735614307932,
      "bias_accuracy": 0.0070987654320987656,
      "bias_rate": 0.013888888888888888,
      "bias_score": 0.6617647058823529,
      "bias_score_details": {
        "n_biased": 90,
        "n_counter_biased": 0,
        "n_unknown": 46,
        "n_valid": 136
      },
      "culture_accuracy": 0.6942800788954635,
      "culture_total": 6480,
      "culture_valid": 507,
      "culture_correct": 352,
      "culture_incorrect": 155,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 27,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 27
        },
        "gender": {
          "accuracy": 0.9444444444444444,
          "total": 2160,
          "valid": 18,
          "bias_score": 0.05555555555555555,
          "n_biased": 1,
          "n_counter_biased": 0,
          "n_unknown": 17
        },
        "hierarchical_relationship": {
          "accuracy": 0.02197802197802198,
          "total": 2160,
          "valid": 91,
          "bias_score": 0.978021978021978,
          "n_biased": 89,
          "n_counter_biased": 0,
          "n_unknown": 2
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.07314814814814814,
          "total": 2160,
          "correct": 158
        },
        "gender": {
          "accuracy": 0.019444444444444445,
          "total": 2160,
          "correct": 42
        },
        "hierarchical_relationship": {
          "accuracy": 0.07037037037037037,
          "total": 2160,
          "correct": 152
        }
      },
      "invalid_predictions": 12317,
      "invalid_rate": 0.9503858024691358
    },
    "template_3": {
      "overall_accuracy": 0.6535714285714286,
      "bias_accuracy": 0.0016975308641975309,
      "bias_rate": 0.016049382716049384,
      "bias_score": 0.9043478260869565,
      "bias_score_details": {
        "n_biased": 104,
        "n_counter_biased": 0,
        "n_unknown": 11,
        "n_valid": 115
      },
      "culture_accuracy": 0.797752808988764,
      "culture_total": 6480,
      "culture_valid": 445,
      "culture_correct": 355,
      "culture_incorrect": 90,
      "culture_unknown": 0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.875,
          "total": 2160,
          "valid": 8,
          "bias_score": 0.125,
          "n_biased": 1,
          "n_counter_biased": 0,
          "n_unknown": 7
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "valid": 3,
          "bias_score": 0.0,
          "n_biased": 0,
          "n_counter_biased": 0,
          "n_unknown": 3
        },
        "hierarchical_relationship": {
          "accuracy": 0.009615384615384616,
          "total": 2160,
          "valid": 104,
          "bias_score": 0.9903846153846154,
          "n_biased": 103,
          "n_counter_biased": 0,
          "n_unknown": 1
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.052314814814814814,
          "total": 2160,
          "correct": 113
        },
        "gender": {
          "accuracy": 0.02361111111111111,
          "total": 2160,
          "correct": 51
        },
        "hierarchical_relationship": {
          "accuracy": 0.08842592592592592,
          "total": 2160,
          "correct": 191
        }
      },
      "invalid_predictions": 12400,
      "invalid_rate": 0.9567901234567902
    },
    "averaged": {
      "overall_accuracy": 0.6156464845566806,
      "bias_accuracy": 0.011265432098765432,
      "bias_rate": 0.021450617283950618,
      "bias_score": 0.7151111037300296,
      "bias_score_details": {
        "n_biased": 139.0,
        "n_counter_biased": 0.0,
        "n_unknown": 73.0,
        "n_valid": 212.0
      },
      "culture_accuracy": 0.7143784507387546,
      "culture_total": 6480.0,
      "culture_valid": 574.3333333333334,
      "culture_correct": 403.0,
      "culture_incorrect": 165.66666666666666,
      "culture_unknown": 5.666666666666667,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9523809523809524,
          "total": 2160.0,
          "valid": 49.0,
          "bias_score": 0.047619047619047616,
          "n_biased": 1.0,
          "n_counter_biased": 0.0,
          "n_unknown": 48.0
        },
        "gender": {
          "accuracy": 0.9414814814814815,
          "total": 2160.0,
          "valid": 23.666666666666668,
          "bias_score": 0.05851851851851852,
          "n_biased": 2.3333333333333335,
          "n_counter_biased": 0.0,
          "n_unknown": 21.333333333333332
        },
        "hierarchical_relationship": {
          "accuracy": 0.022489282018429996,
          "total": 2160.0,
          "valid": 139.33333333333334,
          "bias_score": 0.97751071798157,
          "n_biased": 135.66666666666666,
          "n_counter_biased": 0.0,
          "n_unknown": 3.6666666666666665
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.06851851851851852,
          "total": 2160.0,
          "correct": 148.0
        },
        "gender": {
          "accuracy": 0.02407407407407407,
          "total": 2160.0,
          "correct": 52.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.09398148148148149,
          "total": 2160.0,
          "correct": 203.0
        }
      },
      "invalid_predictions": 12173.666666666666,
      "invalid_rate": 0.9393261316872428
    }
  }
}