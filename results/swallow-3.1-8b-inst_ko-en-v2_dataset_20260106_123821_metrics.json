{
  "model": "swallow-3.1-8b-inst",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-06T12:38:21.236185",
  "metrics": {
    "overall_accuracy": 0.32936507936507936,
    "bias_accuracy": 0.29484126984126985,
    "bias_rate": 0.39986772486772487,
    "bias_score": 0.09457671957671958,
    "bias_score_details": {
      "n_biased": 3023,
      "n_counter_biased": 2308,
      "n_unknown": 2229,
      "n_valid": 7560
    },
    "culture_accuracy": 0.3638888888888889,
    "culture_total": 7560,
    "culture_valid": 7560,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.4779693486590038,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.16666666666666666,
        "n_biased": 719,
        "n_counter_biased": 371,
        "n_unknown": 998
      },
      "gender": {
        "accuracy": 0.0726010101010101,
        "total": 1584,
        "valid": 1584,
        "bias_score": -0.032196969696969696,
        "n_biased": 709,
        "n_counter_biased": 760,
        "n_unknown": 115
      },
      "age": {
        "accuracy": 0.22073412698412698,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.18601190476190477,
        "n_biased": 973,
        "n_counter_biased": 598,
        "n_unknown": 445
      },
      "title": {
        "accuracy": 0.24404761904761904,
        "total": 1008,
        "valid": 1008,
        "bias_score": -0.2222222222222222,
        "n_biased": 269,
        "n_counter_biased": 493,
        "n_unknown": 246
      },
      "elitism": {
        "accuracy": 0.49189814814814814,
        "total": 864,
        "valid": 864,
        "bias_score": 0.3090277777777778,
        "n_biased": 353,
        "n_counter_biased": 86,
        "n_unknown": 425
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.3544061302681992,
        "total": 2088,
        "correct": 740
      },
      "gender": {
        "accuracy": 0.3680555555555556,
        "total": 1584,
        "correct": 583
      },
      "age": {
        "accuracy": 0.34970238095238093,
        "total": 2016,
        "correct": 705
      },
      "title": {
        "accuracy": 0.4117063492063492,
        "total": 1008,
        "correct": 415
      },
      "elitism": {
        "accuracy": 0.35648148148148145,
        "total": 864,
        "correct": 308
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}