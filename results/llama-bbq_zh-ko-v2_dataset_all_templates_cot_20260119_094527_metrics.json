{
  "model": "llama-bbq",
  "dataset": "csv/zh-ko-v2_dataset.csv",
  "timestamp": "2026-01-19T09:45:27.519911",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.39145613229214515,
      "bias_accuracy": 0.08070987654320988,
      "bias_rate": 0.512962962962963,
      "bias_score": 0.40195503421309875,
      "bias_score_details": {
        "n_biased": 3324,
        "n_counter_biased": 1268,
        "n_unknown": 523,
        "n_valid": 5115
      },
      "culture_accuracy": 0.6478336221837089,
      "culture_total": 6480,
      "culture_valid": 5770,
      "culture_correct": 3738,
      "culture_incorrect": 1258,
      "culture_unknown": 774,
      "per_category_bias": {
        "age": {
          "accuracy": 0.1260096930533118,
          "total": 2160,
          "valid": 1857,
          "bias_score": 0.35918147549811524,
          "n_biased": 1145,
          "n_counter_biased": 478,
          "n_unknown": 234
        },
        "gender": {
          "accuracy": 0.09202851587815943,
          "total": 2160,
          "valid": 1543,
          "bias_score": 0.2508101101749838,
          "n_biased": 894,
          "n_counter_biased": 507,
          "n_unknown": 142
        },
        "hierarchical_relationship": {
          "accuracy": 0.08571428571428572,
          "total": 2160,
          "valid": 1715,
          "bias_score": 0.5842565597667638,
          "n_biased": 1285,
          "n_counter_biased": 283,
          "n_unknown": 147
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5037037037037037,
          "total": 2160,
          "correct": 1088
        },
        "gender": {
          "accuracy": 0.7240740740740741,
          "total": 2160,
          "correct": 1564
        },
        "hierarchical_relationship": {
          "accuracy": 0.5027777777777778,
          "total": 2160,
          "correct": 1086
        }
      },
      "invalid_predictions": 2075,
      "invalid_rate": 0.16010802469135801
    },
    "template_2": {
      "overall_accuracy": 0.38032226122963364,
      "bias_accuracy": 0.11805555555555555,
      "bias_rate": 0.4601851851851852,
      "bias_score": 0.2898217366302473,
      "bias_score_details": {
        "n_biased": 2982,
        "n_counter_biased": 1470,
        "n_unknown": 765,
        "n_valid": 5217
      },
      "culture_accuracy": 0.5872369314324508,
      "culture_total": 6480,
      "culture_valid": 5892,
      "culture_correct": 3460,
      "culture_incorrect": 1407,
      "culture_unknown": 1025,
      "per_category_bias": {
        "age": {
          "accuracy": 0.15828480677607198,
          "total": 2160,
          "valid": 1889,
          "bias_score": 0.25516146109052407,
          "n_biased": 1036,
          "n_counter_biased": 554,
          "n_unknown": 299
        },
        "gender": {
          "accuracy": 0.14040465971796443,
          "total": 2160,
          "valid": 1631,
          "bias_score": 0.2317596566523605,
          "n_biased": 890,
          "n_counter_biased": 512,
          "n_unknown": 229
        },
        "hierarchical_relationship": {
          "accuracy": 0.13965822038892162,
          "total": 2160,
          "valid": 1697,
          "bias_score": 0.3842074248674131,
          "n_biased": 1056,
          "n_counter_biased": 404,
          "n_unknown": 237
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.46620370370370373,
          "total": 2160,
          "correct": 1007
        },
        "gender": {
          "accuracy": 0.6773148148148148,
          "total": 2160,
          "correct": 1463
        },
        "hierarchical_relationship": {
          "accuracy": 0.4583333333333333,
          "total": 2160,
          "correct": 990
        }
      },
      "invalid_predictions": 1851,
      "invalid_rate": 0.14282407407407408
    },
    "template_3": {
      "overall_accuracy": 0.4128696657354783,
      "bias_accuracy": 0.05848765432098765,
      "bias_rate": 0.45632716049382716,
      "bias_score": 0.3524607779926929,
      "bias_score_details": {
        "n_biased": 2957,
        "n_counter_biased": 1317,
        "n_unknown": 379,
        "n_valid": 4653
      },
      "culture_accuracy": 0.6820879888268156,
      "culture_total": 6480,
      "culture_valid": 5728,
      "culture_correct": 3907,
      "culture_incorrect": 1160,
      "culture_unknown": 661,
      "per_category_bias": {
        "age": {
          "accuracy": 0.09448356807511737,
          "total": 2160,
          "valid": 1704,
          "bias_score": 0.29284037558685444,
          "n_biased": 1021,
          "n_counter_biased": 522,
          "n_unknown": 161
        },
        "gender": {
          "accuracy": 0.05729513657561625,
          "total": 2160,
          "valid": 1501,
          "bias_score": 0.24050632911392406,
          "n_biased": 888,
          "n_counter_biased": 527,
          "n_unknown": 86
        },
        "hierarchical_relationship": {
          "accuracy": 0.09116022099447514,
          "total": 2160,
          "valid": 1448,
          "bias_score": 0.5386740331491713,
          "n_biased": 1048,
          "n_counter_biased": 268,
          "n_unknown": 132
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.4583333333333333,
          "total": 2160,
          "correct": 990
        },
        "gender": {
          "accuracy": 0.8111111111111111,
          "total": 2160,
          "correct": 1752
        },
        "hierarchical_relationship": {
          "accuracy": 0.5393518518518519,
          "total": 2160,
          "correct": 1165
        }
      },
      "invalid_predictions": 2579,
      "invalid_rate": 0.1989969135802469
    },
    "averaged": {
      "overall_accuracy": 0.3948826864190857,
      "bias_accuracy": 0.08575102880658436,
      "bias_rate": 0.47649176954732503,
      "bias_score": 0.3480791829453463,
      "bias_score_details": {
        "n_biased": 3087.6666666666665,
        "n_counter_biased": 1351.6666666666667,
        "n_unknown": 555.6666666666666,
        "n_valid": 4995.0
      },
      "culture_accuracy": 0.6390528474809918,
      "culture_total": 6480.0,
      "culture_valid": 5796.666666666667,
      "culture_correct": 3701.6666666666665,
      "culture_incorrect": 1275.0,
      "culture_unknown": 820.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.12625935596816706,
          "total": 2160.0,
          "valid": 1816.6666666666667,
          "bias_score": 0.30239443739183125,
          "n_biased": 1067.3333333333333,
          "n_counter_biased": 518.0,
          "n_unknown": 231.33333333333334
        },
        "gender": {
          "accuracy": 0.0965761040572467,
          "total": 2160.0,
          "valid": 1558.3333333333333,
          "bias_score": 0.2410253653137561,
          "n_biased": 890.6666666666666,
          "n_counter_biased": 515.3333333333334,
          "n_unknown": 152.33333333333334
        },
        "hierarchical_relationship": {
          "accuracy": 0.10551090903256083,
          "total": 2160.0,
          "valid": 1620.0,
          "bias_score": 0.5023793392611161,
          "n_biased": 1129.6666666666667,
          "n_counter_biased": 318.3333333333333,
          "n_unknown": 172.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.47608024691358025,
          "total": 2160.0,
          "correct": 1028.3333333333333
        },
        "gender": {
          "accuracy": 0.7375000000000002,
          "total": 2160.0,
          "correct": 1593.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.5001543209876543,
          "total": 2160.0,
          "correct": 1080.3333333333333
        }
      },
      "invalid_predictions": 2168.3333333333335,
      "invalid_rate": 0.167309670781893
    }
  }
}