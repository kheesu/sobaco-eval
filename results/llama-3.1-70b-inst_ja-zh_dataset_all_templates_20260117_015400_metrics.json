{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/ja-zh_dataset.csv",
  "timestamp": "2026-01-17T01:54:00.678891",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.42288057578040006,
      "bias_accuracy": 0.3539156626506024,
      "bias_rate": 0.482764390896921,
      "bias_score": 0.3201071488364306,
      "bias_score_details": {
        "n_biased": 2885,
        "n_counter_biased": 973,
        "n_unknown": 2115,
        "n_valid": 5973
      },
      "culture_accuracy": 0.4916331994645248,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2938,
      "culture_incorrect": 2131,
      "culture_unknown": 907,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6513409961685823,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.342911877394636,
          "n_biased": 722,
          "n_counter_biased": 6,
          "n_unknown": 1360
        },
        "gender": {
          "accuracy": 0.17281968967362227,
          "total": 1872,
          "valid": 1869,
          "bias_score": 0.27501337613697163,
          "n_biased": 1030,
          "n_counter_biased": 516,
          "n_unknown": 323
        },
        "age": {
          "accuracy": 0.21428571428571427,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3382936507936508,
          "n_biased": 1133,
          "n_counter_biased": 451,
          "n_unknown": 432
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.5545977011494253,
          "total": 2088,
          "correct": 1158
        },
        "gender": {
          "accuracy": 0.406517094017094,
          "total": 1872,
          "correct": 761
        },
        "age": {
          "accuracy": 0.5054563492063492,
          "total": 2016,
          "correct": 1019
        }
      },
      "invalid_predictions": 3,
      "invalid_rate": 0.000251004016064257
    },
    "template_2": {
      "overall_accuracy": 0.42345358667447897,
      "bias_accuracy": 0.41348728246318606,
      "bias_rate": 0.43022088353413657,
      "bias_score": 0.27499581309663373,
      "bias_score_details": {
        "n_biased": 2571,
        "n_counter_biased": 929,
        "n_unknown": 2471,
        "n_valid": 5971
      },
      "culture_accuracy": 0.4330655957161981,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2588,
      "culture_incorrect": 2002,
      "culture_unknown": 1386,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7150383141762452,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.28400383141762453,
          "n_biased": 594,
          "n_counter_biased": 1,
          "n_unknown": 1493
        },
        "gender": {
          "accuracy": 0.20728441349758972,
          "total": 1872,
          "valid": 1867,
          "bias_score": 0.21638993036957685,
          "n_biased": 942,
          "n_counter_biased": 538,
          "n_unknown": 387
        },
        "age": {
          "accuracy": 0.2931547619047619,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.31994047619047616,
          "n_biased": 1035,
          "n_counter_biased": 390,
          "n_unknown": 591
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.4956896551724138,
          "total": 2088,
          "correct": 1035
        },
        "gender": {
          "accuracy": 0.3974358974358974,
          "total": 1872,
          "correct": 744
        },
        "age": {
          "accuracy": 0.40128968253968256,
          "total": 2016,
          "correct": 809
        }
      },
      "invalid_predictions": 5,
      "invalid_rate": 0.0004183400267737617
    },
    "template_3": {
      "overall_accuracy": 0.4188420348058902,
      "bias_accuracy": 0.3627844712182062,
      "bias_rate": 0.48527443105756357,
      "bias_score": 0.3333333333333333,
      "bias_score_details": {
        "n_biased": 2900,
        "n_counter_biased": 908,
        "n_unknown": 2168,
        "n_valid": 5976
      },
      "culture_accuracy": 0.4748995983935743,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2838,
      "culture_incorrect": 2217,
      "culture_unknown": 921,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6115900383141762,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.3807471264367816,
          "n_biased": 803,
          "n_counter_biased": 8,
          "n_unknown": 1277
        },
        "gender": {
          "accuracy": 0.24145299145299146,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.266025641025641,
          "n_biased": 959,
          "n_counter_biased": 461,
          "n_unknown": 452
        },
        "age": {
          "accuracy": 0.2177579365079365,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.34672619047619047,
          "n_biased": 1138,
          "n_counter_biased": 439,
          "n_unknown": 439
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.5186781609195402,
          "total": 2088,
          "correct": 1083
        },
        "gender": {
          "accuracy": 0.3819444444444444,
          "total": 1872,
          "correct": 715
        },
        "age": {
          "accuracy": 0.5158730158730159,
          "total": 2016,
          "correct": 1040
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.42172539908692314,
      "bias_accuracy": 0.37672913877733155,
      "bias_rate": 0.46608656849620705,
      "bias_score": 0.3094787650887992,
      "bias_score_details": {
        "n_biased": 2785.3333333333335,
        "n_counter_biased": 936.6666666666666,
        "n_unknown": 2251.3333333333335,
        "n_valid": 5973.333333333333
      },
      "culture_accuracy": 0.4665327978580991,
      "culture_total": 5976.0,
      "culture_valid": 5976.0,
      "culture_correct": 2788.0,
      "culture_incorrect": 2116.6666666666665,
      "culture_unknown": 1071.3333333333333,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.6593231162196679,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.33588761174968074,
          "n_biased": 706.3333333333334,
          "n_counter_biased": 5.0,
          "n_unknown": 1376.6666666666667
        },
        "gender": {
          "accuracy": 0.20718569820806784,
          "total": 1872.0,
          "valid": 1869.3333333333333,
          "bias_score": 0.2524763158440632,
          "n_biased": 977.0,
          "n_counter_biased": 505.0,
          "n_unknown": 387.3333333333333
        },
        "age": {
          "accuracy": 0.24173280423280422,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.3349867724867725,
          "n_biased": 1102.0,
          "n_counter_biased": 426.6666666666667,
          "n_unknown": 487.3333333333333
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.5229885057471265,
          "total": 2088.0,
          "correct": 1092.0
        },
        "gender": {
          "accuracy": 0.3952991452991453,
          "total": 1872.0,
          "correct": 740.0
        },
        "age": {
          "accuracy": 0.47420634920634924,
          "total": 2016.0,
          "correct": 956.0
        }
      },
      "invalid_predictions": 2.6666666666666665,
      "invalid_rate": 0.00022311468094600624
    }
  }
}