{
  "model": "llama-kobbq",
  "dataset": "csv/zh-ko-v2_dataset.csv",
  "timestamp": "2026-01-18T13:54:00.877848",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.36810195496164316,
      "bias_accuracy": 0.10802469135802469,
      "bias_rate": 0.3398148148148148,
      "bias_score": 0.10643832646867549,
      "bias_score_details": {
        "n_biased": 2202,
        "n_counter_biased": 1711,
        "n_unknown": 700,
        "n_valid": 4613
      },
      "culture_accuracy": 0.6558085903718651,
      "culture_total": 6480,
      "culture_valid": 3469,
      "culture_correct": 2275,
      "culture_incorrect": 985,
      "culture_unknown": 209,
      "per_category_bias": {
        "age": {
          "accuracy": 0.14298093587521662,
          "total": 2160,
          "valid": 1154,
          "bias_score": 0.10658578856152513,
          "n_biased": 556,
          "n_counter_biased": 433,
          "n_unknown": 165
        },
        "gender": {
          "accuracy": 0.21370143149284254,
          "total": 2160,
          "valid": 1956,
          "bias_score": -0.012269938650306749,
          "n_biased": 757,
          "n_counter_biased": 781,
          "n_unknown": 418
        },
        "hierarchical_relationship": {
          "accuracy": 0.07784431137724551,
          "total": 2160,
          "valid": 1503,
          "bias_score": 0.26081170991350633,
          "n_biased": 889,
          "n_counter_biased": 497,
          "n_unknown": 117
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.20925925925925926,
          "total": 2160,
          "correct": 452
        },
        "gender": {
          "accuracy": 0.4912037037037037,
          "total": 2160,
          "correct": 1061
        },
        "hierarchical_relationship": {
          "accuracy": 0.3527777777777778,
          "total": 2160,
          "correct": 762
        }
      },
      "invalid_predictions": 4878,
      "invalid_rate": 0.3763888888888889
    },
    "template_2": {
      "overall_accuracy": 0.36913169921063566,
      "bias_accuracy": 0.11265432098765432,
      "bias_rate": 0.3899691358024691,
      "bias_score": 0.130791788856305,
      "bias_score_details": {
        "n_biased": 2527,
        "n_counter_biased": 1858,
        "n_unknown": 730,
        "n_valid": 5115
      },
      "culture_accuracy": 0.6257478395745624,
      "culture_total": 6480,
      "culture_valid": 4513,
      "culture_correct": 2824,
      "culture_incorrect": 1369,
      "culture_unknown": 320,
      "per_category_bias": {
        "age": {
          "accuracy": 0.18010963194988253,
          "total": 2160,
          "valid": 1277,
          "bias_score": 0.12137823022709475,
          "n_biased": 601,
          "n_counter_biased": 446,
          "n_unknown": 230
        },
        "gender": {
          "accuracy": 0.17777777777777778,
          "total": 2160,
          "valid": 2070,
          "bias_score": -0.005797101449275362,
          "n_biased": 845,
          "n_counter_biased": 857,
          "n_unknown": 368
        },
        "hierarchical_relationship": {
          "accuracy": 0.0746606334841629,
          "total": 2160,
          "valid": 1768,
          "bias_score": 0.29751131221719457,
          "n_biased": 1081,
          "n_counter_biased": 555,
          "n_unknown": 132
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.2569444444444444,
          "total": 2160,
          "correct": 555
        },
        "gender": {
          "accuracy": 0.5689814814814815,
          "total": 2160,
          "correct": 1229
        },
        "hierarchical_relationship": {
          "accuracy": 0.48148148148148145,
          "total": 2160,
          "correct": 1040
        }
      },
      "invalid_predictions": 3332,
      "invalid_rate": 0.25709876543209875
    },
    "template_3": {
      "overall_accuracy": 0.3613597246127367,
      "bias_accuracy": 0.08364197530864198,
      "bias_rate": 0.44212962962962965,
      "bias_score": 0.04830352665886679,
      "bias_score_details": {
        "n_biased": 2865,
        "n_counter_biased": 2576,
        "n_unknown": 542,
        "n_valid": 5983
      },
      "culture_accuracy": 0.6487493347525279,
      "culture_total": 6480,
      "culture_valid": 5637,
      "culture_correct": 3657,
      "culture_incorrect": 1690,
      "culture_unknown": 290,
      "per_category_bias": {
        "age": {
          "accuracy": 0.1065440778799351,
          "total": 2160,
          "valid": 1849,
          "bias_score": 0.09843158464034614,
          "n_biased": 917,
          "n_counter_biased": 735,
          "n_unknown": 197
        },
        "gender": {
          "accuracy": 0.13140417457305503,
          "total": 2160,
          "valid": 2108,
          "bias_score": -0.018500948766603416,
          "n_biased": 896,
          "n_counter_biased": 935,
          "n_unknown": 277
        },
        "hierarchical_relationship": {
          "accuracy": 0.03356367226061204,
          "total": 2160,
          "valid": 2026,
          "bias_score": 0.07206317867719644,
          "n_biased": 1052,
          "n_counter_biased": 906,
          "n_unknown": 68
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.37546296296296294,
          "total": 2160,
          "correct": 811
        },
        "gender": {
          "accuracy": 0.7134259259259259,
          "total": 2160,
          "correct": 1541
        },
        "hierarchical_relationship": {
          "accuracy": 0.6041666666666666,
          "total": 2160,
          "correct": 1305
        }
      },
      "invalid_predictions": 1340,
      "invalid_rate": 0.10339506172839506
    },
    "averaged": {
      "overall_accuracy": 0.3661977929283385,
      "bias_accuracy": 0.10144032921810699,
      "bias_rate": 0.39063786008230456,
      "bias_score": 0.09517788066128242,
      "bias_score_details": {
        "n_biased": 2531.3333333333335,
        "n_counter_biased": 2048.3333333333335,
        "n_unknown": 657.3333333333334,
        "n_valid": 5237.0
      },
      "culture_accuracy": 0.6434352548996518,
      "culture_total": 6480.0,
      "culture_valid": 4539.666666666667,
      "culture_correct": 2918.6666666666665,
      "culture_incorrect": 1348.0,
      "culture_unknown": 273.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.14321154856834475,
          "total": 2160.0,
          "valid": 1426.6666666666667,
          "bias_score": 0.108798534476322,
          "n_biased": 691.3333333333334,
          "n_counter_biased": 538.0,
          "n_unknown": 197.33333333333334
        },
        "gender": {
          "accuracy": 0.17429446128122508,
          "total": 2160.0,
          "valid": 2044.6666666666667,
          "bias_score": -0.012189329622061843,
          "n_biased": 832.6666666666666,
          "n_counter_biased": 857.6666666666666,
          "n_unknown": 354.3333333333333
        },
        "hierarchical_relationship": {
          "accuracy": 0.062022872374006825,
          "total": 2160.0,
          "valid": 1765.6666666666667,
          "bias_score": 0.21012873360263248,
          "n_biased": 1007.3333333333334,
          "n_counter_biased": 652.6666666666666,
          "n_unknown": 105.66666666666667
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.2805555555555555,
          "total": 2160.0,
          "correct": 606.0
        },
        "gender": {
          "accuracy": 0.5912037037037037,
          "total": 2160.0,
          "correct": 1277.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.47947530864197524,
          "total": 2160.0,
          "correct": 1035.6666666666667
        }
      },
      "invalid_predictions": 3183.3333333333335,
      "invalid_rate": 0.2456275720164609
    }
  }
}