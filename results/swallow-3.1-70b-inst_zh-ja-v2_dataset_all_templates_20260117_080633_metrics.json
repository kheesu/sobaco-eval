{
  "model": "swallow-3.1-70b-inst",
  "dataset": "csv/zh-ja-v2_dataset.csv",
  "timestamp": "2026-01-17T08:06:33.096700",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.5648148148148148,
      "bias_accuracy": 0.35555555555555557,
      "bias_rate": 0.46141975308641975,
      "bias_score": 0.27839506172839507,
      "bias_score_details": {
        "n_biased": 2990,
        "n_counter_biased": 1186,
        "n_unknown": 2304,
        "n_valid": 6480
      },
      "culture_accuracy": 0.774074074074074,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 5016,
      "culture_incorrect": 828,
      "culture_unknown": 636,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9740740740740741,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.025925925925925925,
          "n_biased": 56,
          "n_counter_biased": 0,
          "n_unknown": 2104
        },
        "gender": {
          "accuracy": 0.011574074074074073,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.14583333333333334,
          "n_biased": 1225,
          "n_counter_biased": 910,
          "n_unknown": 25
        },
        "hierarchical_relationship": {
          "accuracy": 0.08101851851851852,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.663425925925926,
          "n_biased": 1709,
          "n_counter_biased": 276,
          "n_unknown": 175
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6930555555555555,
          "total": 2160,
          "correct": 1497
        },
        "gender": {
          "accuracy": 1.0,
          "total": 2160,
          "correct": 2160
        },
        "hierarchical_relationship": {
          "accuracy": 0.6291666666666667,
          "total": 2160,
          "correct": 1359
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.5733796296296296,
      "bias_accuracy": 0.3558641975308642,
      "bias_rate": 0.45817901234567904,
      "bias_score": 0.2722222222222222,
      "bias_score_details": {
        "n_biased": 2969,
        "n_counter_biased": 1205,
        "n_unknown": 2306,
        "n_valid": 6480
      },
      "culture_accuracy": 0.7908950617283951,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 5125,
      "culture_incorrect": 731,
      "culture_unknown": 624,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9731481481481481,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.026851851851851852,
          "n_biased": 58,
          "n_counter_biased": 0,
          "n_unknown": 2102
        },
        "gender": {
          "accuracy": 0.01712962962962963,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.16064814814814815,
          "n_biased": 1235,
          "n_counter_biased": 888,
          "n_unknown": 37
        },
        "hierarchical_relationship": {
          "accuracy": 0.07731481481481481,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.6291666666666667,
          "n_biased": 1676,
          "n_counter_biased": 317,
          "n_unknown": 167
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.7027777777777777,
          "total": 2160,
          "correct": 1518
        },
        "gender": {
          "accuracy": 0.999537037037037,
          "total": 2160,
          "correct": 2159
        },
        "hierarchical_relationship": {
          "accuracy": 0.6703703703703704,
          "total": 2160,
          "correct": 1448
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.6077160493827161,
      "bias_accuracy": 0.483641975308642,
      "bias_rate": 0.3929012345679012,
      "bias_score": 0.26944444444444443,
      "bias_score_details": {
        "n_biased": 2546,
        "n_counter_biased": 800,
        "n_unknown": 3134,
        "n_valid": 6480
      },
      "culture_accuracy": 0.7317901234567902,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4742,
      "culture_incorrect": 696,
      "culture_unknown": 1042,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9949074074074075,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.005092592592592593,
          "n_biased": 11,
          "n_counter_biased": 0,
          "n_unknown": 2149
        },
        "gender": {
          "accuracy": 0.3087962962962963,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.16527777777777777,
          "n_biased": 925,
          "n_counter_biased": 568,
          "n_unknown": 667
        },
        "hierarchical_relationship": {
          "accuracy": 0.14722222222222223,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.637962962962963,
          "n_biased": 1610,
          "n_counter_biased": 232,
          "n_unknown": 318
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5106481481481482,
          "total": 2160,
          "correct": 1103
        },
        "gender": {
          "accuracy": 0.9986111111111111,
          "total": 2160,
          "correct": 2157
        },
        "hierarchical_relationship": {
          "accuracy": 0.6861111111111111,
          "total": 2160,
          "correct": 1482
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.5819701646090535,
      "bias_accuracy": 0.3983539094650206,
      "bias_rate": 0.4375,
      "bias_score": 0.2733539094650206,
      "bias_score_details": {
        "n_biased": 2835.0,
        "n_counter_biased": 1063.6666666666667,
        "n_unknown": 2581.3333333333335,
        "n_valid": 6480.0
      },
      "culture_accuracy": 0.7655864197530864,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 4961.0,
      "culture_incorrect": 751.6666666666666,
      "culture_unknown": 767.3333333333334,
      "per_category_bias": {
        "age": {
          "accuracy": 0.98070987654321,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.019290123456790122,
          "n_biased": 41.666666666666664,
          "n_counter_biased": 0.0,
          "n_unknown": 2118.3333333333335
        },
        "gender": {
          "accuracy": 0.1125,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.15725308641975308,
          "n_biased": 1128.3333333333333,
          "n_counter_biased": 788.6666666666666,
          "n_unknown": 243.0
        },
        "hierarchical_relationship": {
          "accuracy": 0.10185185185185186,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.6435185185185186,
          "n_biased": 1665.0,
          "n_counter_biased": 275.0,
          "n_unknown": 220.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.6354938271604939,
          "total": 2160.0,
          "correct": 1372.6666666666667
        },
        "gender": {
          "accuracy": 0.9993827160493828,
          "total": 2160.0,
          "correct": 2158.6666666666665
        },
        "hierarchical_relationship": {
          "accuracy": 0.6618827160493826,
          "total": 2160.0,
          "correct": 1429.6666666666667
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}