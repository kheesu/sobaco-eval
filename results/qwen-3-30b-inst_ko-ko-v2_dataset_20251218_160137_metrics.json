{
  "model": "qwen-3-30b-inst",
  "dataset": "csv/ko-ko-v2_dataset.csv",
  "timestamp": "2025-12-18T16:01:37.452370",
  "metrics": {
    "overall_accuracy": 0.3305812417437252,
    "bias_accuracy": 0.18994708994708995,
    "bias_rate": 0.20066137566137565,
    "bias_score": -0.006224988883948421,
    "bias_score_details": {
      "n_biased": 1517,
      "n_counter_biased": 1545,
      "n_unknown": 1436,
      "n_valid": 4498
    },
    "culture_accuracy": 0.34169210641081554,
    "culture_total": 7560,
    "culture_valid": 4586,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.32,
        "total": 2088,
        "valid": 1225,
        "bias_score": -0.022040816326530613,
        "n_biased": 403,
        "n_counter_biased": 430,
        "n_unknown": 392
      },
      "gender": {
        "accuracy": 0.2892209178228389,
        "total": 1584,
        "valid": 937,
        "bias_score": -0.032017075773746,
        "n_biased": 318,
        "n_counter_biased": 348,
        "n_unknown": 271
      },
      "age": {
        "accuracy": 0.3123446561723281,
        "total": 2016,
        "valid": 1207,
        "bias_score": 0.0033140016570008283,
        "n_biased": 417,
        "n_counter_biased": 413,
        "n_unknown": 377
      },
      "title": {
        "accuracy": 0.3582089552238806,
        "total": 1008,
        "valid": 603,
        "bias_score": -0.008291873963515755,
        "n_biased": 191,
        "n_counter_biased": 196,
        "n_unknown": 216
      },
      "elitism": {
        "accuracy": 0.34220532319391633,
        "total": 864,
        "valid": 526,
        "bias_score": 0.057034220532319393,
        "n_biased": 188,
        "n_counter_biased": 158,
        "n_unknown": 180
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.22365900383141762,
        "total": 2088,
        "correct": 467
      },
      "gender": {
        "accuracy": 0.21464646464646464,
        "total": 1584,
        "correct": 340
      },
      "age": {
        "accuracy": 0.18154761904761904,
        "total": 2016,
        "correct": 366
      },
      "title": {
        "accuracy": 0.20238095238095238,
        "total": 1008,
        "correct": 204
      },
      "elitism": {
        "accuracy": 0.2199074074074074,
        "total": 864,
        "correct": 190
      }
    },
    "invalid_predictions": 6036,
    "invalid_rate": 0.39920634920634923
  }
}