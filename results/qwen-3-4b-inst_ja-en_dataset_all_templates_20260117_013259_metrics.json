{
  "model": "qwen-3-4b-inst",
  "dataset": "csv/ja-en_dataset.csv",
  "timestamp": "2026-01-17T01:32:59.498281",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.35157295850066933,
      "bias_accuracy": 0.31977911646586343,
      "bias_rate": 0.4680388219544846,
      "bias_score": 0.2558567603748327,
      "bias_score_details": {
        "n_biased": 2797,
        "n_counter_biased": 1268,
        "n_unknown": 1911,
        "n_valid": 5976
      },
      "culture_accuracy": 0.3833668005354752,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2291,
      "culture_incorrect": 3236,
      "culture_unknown": 449,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5814176245210728,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.3879310344827586,
          "n_biased": 842,
          "n_counter_biased": 32,
          "n_unknown": 1214
        },
        "gender": {
          "accuracy": 0.18376068376068377,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.057692307692307696,
          "n_biased": 818,
          "n_counter_biased": 710,
          "n_unknown": 344
        },
        "age": {
          "accuracy": 0.17509920634920634,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3030753968253968,
          "n_biased": 1137,
          "n_counter_biased": 526,
          "n_unknown": 353
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.39511494252873564,
          "total": 2088,
          "correct": 825
        },
        "gender": {
          "accuracy": 0.3717948717948718,
          "total": 1872,
          "correct": 696
        },
        "age": {
          "accuracy": 0.3819444444444444,
          "total": 2016,
          "correct": 770
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.3465528781793842,
      "bias_accuracy": 0.3135876840696118,
      "bias_rate": 0.4702141900937082,
      "bias_score": 0.2540160642570281,
      "bias_score_details": {
        "n_biased": 2810,
        "n_counter_biased": 1292,
        "n_unknown": 1874,
        "n_valid": 5976
      },
      "culture_accuracy": 0.3795180722891566,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2268,
      "culture_incorrect": 3322,
      "culture_unknown": 386,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5924329501915708,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.3864942528735632,
          "n_biased": 829,
          "n_counter_biased": 22,
          "n_unknown": 1237
        },
        "gender": {
          "accuracy": 0.15758547008547008,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.046474358974358976,
          "n_biased": 832,
          "n_counter_biased": 745,
          "n_unknown": 295
        },
        "age": {
          "accuracy": 0.16964285714285715,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.30952380952380953,
          "n_biased": 1149,
          "n_counter_biased": 525,
          "n_unknown": 342
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3946360153256705,
          "total": 2088,
          "correct": 824
        },
        "gender": {
          "accuracy": 0.36378205128205127,
          "total": 1872,
          "correct": 681
        },
        "age": {
          "accuracy": 0.3784722222222222,
          "total": 2016,
          "correct": 763
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.34621820615796517,
      "bias_accuracy": 0.29685408299866134,
      "bias_rate": 0.4923025435073628,
      "bias_score": 0.2814591700133869,
      "bias_score_details": {
        "n_biased": 2942,
        "n_counter_biased": 1260,
        "n_unknown": 1774,
        "n_valid": 5976
      },
      "culture_accuracy": 0.39558232931726905,
      "culture_total": 5976,
      "culture_valid": 5976,
      "culture_correct": 2364,
      "culture_incorrect": 3260,
      "culture_unknown": 352,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5646551724137931,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.3960727969348659,
          "n_biased": 868,
          "n_counter_biased": 41,
          "n_unknown": 1179
        },
        "gender": {
          "accuracy": 0.15224358974358973,
          "total": 1872,
          "valid": 1872,
          "bias_score": 0.10416666666666667,
          "n_biased": 891,
          "n_counter_biased": 696,
          "n_unknown": 285
        },
        "age": {
          "accuracy": 0.15376984126984128,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.3273809523809524,
          "n_biased": 1183,
          "n_counter_biased": 523,
          "n_unknown": 310
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.3975095785440613,
          "total": 2088,
          "correct": 830
        },
        "gender": {
          "accuracy": 0.4155982905982906,
          "total": 1872,
          "correct": 778
        },
        "age": {
          "accuracy": 0.375,
          "total": 2016,
          "correct": 756
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.3481146809460062,
      "bias_accuracy": 0.3100736278447122,
      "bias_rate": 0.47685185185185186,
      "bias_score": 0.2637773315484159,
      "bias_score_details": {
        "n_biased": 2849.6666666666665,
        "n_counter_biased": 1273.3333333333333,
        "n_unknown": 1853.0,
        "n_valid": 5976.0
      },
      "culture_accuracy": 0.3861557340473003,
      "culture_total": 5976.0,
      "culture_valid": 5976.0,
      "culture_correct": 2307.6666666666665,
      "culture_incorrect": 3272.6666666666665,
      "culture_unknown": 395.6666666666667,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5795019157088123,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.3901660280970625,
          "n_biased": 846.3333333333334,
          "n_counter_biased": 31.666666666666668,
          "n_unknown": 1210.0
        },
        "gender": {
          "accuracy": 0.16452991452991453,
          "total": 1872.0,
          "valid": 1872.0,
          "bias_score": 0.06944444444444445,
          "n_biased": 847.0,
          "n_counter_biased": 717.0,
          "n_unknown": 308.0
        },
        "age": {
          "accuracy": 0.1661706349206349,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.31332671957671954,
          "n_biased": 1156.3333333333333,
          "n_counter_biased": 524.6666666666666,
          "n_unknown": 335.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.39575351213282256,
          "total": 2088.0,
          "correct": 826.3333333333334
        },
        "gender": {
          "accuracy": 0.38372507122507127,
          "total": 1872.0,
          "correct": 718.3333333333334
        },
        "age": {
          "accuracy": 0.37847222222222215,
          "total": 2016.0,
          "correct": 763.0
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}