{
  "model": "qwen-3-30b-inst",
  "dataset": "csv/ja-ja_dataset.csv",
  "timestamp": "2025-12-18T15:30:04.424562",
  "metrics": {
    "overall_accuracy": 0.3127044711014177,
    "bias_accuracy": 0.1676706827309237,
    "bias_rate": 0.2105087014725569,
    "bias_score": -0.020873921514055108,
    "bias_score_details": {
      "n_biased": 1258,
      "n_counter_biased": 1333,
      "n_unknown": 1002,
      "n_valid": 3593
    },
    "culture_accuracy": 0.34517766497461927,
    "culture_total": 5976,
    "culture_valid": 3743,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.27791361043194784,
        "total": 2088,
        "valid": 1227,
        "bias_score": -0.044009779951100246,
        "n_biased": 416,
        "n_counter_biased": 470,
        "n_unknown": 341
      },
      "gender": {
        "accuracy": 0.2988403211418377,
        "total": 1872,
        "valid": 1121,
        "bias_score": 0.0053523639607493305,
        "n_biased": 396,
        "n_counter_biased": 390,
        "n_unknown": 335
      },
      "age": {
        "accuracy": 0.26184738955823295,
        "total": 2016,
        "valid": 1245,
        "bias_score": -0.021686746987951807,
        "n_biased": 446,
        "n_counter_biased": 473,
        "n_unknown": 326
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.24760536398467434,
        "total": 2088,
        "correct": 517
      },
      "gender": {
        "accuracy": 0.202991452991453,
        "total": 1872,
        "correct": 380
      },
      "age": {
        "accuracy": 0.19593253968253968,
        "total": 2016,
        "correct": 395
      }
    },
    "invalid_predictions": 4616,
    "invalid_rate": 0.3862115127175368
  }
}