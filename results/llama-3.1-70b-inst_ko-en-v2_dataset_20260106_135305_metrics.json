{
  "model": "llama-3.1-70b-inst",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-06T13:53:05.749132",
  "metrics": {
    "overall_accuracy": 0.5144841269841269,
    "bias_accuracy": 0.6923280423280423,
    "bias_rate": 0.22486772486772486,
    "bias_score": 0.14206349206349206,
    "bias_score_details": {
      "n_biased": 1700,
      "n_counter_biased": 626,
      "n_unknown": 5234,
      "n_valid": 7560
    },
    "culture_accuracy": 0.33664021164021163,
    "culture_total": 7560,
    "culture_valid": 7560,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.8788314176245211,
        "total": 2088,
        "valid": 2088,
        "bias_score": 0.10488505747126436,
        "n_biased": 236,
        "n_counter_biased": 17,
        "n_unknown": 1835
      },
      "gender": {
        "accuracy": 0.4797979797979798,
        "total": 1584,
        "valid": 1584,
        "bias_score": 0.23106060606060605,
        "n_biased": 595,
        "n_counter_biased": 229,
        "n_unknown": 760
      },
      "age": {
        "accuracy": 0.6845238095238095,
        "total": 2016,
        "valid": 2016,
        "bias_score": 0.16865079365079366,
        "n_biased": 488,
        "n_counter_biased": 148,
        "n_unknown": 1380
      },
      "title": {
        "accuracy": 0.6488095238095238,
        "total": 1008,
        "valid": 1008,
        "bias_score": -0.007936507936507936,
        "n_biased": 173,
        "n_counter_biased": 181,
        "n_unknown": 654
      },
      "elitism": {
        "accuracy": 0.7002314814814815,
        "total": 864,
        "valid": 864,
        "bias_score": 0.18171296296296297,
        "n_biased": 208,
        "n_counter_biased": 51,
        "n_unknown": 605
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.25383141762452105,
        "total": 2088,
        "correct": 530
      },
      "gender": {
        "accuracy": 0.29987373737373735,
        "total": 1584,
        "correct": 475
      },
      "age": {
        "accuracy": 0.19246031746031747,
        "total": 2016,
        "correct": 388
      },
      "title": {
        "accuracy": 0.9067460317460317,
        "total": 1008,
        "correct": 914
      },
      "elitism": {
        "accuracy": 0.27546296296296297,
        "total": 864,
        "correct": 238
      }
    },
    "invalid_predictions": 0,
    "invalid_rate": 0.0
  }
}