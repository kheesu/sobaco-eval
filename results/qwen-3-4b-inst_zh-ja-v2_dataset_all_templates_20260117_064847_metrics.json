{
  "model": "qwen-3-4b-inst",
  "dataset": "csv/zh-ja-v2_dataset.csv",
  "timestamp": "2026-01-17T06:48:47.879979",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.5231481481481481,
      "bias_accuracy": 0.3939814814814815,
      "bias_rate": 0.4396604938271605,
      "bias_score": 0.2733024691358025,
      "bias_score_details": {
        "n_biased": 2849,
        "n_counter_biased": 1078,
        "n_unknown": 2553,
        "n_valid": 6480
      },
      "culture_accuracy": 0.6523148148148148,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4227,
      "culture_incorrect": 1269,
      "culture_unknown": 984,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9416666666666667,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.040740740740740744,
          "n_biased": 107,
          "n_counter_biased": 19,
          "n_unknown": 2034
        },
        "gender": {
          "accuracy": 0.032407407407407406,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.46296296296296297,
          "n_biased": 1545,
          "n_counter_biased": 545,
          "n_unknown": 70
        },
        "hierarchical_relationship": {
          "accuracy": 0.20787037037037037,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.3162037037037037,
          "n_biased": 1197,
          "n_counter_biased": 514,
          "n_unknown": 449
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5300925925925926,
          "total": 2160,
          "correct": 1145
        },
        "gender": {
          "accuracy": 0.975,
          "total": 2160,
          "correct": 2106
        },
        "hierarchical_relationship": {
          "accuracy": 0.45185185185185184,
          "total": 2160,
          "correct": 976
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.534104938271605,
      "bias_accuracy": 0.41867283950617284,
      "bias_rate": 0.42083333333333334,
      "bias_score": 0.2603395061728395,
      "bias_score_details": {
        "n_biased": 2727,
        "n_counter_biased": 1040,
        "n_unknown": 2713,
        "n_valid": 6480
      },
      "culture_accuracy": 0.649537037037037,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4209,
      "culture_incorrect": 1287,
      "culture_unknown": 984,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9476851851851852,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.0412037037037037,
          "n_biased": 101,
          "n_counter_biased": 12,
          "n_unknown": 2047
        },
        "gender": {
          "accuracy": 0.06851851851851852,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.42685185185185187,
          "n_biased": 1467,
          "n_counter_biased": 545,
          "n_unknown": 148
        },
        "hierarchical_relationship": {
          "accuracy": 0.23981481481481481,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.31296296296296294,
          "n_biased": 1159,
          "n_counter_biased": 483,
          "n_unknown": 518
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5305555555555556,
          "total": 2160,
          "correct": 1146
        },
        "gender": {
          "accuracy": 0.9717592592592592,
          "total": 2160,
          "correct": 2099
        },
        "hierarchical_relationship": {
          "accuracy": 0.4462962962962963,
          "total": 2160,
          "correct": 964
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.5236111111111111,
      "bias_accuracy": 0.3658950617283951,
      "bias_rate": 0.4867283950617284,
      "bias_score": 0.33935185185185185,
      "bias_score_details": {
        "n_biased": 3154,
        "n_counter_biased": 955,
        "n_unknown": 2371,
        "n_valid": 6480
      },
      "culture_accuracy": 0.6813271604938271,
      "culture_total": 6480,
      "culture_valid": 6480,
      "culture_correct": 4415,
      "culture_incorrect": 1210,
      "culture_unknown": 855,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9425925925925925,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.04259259259259259,
          "n_biased": 108,
          "n_counter_biased": 16,
          "n_unknown": 2036
        },
        "gender": {
          "accuracy": 0.03333333333333333,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.4842592592592593,
          "n_biased": 1567,
          "n_counter_biased": 521,
          "n_unknown": 72
        },
        "hierarchical_relationship": {
          "accuracy": 0.12175925925925926,
          "total": 2160,
          "valid": 2160,
          "bias_score": 0.4912037037037037,
          "n_biased": 1479,
          "n_counter_biased": 418,
          "n_unknown": 263
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5925925925925926,
          "total": 2160,
          "correct": 1280
        },
        "gender": {
          "accuracy": 0.9708333333333333,
          "total": 2160,
          "correct": 2097
        },
        "hierarchical_relationship": {
          "accuracy": 0.48055555555555557,
          "total": 2160,
          "correct": 1038
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.526954732510288,
      "bias_accuracy": 0.3928497942386831,
      "bias_rate": 0.4490740740740741,
      "bias_score": 0.29099794238683124,
      "bias_score_details": {
        "n_biased": 2910.0,
        "n_counter_biased": 1024.3333333333333,
        "n_unknown": 2545.6666666666665,
        "n_valid": 6480.0
      },
      "culture_accuracy": 0.661059670781893,
      "culture_total": 6480.0,
      "culture_valid": 6480.0,
      "culture_correct": 4283.666666666667,
      "culture_incorrect": 1255.3333333333333,
      "culture_unknown": 941.0,
      "per_category_bias": {
        "age": {
          "accuracy": 0.9439814814814814,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.041512345679012345,
          "n_biased": 105.33333333333333,
          "n_counter_biased": 15.666666666666666,
          "n_unknown": 2039.0
        },
        "gender": {
          "accuracy": 0.044753086419753084,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.45802469135802476,
          "n_biased": 1526.3333333333333,
          "n_counter_biased": 537.0,
          "n_unknown": 96.66666666666667
        },
        "hierarchical_relationship": {
          "accuracy": 0.1898148148148148,
          "total": 2160.0,
          "valid": 2160.0,
          "bias_score": 0.37345679012345673,
          "n_biased": 1278.3333333333333,
          "n_counter_biased": 471.6666666666667,
          "n_unknown": 410.0
        }
      },
      "per_category_culture": {
        "age": {
          "accuracy": 0.5510802469135802,
          "total": 2160.0,
          "correct": 1190.3333333333333
        },
        "gender": {
          "accuracy": 0.9725308641975308,
          "total": 2160.0,
          "correct": 2100.6666666666665
        },
        "hierarchical_relationship": {
          "accuracy": 0.4595679012345679,
          "total": 2160.0,
          "correct": 992.6666666666666
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}