{
  "model": "qwen-3-30b-inst",
  "dataset": "csv/ko_dataset.csv",
  "timestamp": "2025-12-18T16:57:25.047898",
  "metrics": {
    "overall_accuracy": 0.3211655796339154,
    "bias_accuracy": 0.1874362895005097,
    "bias_rate": 0.20833333333333334,
    "bias_score": 0.014768835616438356,
    "bias_score_details": {
      "n_biased": 1635,
      "n_counter_biased": 1566,
      "n_unknown": 1471,
      "n_valid": 4672
    },
    "culture_accuracy": 0.327265204799338,
    "culture_total": 7848,
    "culture_valid": 4834,
    "per_category_bias": {
      "hierarchical_relationship": {
        "accuracy": 0.3258964143426295,
        "total": 2088,
        "valid": 1255,
        "bias_score": -0.006374501992031873,
        "n_biased": 419,
        "n_counter_biased": 427,
        "n_unknown": 409
      },
      "gender": {
        "accuracy": 0.29464285714285715,
        "total": 1872,
        "valid": 1120,
        "bias_score": 0.008928571428571428,
        "n_biased": 400,
        "n_counter_biased": 390,
        "n_unknown": 330
      },
      "age": {
        "accuracy": 0.30548523206751055,
        "total": 2016,
        "valid": 1185,
        "bias_score": 0.010970464135021098,
        "n_biased": 418,
        "n_counter_biased": 405,
        "n_unknown": 362
      },
      "title": {
        "accuracy": 0.330550918196995,
        "total": 1008,
        "valid": 599,
        "bias_score": 0.041736227045075125,
        "n_biased": 213,
        "n_counter_biased": 188,
        "n_unknown": 198
      },
      "elitism": {
        "accuracy": 0.33528265107212474,
        "total": 864,
        "valid": 513,
        "bias_score": 0.056530214424951264,
        "n_biased": 185,
        "n_counter_biased": 156,
        "n_unknown": 172
      }
    },
    "per_category_culture": {
      "hierarchical_relationship": {
        "accuracy": 0.20019157088122605,
        "total": 2088,
        "correct": 418
      },
      "gender": {
        "accuracy": 0.22702991452991453,
        "total": 1872,
        "correct": 425
      },
      "age": {
        "accuracy": 0.1840277777777778,
        "total": 2016,
        "correct": 371
      },
      "title": {
        "accuracy": 0.19940476190476192,
        "total": 1008,
        "correct": 201
      },
      "elitism": {
        "accuracy": 0.19328703703703703,
        "total": 864,
        "correct": 167
      }
    },
    "invalid_predictions": 6190,
    "invalid_rate": 0.39436799184505605
  }
}