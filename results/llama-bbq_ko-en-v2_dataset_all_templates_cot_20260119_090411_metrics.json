{
  "model": "llama-bbq",
  "dataset": "csv/ko-en-v2_dataset.csv",
  "timestamp": "2026-01-19T09:04:11.389021",
  "evaluation_type": "all_templates_cot",
  "results": {
    "template_1": {
      "overall_accuracy": 0.3239800627653683,
      "bias_accuracy": 0.05515873015873016,
      "bias_rate": 0.19126984126984126,
      "bias_score": 0.3104950495049505,
      "bias_score_details": {
        "n_biased": 1446,
        "n_counter_biased": 662,
        "n_unknown": 417,
        "n_valid": 2525
      },
      "culture_accuracy": 0.46265560165975106,
      "culture_total": 7560,
      "culture_valid": 2892,
      "culture_correct": 1338,
      "culture_incorrect": 1278,
      "culture_unknown": 276,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.35376044568245124,
          "total": 2088,
          "valid": 718,
          "bias_score": 0.4011142061281337,
          "n_biased": 376,
          "n_counter_biased": 88,
          "n_unknown": 254
        },
        "gender": {
          "accuracy": 0.04136947218259629,
          "total": 1584,
          "valid": 701,
          "bias_score": 0.3024251069900143,
          "n_biased": 442,
          "n_counter_biased": 230,
          "n_unknown": 29
        },
        "age": {
          "accuracy": 0.28354978354978355,
          "total": 2016,
          "valid": 462,
          "bias_score": 0.07142857142857142,
          "n_biased": 182,
          "n_counter_biased": 149,
          "n_unknown": 131
        },
        "title": {
          "accuracy": 0.007894736842105263,
          "total": 1008,
          "valid": 380,
          "bias_score": 0.20789473684210527,
          "n_biased": 228,
          "n_counter_biased": 149,
          "n_unknown": 3
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "valid": 264,
          "bias_score": 0.6515151515151515,
          "n_biased": 218,
          "n_counter_biased": 46,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2523946360153257,
          "total": 2088,
          "correct": 527
        },
        "gender": {
          "accuracy": 0.17676767676767677,
          "total": 1584,
          "correct": 280
        },
        "age": {
          "accuracy": 0.1259920634920635,
          "total": 2016,
          "correct": 254
        },
        "title": {
          "accuracy": 0.09424603174603174,
          "total": 1008,
          "correct": 95
        },
        "elitism": {
          "accuracy": 0.21064814814814814,
          "total": 864,
          "correct": 182
        }
      },
      "invalid_predictions": 9703,
      "invalid_rate": 0.6417328042328042
    },
    "template_2": {
      "overall_accuracy": 0.32269285860446084,
      "bias_accuracy": 0.04523809523809524,
      "bias_rate": 0.1728835978835979,
      "bias_score": 0.32973459289248763,
      "bias_score_details": {
        "n_biased": 1307,
        "n_counter_biased": 574,
        "n_unknown": 342,
        "n_valid": 2223
      },
      "culture_accuracy": 0.4635885885885886,
      "culture_total": 7560,
      "culture_valid": 2664,
      "culture_correct": 1235,
      "culture_incorrect": 1217,
      "culture_unknown": 212,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.35379644588045234,
          "total": 2088,
          "valid": 619,
          "bias_score": 0.4168012924071082,
          "n_biased": 329,
          "n_counter_biased": 71,
          "n_unknown": 219
        },
        "gender": {
          "accuracy": 0.02773497688751926,
          "total": 1584,
          "valid": 649,
          "bias_score": 0.31587057010785824,
          "n_biased": 418,
          "n_counter_biased": 213,
          "n_unknown": 18
        },
        "age": {
          "accuracy": 0.2536945812807882,
          "total": 2016,
          "valid": 406,
          "bias_score": 0.03694581280788178,
          "n_biased": 159,
          "n_counter_biased": 144,
          "n_unknown": 103
        },
        "title": {
          "accuracy": 0.0058309037900874635,
          "total": 1008,
          "valid": 343,
          "bias_score": 0.2478134110787172,
          "n_biased": 213,
          "n_counter_biased": 128,
          "n_unknown": 2
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "valid": 206,
          "bias_score": 0.8252427184466019,
          "n_biased": 188,
          "n_counter_biased": 18,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2274904214559387,
          "total": 2088,
          "correct": 475
        },
        "gender": {
          "accuracy": 0.16666666666666666,
          "total": 1584,
          "correct": 264
        },
        "age": {
          "accuracy": 0.11755952380952381,
          "total": 2016,
          "correct": 237
        },
        "title": {
          "accuracy": 0.0873015873015873,
          "total": 1008,
          "correct": 88
        },
        "elitism": {
          "accuracy": 0.19791666666666666,
          "total": 864,
          "correct": 171
        }
      },
      "invalid_predictions": 10233,
      "invalid_rate": 0.6767857142857143
    },
    "template_3": {
      "overall_accuracy": 0.38992574754164155,
      "bias_accuracy": 0.07764550264550264,
      "bias_rate": 0.1541005291005291,
      "bias_score": 0.30397854269110414,
      "bias_score_details": {
        "n_biased": 1165,
        "n_counter_biased": 485,
        "n_unknown": 587,
        "n_valid": 2237
      },
      "culture_accuracy": 0.493809176984705,
      "culture_total": 7560,
      "culture_valid": 2746,
      "culture_correct": 1356,
      "culture_incorrect": 1161,
      "culture_unknown": 229,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.5091575091575091,
          "total": 2088,
          "valid": 546,
          "bias_score": 0.3992673992673993,
          "n_biased": 243,
          "n_counter_biased": 25,
          "n_unknown": 278
        },
        "gender": {
          "accuracy": 0.06525037936267071,
          "total": 1584,
          "valid": 659,
          "bias_score": 0.33687405159332323,
          "n_biased": 419,
          "n_counter_biased": 197,
          "n_unknown": 43
        },
        "age": {
          "accuracy": 0.4503968253968254,
          "total": 2016,
          "valid": 504,
          "bias_score": 0.057539682539682536,
          "n_biased": 153,
          "n_counter_biased": 124,
          "n_unknown": 227
        },
        "title": {
          "accuracy": 0.12149532710280374,
          "total": 1008,
          "valid": 321,
          "bias_score": 0.29906542056074764,
          "n_biased": 189,
          "n_counter_biased": 93,
          "n_unknown": 39
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864,
          "valid": 207,
          "bias_score": 0.5555555555555556,
          "n_biased": 161,
          "n_counter_biased": 46,
          "n_unknown": 0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.22940613026819923,
          "total": 2088,
          "correct": 479
        },
        "gender": {
          "accuracy": 0.20896464646464646,
          "total": 1584,
          "correct": 331
        },
        "age": {
          "accuracy": 0.12996031746031747,
          "total": 2016,
          "correct": 262
        },
        "title": {
          "accuracy": 0.10317460317460317,
          "total": 1008,
          "correct": 104
        },
        "elitism": {
          "accuracy": 0.20833333333333334,
          "total": 864,
          "correct": 180
        }
      },
      "invalid_predictions": 10137,
      "invalid_rate": 0.6704365079365079
    },
    "averaged": {
      "overall_accuracy": 0.34553288963715695,
      "bias_accuracy": 0.05934744268077602,
      "bias_rate": 0.17275132275132274,
      "bias_score": 0.31473606169618074,
      "bias_score_details": {
        "n_biased": 1306.0,
        "n_counter_biased": 573.6666666666666,
        "n_unknown": 448.6666666666667,
        "n_valid": 2328.3333333333335
      },
      "culture_accuracy": 0.4733511224110149,
      "culture_total": 7560.0,
      "culture_valid": 2767.3333333333335,
      "culture_correct": 1309.6666666666667,
      "culture_incorrect": 1218.6666666666667,
      "culture_unknown": 239.0,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.40557146690680423,
          "total": 2088.0,
          "valid": 627.6666666666666,
          "bias_score": 0.40572763260088046,
          "n_biased": 316.0,
          "n_counter_biased": 61.333333333333336,
          "n_unknown": 250.33333333333334
        },
        "gender": {
          "accuracy": 0.044784942810928756,
          "total": 1584.0,
          "valid": 669.6666666666666,
          "bias_score": 0.3183899095637319,
          "n_biased": 426.3333333333333,
          "n_counter_biased": 213.33333333333334,
          "n_unknown": 30.0
        },
        "age": {
          "accuracy": 0.32921373007579907,
          "total": 2016.0,
          "valid": 457.3333333333333,
          "bias_score": 0.05530468892537858,
          "n_biased": 164.66666666666666,
          "n_counter_biased": 139.0,
          "n_unknown": 153.66666666666666
        },
        "title": {
          "accuracy": 0.04507365591166549,
          "total": 1008.0,
          "valid": 348.0,
          "bias_score": 0.2515911894938567,
          "n_biased": 210.0,
          "n_counter_biased": 123.33333333333333,
          "n_unknown": 14.666666666666666
        },
        "elitism": {
          "accuracy": 0.0,
          "total": 864.0,
          "valid": 225.66666666666666,
          "bias_score": 0.6774378085057696,
          "n_biased": 189.0,
          "n_counter_biased": 36.666666666666664,
          "n_unknown": 0.0
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.23643039591315454,
          "total": 2088.0,
          "correct": 493.6666666666667
        },
        "gender": {
          "accuracy": 0.18413299663299662,
          "total": 1584.0,
          "correct": 291.6666666666667
        },
        "age": {
          "accuracy": 0.12450396825396826,
          "total": 2016.0,
          "correct": 251.0
        },
        "title": {
          "accuracy": 0.0949074074074074,
          "total": 1008.0,
          "correct": 95.66666666666667
        },
        "elitism": {
          "accuracy": 0.2056327160493827,
          "total": 864.0,
          "correct": 177.66666666666666
        }
      },
      "invalid_predictions": 10024.333333333334,
      "invalid_rate": 0.6629850088183421
    }
  }
}