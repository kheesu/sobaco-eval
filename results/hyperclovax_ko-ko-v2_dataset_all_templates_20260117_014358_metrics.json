{
  "model": "hyperclovax",
  "dataset": "csv/ko-ko-v2_dataset.csv",
  "timestamp": "2026-01-17T01:43:58.953196",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.3537698412698413,
      "bias_accuracy": 0.3986772486772487,
      "bias_rate": 0.31216931216931215,
      "bias_score": 0.023015873015873017,
      "bias_score_details": {
        "n_biased": 2360,
        "n_counter_biased": 2186,
        "n_unknown": 3014,
        "n_valid": 7560
      },
      "culture_accuracy": 0.30886243386243384,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2335,
      "culture_incorrect": 2473,
      "culture_unknown": 2752,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.4363026819923372,
          "total": 2088,
          "valid": 2088,
          "bias_score": -0.008141762452107279,
          "n_biased": 580,
          "n_counter_biased": 597,
          "n_unknown": 911
        },
        "gender": {
          "accuracy": 0.3547979797979798,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.027777777777777776,
          "n_biased": 533,
          "n_counter_biased": 489,
          "n_unknown": 562
        },
        "age": {
          "accuracy": 0.3814484126984127,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.053075396825396824,
          "n_biased": 677,
          "n_counter_biased": 570,
          "n_unknown": 769
        },
        "title": {
          "accuracy": 0.3888888888888889,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.013888888888888888,
          "n_biased": 301,
          "n_counter_biased": 315,
          "n_unknown": 392
        },
        "elitism": {
          "accuracy": 0.4398148148148148,
          "total": 864,
          "valid": 864,
          "bias_score": 0.0625,
          "n_biased": 269,
          "n_counter_biased": 215,
          "n_unknown": 380
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.31082375478927204,
          "total": 2088,
          "correct": 649
        },
        "gender": {
          "accuracy": 0.30303030303030304,
          "total": 1584,
          "correct": 480
        },
        "age": {
          "accuracy": 0.30654761904761907,
          "total": 2016,
          "correct": 618
        },
        "title": {
          "accuracy": 0.2876984126984127,
          "total": 1008,
          "correct": 290
        },
        "elitism": {
          "accuracy": 0.3449074074074074,
          "total": 864,
          "correct": 298
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.3408294199351809,
      "bias_accuracy": 0.3505291005291005,
      "bias_rate": 0.342989417989418,
      "bias_score": 0.03650793650793651,
      "bias_score_details": {
        "n_biased": 2593,
        "n_counter_biased": 2317,
        "n_unknown": 2650,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3311284561449927,
      "culture_total": 7560,
      "culture_valid": 7559,
      "culture_correct": 2503,
      "culture_incorrect": 2629,
      "culture_unknown": 2427,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.40325670498084293,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.012452107279693486,
          "n_biased": 636,
          "n_counter_biased": 610,
          "n_unknown": 842
        },
        "gender": {
          "accuracy": 0.3181818181818182,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.017676767676767676,
          "n_biased": 554,
          "n_counter_biased": 526,
          "n_unknown": 504
        },
        "age": {
          "accuracy": 0.3308531746031746,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.05109126984126984,
          "n_biased": 726,
          "n_counter_biased": 623,
          "n_unknown": 667
        },
        "title": {
          "accuracy": 0.34424603174603174,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.050595238095238096,
          "n_biased": 356,
          "n_counter_biased": 305,
          "n_unknown": 347
        },
        "elitism": {
          "accuracy": 0.33564814814814814,
          "total": 864,
          "valid": 864,
          "bias_score": 0.0787037037037037,
          "n_biased": 321,
          "n_counter_biased": 253,
          "n_unknown": 290
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.34339080459770116,
          "total": 2088,
          "correct": 717
        },
        "gender": {
          "accuracy": 0.3162878787878788,
          "total": 1584,
          "correct": 501
        },
        "age": {
          "accuracy": 0.31646825396825395,
          "total": 2016,
          "correct": 638
        },
        "title": {
          "accuracy": 0.2896825396825397,
          "total": 1008,
          "correct": 292
        },
        "elitism": {
          "accuracy": 0.41087962962962965,
          "total": 864,
          "correct": 355
        }
      },
      "invalid_predictions": 1,
      "invalid_rate": 6.613756613756614e-05
    },
    "template_3": {
      "overall_accuracy": 0.34444444444444444,
      "bias_accuracy": 0.3775132275132275,
      "bias_rate": 0.32764550264550263,
      "bias_score": 0.0328042328042328,
      "bias_score_details": {
        "n_biased": 2477,
        "n_counter_biased": 2229,
        "n_unknown": 2854,
        "n_valid": 7560
      },
      "culture_accuracy": 0.31137566137566136,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2354,
      "culture_incorrect": 2557,
      "culture_unknown": 2649,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.4468390804597701,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.021551724137931036,
          "n_biased": 600,
          "n_counter_biased": 555,
          "n_unknown": 933
        },
        "gender": {
          "accuracy": 0.2796717171717172,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.027146464646464648,
          "n_biased": 592,
          "n_counter_biased": 549,
          "n_unknown": 443
        },
        "age": {
          "accuracy": 0.33630952380952384,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.025793650793650792,
          "n_biased": 695,
          "n_counter_biased": 643,
          "n_unknown": 678
        },
        "title": {
          "accuracy": 0.3740079365079365,
          "total": 1008,
          "valid": 1008,
          "bias_score": 0.034722222222222224,
          "n_biased": 333,
          "n_counter_biased": 298,
          "n_unknown": 377
        },
        "elitism": {
          "accuracy": 0.4895833333333333,
          "total": 864,
          "valid": 864,
          "bias_score": 0.08449074074074074,
          "n_biased": 257,
          "n_counter_biased": 184,
          "n_unknown": 423
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.30268199233716475,
          "total": 2088,
          "correct": 632
        },
        "gender": {
          "accuracy": 0.3402777777777778,
          "total": 1584,
          "correct": 539
        },
        "age": {
          "accuracy": 0.30505952380952384,
          "total": 2016,
          "correct": 615
        },
        "title": {
          "accuracy": 0.25892857142857145,
          "total": 1008,
          "correct": 261
        },
        "elitism": {
          "accuracy": 0.35532407407407407,
          "total": 864,
          "correct": 307
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.3463479018831556,
      "bias_accuracy": 0.3755731922398589,
      "bias_rate": 0.32760141093474426,
      "bias_score": 0.03077601410934744,
      "bias_score_details": {
        "n_biased": 2476.6666666666665,
        "n_counter_biased": 2244.0,
        "n_unknown": 2839.3333333333335,
        "n_valid": 7560.0
      },
      "culture_accuracy": 0.3171221837943626,
      "culture_total": 7560.0,
      "culture_valid": 7559.666666666667,
      "culture_correct": 2397.3333333333335,
      "culture_incorrect": 2553.0,
      "culture_unknown": 2609.3333333333335,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.42879948914431676,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.008620689655172415,
          "n_biased": 605.3333333333334,
          "n_counter_biased": 587.3333333333334,
          "n_unknown": 895.3333333333334
        },
        "gender": {
          "accuracy": 0.317550505050505,
          "total": 1584.0,
          "valid": 1584.0,
          "bias_score": 0.024200336700336705,
          "n_biased": 559.6666666666666,
          "n_counter_biased": 521.3333333333334,
          "n_unknown": 503.0
        },
        "age": {
          "accuracy": 0.34953703703703703,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.043320105820105814,
          "n_biased": 699.3333333333334,
          "n_counter_biased": 612.0,
          "n_unknown": 704.6666666666666
        },
        "title": {
          "accuracy": 0.36904761904761907,
          "total": 1008.0,
          "valid": 1008.0,
          "bias_score": 0.023809523809523808,
          "n_biased": 330.0,
          "n_counter_biased": 306.0,
          "n_unknown": 372.0
        },
        "elitism": {
          "accuracy": 0.4216820987654321,
          "total": 864.0,
          "valid": 864.0,
          "bias_score": 0.0752314814814815,
          "n_biased": 282.3333333333333,
          "n_counter_biased": 217.33333333333334,
          "n_unknown": 364.3333333333333
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.31896551724137934,
          "total": 2088.0,
          "correct": 666.0
        },
        "gender": {
          "accuracy": 0.3198653198653199,
          "total": 1584.0,
          "correct": 506.6666666666667
        },
        "age": {
          "accuracy": 0.30935846560846564,
          "total": 2016.0,
          "correct": 623.6666666666666
        },
        "title": {
          "accuracy": 0.2787698412698412,
          "total": 1008.0,
          "correct": 281.0
        },
        "elitism": {
          "accuracy": 0.3703703703703704,
          "total": 864.0,
          "correct": 320.0
        }
      },
      "invalid_predictions": 0.3333333333333333,
      "invalid_rate": 2.2045855379188714e-05
    }
  }
}