{
  "model": "swallow-3.1-70b-inst",
  "dataset": "csv/ko-ko-v2_dataset.csv",
  "timestamp": "2026-01-17T03:22:34.166702",
  "evaluation_type": "all_templates",
  "results": {
    "template_1": {
      "overall_accuracy": 0.44371693121693123,
      "bias_accuracy": 0.5128306878306879,
      "bias_rate": 0.3205026455026455,
      "bias_score": 0.15383597883597883,
      "bias_score_details": {
        "n_biased": 2423,
        "n_counter_biased": 1260,
        "n_unknown": 3877,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3746031746031746,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2832,
      "culture_incorrect": 2482,
      "culture_unknown": 2246,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7782567049808429,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.2064176245210728,
          "n_biased": 447,
          "n_counter_biased": 16,
          "n_unknown": 1625
        },
        "gender": {
          "accuracy": 0.2619949494949495,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.16224747474747475,
          "n_biased": 713,
          "n_counter_biased": 456,
          "n_unknown": 415
        },
        "age": {
          "accuracy": 0.345734126984127,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.21081349206349206,
          "n_biased": 872,
          "n_counter_biased": 447,
          "n_unknown": 697
        },
        "title": {
          "accuracy": 0.42162698412698413,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.09027777777777778,
          "n_biased": 246,
          "n_counter_biased": 337,
          "n_unknown": 425
        },
        "elitism": {
          "accuracy": 0.8275462962962963,
          "total": 864,
          "valid": 864,
          "bias_score": 0.16319444444444445,
          "n_biased": 145,
          "n_counter_biased": 4,
          "n_unknown": 715
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2868773946360153,
          "total": 2088,
          "correct": 599
        },
        "gender": {
          "accuracy": 0.4046717171717172,
          "total": 1584,
          "correct": 641
        },
        "age": {
          "accuracy": 0.30952380952380953,
          "total": 2016,
          "correct": 624
        },
        "title": {
          "accuracy": 0.7797619047619048,
          "total": 1008,
          "correct": 786
        },
        "elitism": {
          "accuracy": 0.21064814814814814,
          "total": 864,
          "correct": 182
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_2": {
      "overall_accuracy": 0.4489417989417989,
      "bias_accuracy": 0.537962962962963,
      "bias_rate": 0.30343915343915345,
      "bias_score": 0.14484126984126985,
      "bias_score_details": {
        "n_biased": 2294,
        "n_counter_biased": 1199,
        "n_unknown": 4067,
        "n_valid": 7560
      },
      "culture_accuracy": 0.3599206349206349,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2721,
      "culture_incorrect": 2401,
      "culture_unknown": 2438,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7686781609195402,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.21408045977011494,
          "n_biased": 465,
          "n_counter_biased": 18,
          "n_unknown": 1605
        },
        "gender": {
          "accuracy": 0.29734848484848486,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.15845959595959597,
          "n_biased": 682,
          "n_counter_biased": 431,
          "n_unknown": 471
        },
        "age": {
          "accuracy": 0.4017857142857143,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.18948412698412698,
          "n_biased": 794,
          "n_counter_biased": 412,
          "n_unknown": 810
        },
        "title": {
          "accuracy": 0.44146825396825395,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.10416666666666667,
          "n_biased": 229,
          "n_counter_biased": 334,
          "n_unknown": 445
        },
        "elitism": {
          "accuracy": 0.8518518518518519,
          "total": 864,
          "valid": 864,
          "bias_score": 0.1388888888888889,
          "n_biased": 124,
          "n_counter_biased": 4,
          "n_unknown": 736
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2777777777777778,
          "total": 2088,
          "correct": 580
        },
        "gender": {
          "accuracy": 0.3939393939393939,
          "total": 1584,
          "correct": 624
        },
        "age": {
          "accuracy": 0.30456349206349204,
          "total": 2016,
          "correct": 614
        },
        "title": {
          "accuracy": 0.7638888888888888,
          "total": 1008,
          "correct": 770
        },
        "elitism": {
          "accuracy": 0.15393518518518517,
          "total": 864,
          "correct": 133
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "template_3": {
      "overall_accuracy": 0.45846560846560847,
      "bias_accuracy": 0.5567460317460318,
      "bias_rate": 0.29193121693121693,
      "bias_score": 0.1406084656084656,
      "bias_score_details": {
        "n_biased": 2207,
        "n_counter_biased": 1144,
        "n_unknown": 4209,
        "n_valid": 7560
      },
      "culture_accuracy": 0.36018518518518516,
      "culture_total": 7560,
      "culture_valid": 7560,
      "culture_correct": 2723,
      "culture_incorrect": 2311,
      "culture_unknown": 2526,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7835249042145593,
          "total": 2088,
          "valid": 2088,
          "bias_score": 0.2021072796934866,
          "n_biased": 437,
          "n_counter_biased": 15,
          "n_unknown": 1636
        },
        "gender": {
          "accuracy": 0.3402777777777778,
          "total": 1584,
          "valid": 1584,
          "bias_score": 0.20265151515151514,
          "n_biased": 683,
          "n_counter_biased": 362,
          "n_unknown": 539
        },
        "age": {
          "accuracy": 0.4027777777777778,
          "total": 2016,
          "valid": 2016,
          "bias_score": 0.1498015873015873,
          "n_biased": 753,
          "n_counter_biased": 451,
          "n_unknown": 812
        },
        "title": {
          "accuracy": 0.4126984126984127,
          "total": 1008,
          "valid": 1008,
          "bias_score": -0.037698412698412696,
          "n_biased": 277,
          "n_counter_biased": 315,
          "n_unknown": 416
        },
        "elitism": {
          "accuracy": 0.9328703703703703,
          "total": 864,
          "valid": 864,
          "bias_score": 0.06481481481481481,
          "n_biased": 57,
          "n_counter_biased": 1,
          "n_unknown": 806
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.2878352490421456,
          "total": 2088,
          "correct": 601
        },
        "gender": {
          "accuracy": 0.39330808080808083,
          "total": 1584,
          "correct": 623
        },
        "age": {
          "accuracy": 0.31101190476190477,
          "total": 2016,
          "correct": 627
        },
        "title": {
          "accuracy": 0.7857142857142857,
          "total": 1008,
          "correct": 792
        },
        "elitism": {
          "accuracy": 0.09259259259259259,
          "total": 864,
          "correct": 80
        }
      },
      "invalid_predictions": 0,
      "invalid_rate": 0.0
    },
    "averaged": {
      "overall_accuracy": 0.45037477954144617,
      "bias_accuracy": 0.5358465608465609,
      "bias_rate": 0.30529100529100534,
      "bias_score": 0.14642857142857144,
      "bias_score_details": {
        "n_biased": 2308.0,
        "n_counter_biased": 1201.0,
        "n_unknown": 4051.0,
        "n_valid": 7560.0
      },
      "culture_accuracy": 0.3649029982363316,
      "culture_total": 7560.0,
      "culture_valid": 7560.0,
      "culture_correct": 2758.6666666666665,
      "culture_incorrect": 2398.0,
      "culture_unknown": 2403.3333333333335,
      "per_category_bias": {
        "hierarchical_relationship": {
          "accuracy": 0.7768199233716476,
          "total": 2088.0,
          "valid": 2088.0,
          "bias_score": 0.2075351213282248,
          "n_biased": 449.6666666666667,
          "n_counter_biased": 16.333333333333332,
          "n_unknown": 1622.0
        },
        "gender": {
          "accuracy": 0.2998737373737374,
          "total": 1584.0,
          "valid": 1584.0,
          "bias_score": 0.17445286195286194,
          "n_biased": 692.6666666666666,
          "n_counter_biased": 416.3333333333333,
          "n_unknown": 475.0
        },
        "age": {
          "accuracy": 0.3834325396825397,
          "total": 2016.0,
          "valid": 2016.0,
          "bias_score": 0.18336640211640212,
          "n_biased": 806.3333333333334,
          "n_counter_biased": 436.6666666666667,
          "n_unknown": 773.0
        },
        "title": {
          "accuracy": 0.42526455026455023,
          "total": 1008.0,
          "valid": 1008.0,
          "bias_score": -0.07738095238095238,
          "n_biased": 250.66666666666666,
          "n_counter_biased": 328.6666666666667,
          "n_unknown": 428.6666666666667
        },
        "elitism": {
          "accuracy": 0.8707561728395062,
          "total": 864.0,
          "valid": 864.0,
          "bias_score": 0.1222993827160494,
          "n_biased": 108.66666666666667,
          "n_counter_biased": 3.0,
          "n_unknown": 752.3333333333334
        }
      },
      "per_category_culture": {
        "hierarchical_relationship": {
          "accuracy": 0.28416347381864626,
          "total": 2088.0,
          "correct": 593.3333333333334
        },
        "gender": {
          "accuracy": 0.39730639730639733,
          "total": 1584.0,
          "correct": 629.3333333333334
        },
        "age": {
          "accuracy": 0.30836640211640215,
          "total": 2016.0,
          "correct": 621.6666666666666
        },
        "title": {
          "accuracy": 0.7764550264550264,
          "total": 1008.0,
          "correct": 782.6666666666666
        },
        "elitism": {
          "accuracy": 0.15239197530864199,
          "total": 864.0,
          "correct": 131.66666666666666
        }
      },
      "invalid_predictions": 0.0,
      "invalid_rate": 0.0
    }
  }
}