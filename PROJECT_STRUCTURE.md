# SOBACO-EVAL Project Structure

```
sobaco-eval/
â”œâ”€â”€ README.md                      # Main project documentation
â”œâ”€â”€ USAGE_GUIDE.md                 # Detailed usage instructions
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ config.yaml                    # Model and evaluation configuration
â”œâ”€â”€ .env.example                   # Environment variables template
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”‚
â”œâ”€â”€ csv/                           # Evaluation datasets
â”‚   â”œâ”€â”€ ja_dataset.csv            # Japanese (11,954 samples)
â”‚   â”œâ”€â”€ ja-ko_dataset.csv         # Korean (11,954 samples)
â”‚   â””â”€â”€ ja-zh_dataset.csv         # Chinese (11,954 samples)
â”‚
â”œâ”€â”€ evaluate.py                    # Main evaluation script
â”œâ”€â”€ analyze_results.py             # Results analysis and visualization
â”œâ”€â”€ utils.py                       # Utility functions
â”œâ”€â”€ quick_test.py                  # Quick test without GPU/API
â”œâ”€â”€ quick_start.sh                 # Setup automation script
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â””â”€â”€ evaluation_demo.ipynb     # Interactive demo and tutorial
â”‚
â””â”€â”€ results/                       # Evaluation results (generated)
    â”œâ”€â”€ *.csv                      # Prediction results
    â”œâ”€â”€ *_metrics.json            # Evaluation metrics
    â””â”€â”€ analysis/                  # Analysis outputs
        â”œâ”€â”€ summary.csv
        â”œâ”€â”€ overall_performance.png
        â”œâ”€â”€ bias_analysis.png
        â””â”€â”€ *.png
```

## File Descriptions

### Core Scripts

- **`evaluate.py`**: Main evaluation script that:
  - Loads LLMs (local or API-based)
  - Runs inference on datasets
  - Calculates metrics
  - Saves results

- **`analyze_results.py`**: Analysis script that:
  - Loads evaluation results
  - Generates comparison plots
  - Creates summary tables
  - Produces visualizations

- **`utils.py`**: Utility module with:
  - Dataset loading functions
  - Prompt formatting
  - Answer extraction
  - Metrics calculation

- **`quick_test.py`**: Testing script that:
  - Runs mock evaluation
  - Verifies setup
  - Demonstrates workflow
  - No GPU/API required

### Configuration Files

- **`config.yaml`**: Central configuration for:
  - Model definitions (local & API)
  - Evaluation parameters
  - Prompt templates
  - Output settings

- **`.env.example`**: Template for:
  - API keys (OpenAI, Anthropic, Google)
  - Hugging Face tokens
  - Environment variables

### Documentation

- **`README.md`**: Project overview with:
  - Quick start guide
  - Feature highlights
  - Installation steps
  - Basic usage examples

- **`USAGE_GUIDE.md`**: Comprehensive guide with:
  - Detailed instructions
  - Configuration options
  - Troubleshooting tips
  - Advanced usage

### Datasets

The `csv/` directory contains three parallel datasets:

Each dataset includes:
- **context**: Main scenario
- **additional_context**: Extra information
- **type**: `bias` or `culture`
- **question**: Evaluation question
- **options**: Multiple choice answers
- **answer**: Correct answer
- **biased_option**: Stereotypical option (for bias questions)
- **category**: Question category

### Results

The `results/` directory (created during evaluation) contains:

- **CSV files**: Full predictions with columns:
  - Original dataset fields
  - `prediction`: Model's answer
  - `raw_response`: Raw model output

- **JSON files**: Metrics including:
  - Overall accuracy
  - Bias accuracy & rate
  - Culture accuracy
  - Per-category performance

- **PNG files**: Visualizations:
  - Performance comparisons
  - Bias analysis charts
  - Category breakdowns
  - Confusion matrices

## Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Dataset   â”‚
â”‚  (csv/*.csv)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Model     â”‚
â”‚  (config.yaml)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run Inference  â”‚
â”‚  (evaluate.py)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save Results    â”‚
â”‚ (results/*.csv) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analyze Results â”‚
â”‚ (analyze_resultsâ”‚
â”‚      .py)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Visualizations â”‚
â”‚  & Reports      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Commands

### Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Set up API keys
cp .env.example .env
# Edit .env with your keys
```

### Evaluation
```bash
# Test setup (no GPU/API needed)
python quick_test.py

# Quick test on 10% of data (recommended first step)
python evaluate.py --model llama-3.1-8b --dataset csv/ja_dataset.csv --subset 0.1

# Evaluate single model (full dataset)
python evaluate.py --model llama-3.1-8b --dataset csv/ja_dataset.csv

# Evaluate all datasets
python evaluate.py --model llama-3.1-8b --all-datasets

# Compare multiple models
python evaluate.py --model llama-3.1-8b gpt-4 --all-datasets
```

### Analysis
```bash
# Analyze single result
python analyze_results.py --results results/llama-3.1-8b_ja_dataset.csv

# Compare all results
python analyze_results.py --results results/*.csv
```

### Notebooks
```bash
# Open Jupyter notebook
jupyter notebook notebooks/evaluation_demo.ipynb
```

## Key Features

âœ… **Multiple Model Support**: Local (Llama, etc.) and API (GPT, Claude, Gemini)
âœ… **Multilingual**: Japanese, Korean, Chinese datasets
âœ… **Bias Detection**: Identifies stereotypical responses
âœ… **Cultural Awareness**: Tests cultural context understanding
âœ… **Comprehensive Metrics**: Accuracy, bias rates, per-category analysis
âœ… **Rich Visualizations**: Charts, plots, confusion matrices
âœ… **Easy Configuration**: YAML-based setup
âœ… **Extensible**: Easy to add new models and metrics

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests with `quick_test.py`
5. Submit a pull request

## Support

- ğŸ“– Documentation: README.md, USAGE_GUIDE.md
- ğŸ› Issues: GitHub Issues
- ğŸ’¬ Discussions: GitHub Discussions
- ğŸ“§ Contact: [Your contact info]
