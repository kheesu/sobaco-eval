{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de1c900",
   "metadata": {},
   "source": [
    "# SOBACO-EVAL: LLM Evaluation Demo\n",
    "\n",
    "This notebook demonstrates how to use the SOBACO-EVAL framework to evaluate LLMs on social bias and cultural awareness.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Data Exploration**: Examine the evaluation datasets\n",
    "2. **Sample Evaluation**: Run a small-scale evaluation\n",
    "3. **Results Analysis**: Analyze and visualize results\n",
    "4. **Bias Detection**: Identify patterns in biased responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e018f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from utils import load_dataset, parse_options, calculate_metrics, print_metrics\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7b948",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Let's explore the datasets and understand their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Japanese dataset\n",
    "df_ja = load_dataset('../csv/ja_dataset.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df_ja.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df_ja.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f630ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample rows\n",
    "print(\"\\nüìã Sample Questions:\")\n",
    "df_ja.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze question types\n",
    "print(\"\\nüìä Question Type Distribution:\")\n",
    "type_counts = df_ja['type'].value_counts()\n",
    "print(type_counts)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Type distribution\n",
    "type_counts.plot(kind='bar', ax=ax[0], color=['coral', 'mediumseagreen'], alpha=0.8)\n",
    "ax[0].set_title('Question Type Distribution', fontsize=14, fontweight='bold')\n",
    "ax[0].set_xlabel('Type')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Category distribution\n",
    "category_counts = df_ja['category'].value_counts().head(10)\n",
    "category_counts.plot(kind='barh', ax=ax[1], color='steelblue', alpha=0.8)\n",
    "ax[1].set_title('Top 10 Categories', fontsize=14, fontweight='bold')\n",
    "ax[1].set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b114ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a bias question\n",
    "bias_sample = df_ja[df_ja['type'] == 'bias'].iloc[0]\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  BIAS QUESTION EXAMPLE:\")\n",
    "print(f\"Context: {bias_sample['context']}\")\n",
    "print(f\"Additional Context: {bias_sample['additional_context']}\")\n",
    "print(f\"Question: {bias_sample['question']}\")\n",
    "print(f\"Options: {bias_sample['options']}\")\n",
    "print(f\"Correct Answer: {bias_sample['answer']}\")\n",
    "print(f\"Biased Option: {bias_sample['biased_option']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a culture question\n",
    "culture_sample = df_ja[df_ja['type'] == 'culture'].iloc[0]\n",
    "\n",
    "print(\"\\nüåè CULTURE QUESTION EXAMPLE:\")\n",
    "print(f\"Context: {culture_sample['context']}\")\n",
    "print(f\"Additional Context: {culture_sample['additional_context']}\")\n",
    "print(f\"Question: {culture_sample['question']}\")\n",
    "print(f\"Options: {culture_sample['options']}\")\n",
    "print(f\"Correct Answer: {culture_sample['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbf7db",
   "metadata": {},
   "source": [
    "## 2. Sample Evaluation\n",
    "\n",
    "Let's run a small evaluation on a subset of data to demonstrate the workflow.\n",
    "\n",
    "**Note**: This example uses a mock evaluation. For real evaluation, use the `evaluate.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset (100 rows)\n",
    "sample_df = df_ja.sample(n=100, random_state=42).copy()\n",
    "\n",
    "print(f\"Sample dataset size: {len(sample_df)}\")\n",
    "print(f\"Bias questions: {(sample_df['type'] == 'bias').sum()}\")\n",
    "print(f\"Culture questions: {(sample_df['type'] == 'culture').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate predictions (in real scenario, these come from LLM)\n",
    "# For demo purposes, we'll create mock predictions with some patterns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def simulate_prediction(row):\n",
    "    \"\"\"Simulate model predictions with realistic patterns\"\"\"\n",
    "    options = parse_options(row['options'])\n",
    "    \n",
    "    # Simulate different behavior for bias vs culture questions\n",
    "    if row['type'] == 'bias':\n",
    "        # Model sometimes picks biased option (30% of the time)\n",
    "        if np.random.random() < 0.3 and pd.notna(row['biased_option']):\n",
    "            return row['biased_option']\n",
    "        # Otherwise 60% correct, 10% random\n",
    "        elif np.random.random() < 0.75:\n",
    "            return row['answer']\n",
    "        else:\n",
    "            return np.random.choice(options)\n",
    "    else:  # culture questions\n",
    "        # Higher accuracy on culture questions (70%)\n",
    "        if np.random.random() < 0.7:\n",
    "            return row['answer']\n",
    "        else:\n",
    "            return np.random.choice(options)\n",
    "\n",
    "sample_df['prediction'] = sample_df.apply(simulate_prediction, axis=1)\n",
    "\n",
    "print(\"‚úÖ Simulated predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd55ae3",
   "metadata": {},
   "source": [
    "## 3. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daee161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics = calculate_metrics(sample_df)\n",
    "\n",
    "# Print detailed metrics\n",
    "print_metrics(metrics, \"Demo Model (Simulated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy by question type\n",
    "type_accuracy = sample_df.groupby('type').apply(\n",
    "    lambda x: (x['prediction'] == x['answer']).sum() / len(x) * 100\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(type_accuracy.index, type_accuracy.values, \n",
    "              color=['coral', 'mediumseagreen'], alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Model Performance by Question Type', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50% baseline')\n",
    "\n",
    "for bar, v in zip(bars, type_accuracy.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, v + 3, f'{v:.1f}%', \n",
    "            ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f1000",
   "metadata": {},
   "source": [
    "## 4. Bias Detection Analysis\n",
    "\n",
    "Let's examine when the model selects biased options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acead865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bias selections\n",
    "bias_df = sample_df[sample_df['type'] == 'bias'].copy()\n",
    "bias_df['is_biased'] = bias_df['prediction'] == bias_df['biased_option']\n",
    "bias_df['is_correct'] = bias_df['prediction'] == bias_df['answer']\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Bias Analysis:\")\n",
    "print(f\"Total bias questions: {len(bias_df)}\")\n",
    "print(f\"Correct answers: {bias_df['is_correct'].sum()} ({bias_df['is_correct'].mean():.1%})\")\n",
    "print(f\"Biased selections: {bias_df['is_biased'].sum()} ({bias_df['is_biased'].mean():.1%})\")\n",
    "print(f\"Other wrong answers: {(~bias_df['is_correct'] & ~bias_df['is_biased']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a48bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples where model selected biased option\n",
    "biased_examples = bias_df[bias_df['is_biased']].head(3)\n",
    "\n",
    "print(\"\\n‚ùå Examples where model chose BIASED option:\\n\")\n",
    "for idx, row in biased_examples.iterrows():\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(f\"  Context: {row['context']}\")\n",
    "    print(f\"  Additional: {row['additional_context']}\")\n",
    "    print(f\"  Question: {row['question']}\")\n",
    "    print(f\"  Correct Answer: {row['answer']}\")\n",
    "    print(f\"  Model Prediction: {row['prediction']} ‚ö†Ô∏è\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion visualization for bias questions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart of bias question outcomes\n",
    "outcomes = pd.Series({\n",
    "    'Correct': bias_df['is_correct'].sum(),\n",
    "    'Biased': bias_df['is_biased'].sum(),\n",
    "    'Other Wrong': (~bias_df['is_correct'] & ~bias_df['is_biased']).sum()\n",
    "})\n",
    "\n",
    "colors = ['mediumseagreen', 'crimson', 'orange']\n",
    "axes[0].pie(outcomes.values, labels=outcomes.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[0].set_title('Bias Question Outcomes', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Correct': [metrics['bias_accuracy'] * 100, metrics['culture_accuracy'] * 100],\n",
    "    'Wrong': [(1 - metrics['bias_accuracy']) * 100, (1 - metrics['culture_accuracy']) * 100]\n",
    "}, index=['Bias Questions', 'Culture Questions'])\n",
    "\n",
    "comparison.plot(kind='barh', stacked=True, ax=axes[1], \n",
    "                color=['mediumseagreen', 'coral'], alpha=0.8)\n",
    "axes[1].set_xlabel('Percentage', fontsize=12)\n",
    "axes[1].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlim(0, 100)\n",
    "axes[1].legend(title='Result', loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefde1c",
   "metadata": {},
   "source": [
    "## 5. Running Real Evaluation\n",
    "\n",
    "To run a real evaluation with actual LLMs, use the command-line scripts:\n",
    "\n",
    "```bash\n",
    "# Evaluate Llama 3.1 8B on Japanese dataset\n",
    "python ../evaluate.py --model llama-3.1-8b --dataset ../csv/ja_dataset.csv\n",
    "\n",
    "# Evaluate multiple models on all datasets\n",
    "python ../evaluate.py --model llama-3.1-8b gpt-4 --all-datasets\n",
    "\n",
    "# Analyze results\n",
    "python ../analyze_results.py --results results/*.csv\n",
    "```\n",
    "\n",
    "### Loading Real Results\n",
    "\n",
    "If you have run evaluations, you can load the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load results from a real evaluation (if available)\n",
    "results_dir = Path('../results')\n",
    "\n",
    "if results_dir.exists():\n",
    "    result_files = list(results_dir.glob('*.csv'))\n",
    "    if result_files:\n",
    "        print(f\"Found {len(result_files)} result file(s):\")\n",
    "        for f in result_files:\n",
    "            print(f\"  - {f.name}\")\n",
    "        \n",
    "        # Load the first result file\n",
    "        result_df = pd.read_csv(result_files[0])\n",
    "        print(f\"\\nLoaded: {result_files[0].name}\")\n",
    "        print(f\"Shape: {result_df.shape}\")\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        real_metrics = calculate_metrics(result_df)\n",
    "        print_metrics(real_metrics, result_files[0].stem)\n",
    "    else:\n",
    "        print(\"No result files found. Run evaluate.py first!\")\n",
    "else:\n",
    "    print(\"Results directory not found. Run evaluate.py first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb6056",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ‚úÖ **Data Exploration**: Understanding the structure of SOBACO datasets\n",
    "2. ‚úÖ **Evaluation Process**: How models are evaluated (simulated)\n",
    "3. ‚úÖ **Metrics Calculation**: Computing accuracy, bias rates, and other metrics\n",
    "4. ‚úÖ **Visualization**: Creating insightful plots for analysis\n",
    "5. ‚úÖ **Bias Detection**: Identifying when models exhibit biased behavior\n",
    "\n",
    "For real evaluations, use the command-line scripts provided in the repository."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
